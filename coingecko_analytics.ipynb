{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b52e314",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'blockchain-j35kOT8t-py3.12 (Python 3.12.10)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/vater/AppData/Local/pypoetry/Cache/virtualenvs/blockchain-j35kOT8t-py3.12/Scripts/python.exe -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "\n",
    "ROOT = \".\"  # run from btc_data/\n",
    "\n",
    "PATHS = {\n",
    "    \"spot_daily\":        f\"{ROOT}/coingecko/market_daily/*.parquet\",\n",
    "    \"spot_hourly\":       f\"{ROOT}/coingecko/market_hourly_chunks/vs=usd/*.parquet\",\n",
    "    \"ohlc_daily\":        f\"{ROOT}/coingecko/ohlc_daily_chunks/vs=usd/*.parquet\",\n",
    "    \"exch_vol\":          f\"{ROOT}/coingecko/exchange_volume_chunks/exchange=*/exchange_volume_*.parquet\",\n",
    "    \"global_snap\":       f\"{ROOT}/coingecko/global_snapshots/*.parquet\",\n",
    "    \"deriv_snap\":        f\"{ROOT}/coingecko/derivatives_btc_snapshots/*.parquet\",\n",
    "    \"cex_trades\":        f\"{ROOT}/cex/cex_trades/*.parquet\",\n",
    "    \"perp_funding_oi\":   f\"{ROOT}/perp/btc_funding_oi/*.parquet\",\n",
    "}\n",
    "\n",
    "con = duckdb.connect(database=\":memory:\")\n",
    "con.execute(\"PRAGMA threads=4;\")\n",
    "\n",
    "def profile(name: str, glob_path: str) -> pd.DataFrame:\n",
    "    # Show schema + basic coverage, without materializing full tables in Python.\n",
    "    q = f\"\"\"\n",
    "    WITH t AS (SELECT * FROM read_parquet('{glob_path}'))\n",
    "    SELECT\n",
    "        '{name}' AS dataset,\n",
    "        COUNT(*) AS rows,\n",
    "        MIN(time) AS min_time,\n",
    "        MAX(time) AS max_time\n",
    "    FROM t\n",
    "    \"\"\"\n",
    "    # Some tables may not have \"time\" (e.g. derivatives uses snapshot_time or similar).\n",
    "    # We'll try \"time\" first; on failure, fall back to \"snapshot_time\" then \"timestamp\".\n",
    "    try:\n",
    "        return con.execute(q).df()\n",
    "    except duckdb.Error:\n",
    "        for alt in [\"snapshot_time\", \"timestamp\"]:\n",
    "            q2 = f\"\"\"\n",
    "            WITH t AS (SELECT * FROM read_parquet('{glob_path}'))\n",
    "            SELECT\n",
    "                '{name}' AS dataset,\n",
    "                COUNT(*) AS rows,\n",
    "                MIN({alt}) AS min_time,\n",
    "                MAX({alt}) AS max_time\n",
    "            FROM t\n",
    "            \"\"\"\n",
    "            try:\n",
    "                return con.execute(q2).df()\n",
    "            except duckdb.Error:\n",
    "                continue\n",
    "        # No time-like column found; just row count.\n",
    "        q3 = f\"\"\"\n",
    "        WITH t AS (SELECT * FROM read_parquet('{glob_path}'))\n",
    "        SELECT '{name}' AS dataset, COUNT(*) AS rows\n",
    "        FROM t\n",
    "        \"\"\"\n",
    "        return con.execute(q3).df()\n",
    "\n",
    "def show_columns(name: str, glob_path: str) -> pd.DataFrame:\n",
    "    q = f\"\"\"\n",
    "    SELECT column_name, data_type\n",
    "    FROM duckdb_columns()\n",
    "    WHERE table_name = 'read_parquet'\n",
    "    \"\"\"\n",
    "    # Trick: DESCRIBE read_parquet(...) is easiest\n",
    "    return con.execute(f\"DESCRIBE SELECT * FROM read_parquet('{glob_path}')\").df()\n",
    "\n",
    "def dedupe_time_key(glob_path: str, key_cols: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Returns a DuckDB subquery that keeps 1 row per key. Useful for chunk overlaps.\n",
    "    \"\"\"\n",
    "    key = \", \".join(key_cols)\n",
    "    return f\"\"\"\n",
    "    (\n",
    "      SELECT * EXCLUDE(rn)\n",
    "      FROM (\n",
    "        SELECT *, row_number() OVER (PARTITION BY {key} ORDER BY {key}) AS rn\n",
    "        FROM read_parquet('{glob_path}')\n",
    "      )\n",
    "      WHERE rn = 1\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) PROFILE ALL DATASETS\n",
    "# -----------------------------\n",
    "profiles = []\n",
    "for k, p in PATHS.items():\n",
    "    profiles.append(profile(k, p))\n",
    "\n",
    "print(\"\\n=== DATASET PROFILE ===\")\n",
    "print(pd.concat(profiles, ignore_index=True).to_string(index=False))\n",
    "\n",
    "# Optional: show schemas for the core ones\n",
    "for k in [\"spot_daily\", \"spot_hourly\", \"exch_vol\", \"perp_funding_oi\", \"deriv_snap\"]:\n",
    "    print(f\"\\n=== COLUMNS: {k} ===\")\n",
    "    try:\n",
    "        print(show_columns(k, PATHS[k]).to_string(index=False))\n",
    "    except Exception as e:\n",
    "        print(f\"(could not describe {k}: {e})\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) QA CHECKS: DUPES + GAPS\n",
    "# -----------------------------\n",
    "# Spot hourly: check missing hours and duplicates after dedupe\n",
    "spot_hourly = dedupe_time_key(PATHS[\"spot_hourly\"], [\"time\"])  # time should be unique per hour\n",
    "q_gaps = f\"\"\"\n",
    "WITH t AS (\n",
    "  SELECT time::TIMESTAMP AS time, price\n",
    "  FROM {spot_hourly}\n",
    "),\n",
    "d AS (\n",
    "  SELECT\n",
    "    time,\n",
    "    LAG(time) OVER (ORDER BY time) AS prev_time\n",
    "  FROM t\n",
    ")\n",
    "SELECT\n",
    "  COUNT(*) FILTER (WHERE prev_time IS NULL) AS first_row,\n",
    "  COUNT(*) FILTER (WHERE prev_time IS NOT NULL AND (time - prev_time) != INTERVAL 1 HOUR) AS non_1h_steps,\n",
    "  MIN(time - prev_time) FILTER (WHERE prev_time IS NOT NULL) AS min_step,\n",
    "  MAX(time - prev_time) FILTER (WHERE prev_time IS NOT NULL) AS max_step\n",
    "FROM d\n",
    "\"\"\"\n",
    "print(\"\\n=== QA: SPOT HOURLY GAPS ===\")\n",
    "print(con.execute(q_gaps).df().to_string(index=False))\n",
    "\n",
    "# Exchange volume: chunks overlap at boundaries; dedupe by (exchange,time)\n",
    "exch_vol = dedupe_time_key(PATHS[\"exch_vol\"], [\"exchange\", \"time\"])\n",
    "q_exch = f\"\"\"\n",
    "WITH t AS (\n",
    "  SELECT exchange, time::TIMESTAMP AS time, volume_btc\n",
    "  FROM {exch_vol}\n",
    ")\n",
    "SELECT exchange, COUNT(*) AS rows, MIN(time) AS min_time, MAX(time) AS max_time\n",
    "FROM t\n",
    "GROUP BY 1\n",
    "ORDER BY 1\n",
    "\"\"\"\n",
    "print(\"\\n=== EXCHANGE VOLUME COVERAGE (DEDUPED) ===\")\n",
    "print(con.execute(q_exch).df().to_string(index=False))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) BUILD ANALYTICAL VIEWS\n",
    "# -----------------------------\n",
    "# (A) Daily spot returns\n",
    "spot_daily = dedupe_time_key(PATHS[\"spot_daily\"], [\"time\"])\n",
    "q_spot_daily = f\"\"\"\n",
    "WITH t AS (\n",
    "  SELECT time::DATE AS day, price\n",
    "  FROM {spot_daily}\n",
    "),\n",
    "r AS (\n",
    "  SELECT\n",
    "    day,\n",
    "    price,\n",
    "    LN(price / LAG(price) OVER (ORDER BY day)) AS log_ret_1d\n",
    "  FROM t\n",
    ")\n",
    "SELECT * FROM r ORDER BY day\n",
    "\"\"\"\n",
    "spot_daily_df = con.execute(q_spot_daily).df()\n",
    "\n",
    "# (B) Exchange volume converted to USD using daily spot price\n",
    "#     Align exchange volume (likely daily points) to day and multiply by spot price.\n",
    "q_exch_usd = f\"\"\"\n",
    "WITH px AS (\n",
    "  SELECT time::DATE AS day, price\n",
    "  FROM {spot_daily}\n",
    "),\n",
    "v AS (\n",
    "  SELECT exchange, time::DATE AS day, volume_btc\n",
    "  FROM {exch_vol}\n",
    "),\n",
    "j AS (\n",
    "  SELECT v.exchange, v.day, v.volume_btc, px.price, (v.volume_btc * px.price) AS volume_usd\n",
    "  FROM v\n",
    "  JOIN px USING (day)\n",
    ")\n",
    "SELECT * FROM j\n",
    "\"\"\"\n",
    "exch_usd_df = con.execute(q_exch_usd).df()\n",
    "\n",
    "# (C) Funding vs subsequent returns (simple example)\n",
    "#     Your perp_funding_oi parquet may have mixed granularities; we attempt to normalize to hourly.\n",
    "perp = dedupe_time_key(PATHS[\"perp_funding_oi\"], [\"exchange\", \"symbol\", \"time\"])\n",
    "q_funding = f\"\"\"\n",
    "WITH t AS (\n",
    "  SELECT\n",
    "    time::TIMESTAMP AS time,\n",
    "    exchange,\n",
    "    symbol,\n",
    "    funding_rate\n",
    "  FROM {perp}\n",
    "  WHERE funding_rate IS NOT NULL\n",
    "),\n",
    "px AS (\n",
    "  SELECT\n",
    "    time::TIMESTAMP AS time,\n",
    "    price,\n",
    "    LN(price / LAG(price) OVER (ORDER BY time)) AS log_ret_1h\n",
    "  FROM {spot_hourly}\n",
    "),\n",
    "j AS (\n",
    "  SELECT\n",
    "    t.time,\n",
    "    t.exchange,\n",
    "    t.symbol,\n",
    "    t.funding_rate,\n",
    "    px.log_ret_1h,\n",
    "    LEAD(px.log_ret_1h, 1) OVER (ORDER BY t.time) AS fwd_log_ret_1h\n",
    "  FROM t\n",
    "  JOIN px USING (time)\n",
    ")\n",
    "SELECT * FROM j\n",
    "\"\"\"\n",
    "try:\n",
    "    funding_join_df = con.execute(q_funding).df()\n",
    "except duckdb.Error as e:\n",
    "    funding_join_df = None\n",
    "    print(\"\\nNOTE: funding join failed (schema mismatch is possible). Error:\\n\", e)\n",
    "\n",
    "# -----------------------------\n",
    "# 4) SIMPLE OUTPUT + PLOTS\n",
    "# -----------------------------\n",
    "print(\"\\n=== DAILY SPOT SAMPLE ===\")\n",
    "print(spot_daily_df.head(5).to_string(index=False))\n",
    "\n",
    "print(\"\\n=== EXCHANGE USD VOLUME SAMPLE ===\")\n",
    "print(exch_usd_df.head(5).to_string(index=False))\n",
    "\n",
    "if funding_join_df is not None:\n",
    "    print(\"\\n=== FUNDING JOIN SAMPLE ===\")\n",
    "    print(funding_join_df.head(5).to_string(index=False))\n",
    "\n",
    "    # quick correlation (overall)\n",
    "    corr = funding_join_df[[\"funding_rate\", \"fwd_log_ret_1h\"]].corr(numeric_only=True).iloc[0,1]\n",
    "    print(f\"\\nCorr(funding_rate, next_hour_return) = {corr:.6f}\")\n",
    "\n",
    "# Optional plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot BTC daily price + daily log returns (two separate figures, no style/colors forced)\n",
    "fig = plt.figure()\n",
    "plt.plot(spot_daily_df[\"day\"], spot_daily_df[\"price\"])\n",
    "plt.title(\"BTC spot price (daily)\")\n",
    "plt.xlabel(\"day\")\n",
    "plt.ylabel(\"price\")\n",
    "plt.tight_layout()\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(spot_daily_df[\"day\"], spot_daily_df[\"log_ret_1d\"])\n",
    "plt.title(\"BTC log returns (1d)\")\n",
    "plt.xlabel(\"day\")\n",
    "plt.ylabel(\"log_ret_1d\")\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blockchain-j35kOT8t-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
