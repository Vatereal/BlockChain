{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a124a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "import datetime as dt\n",
    "import gc\n",
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "import math\n",
    "import sys\n",
    "from datetime import date, datetime, time as dtime\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Iterable, Iterator\n",
    "\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "# Third-party\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullFormatter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pyarrow.dataset as ds\n",
    "import requests\n",
    "from scipy.stats import expon, gaussian_kde, kstest\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985188bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Address-to-Address Flow Graph (Top-K SOURCES → Top-M DESTS)\n",
    "# MEMORY-SAFE + DIAGNOSTICS + PATTERNS AWARE + PREFIX FIXES\n",
    "# ============================\n",
    "\n",
    "# ---------- knobs ----------\n",
    "TOPK_SRC           = 25                       # Top spenders we trace as SOURCES\n",
    "TOPM_DST           = 150                      # Keep top-M destinations overall (after reduce)\n",
    "START_DAY          = \"2013-01-01\"             # str or datetime.date; we coerce below\n",
    "MAX_TX_OUTPAIRS    = 400                      # skip pathological tx (sources*destinations)\n",
    "SHOW_EDGES         = 150                      # edges to draw after pruning\n",
    "FINAL_MIN_EDGE_BTC = 0.5                      # filter AFTER aggregation (visual clarity)\n",
    "TX_MIN_EDGE_BTC    = 0.0                      # per-transaction edge cutoff BEFORE aggregation\n",
    "MAX_DST_PER_TX     = 20                       # per spend tx, keep at most top-N dest outputs\n",
    "CHUNK_SIZE         = int(os.getenv(\"CHUNK_SIZE\", \"200\"))\n",
    "\n",
    "# On-disk scratch (external drive recommended)\n",
    "WORKDIR            = Path(\"/media/vatereal/Main/flows_tmp\")\n",
    "OUTPOINTS_DS       = WORKDIR / \"top_outpoints_ds\"     # (txid,n)->(addr,value) for TOP SOURCES\n",
    "EDGES_DS           = WORKDIR / \"edges_fragments\"      # edge fragments to be reduced later\n",
    "PFX_LEN            = 3                                 # txid prefix length for partitioning (3–4 ok)\n",
    "\n",
    "WORKDIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPOINTS_DS.mkdir(parents=True, exist_ok=True)\n",
    "EDGES_DS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- config helpers ----------\n",
    "def _coerce_start_day(x) -> date:\n",
    "    if isinstance(x, date):\n",
    "        return x\n",
    "    if isinstance(x, str):\n",
    "        return date.fromisoformat(x)\n",
    "    return date(2013, 1, 1)\n",
    "\n",
    "SAFER_START: date = _coerce_start_day(START_DAY)\n",
    "\n",
    "# VALUE_UNITS controls btc conversion; if not pre-defined, make a safe default\n",
    "try:\n",
    "    VALUE_UNITS  # may be defined earlier\n",
    "except NameError:\n",
    "    VALUE_UNITS = os.getenv(\"VALUE_UNITS\", \"sats\").lower()\n",
    "\n",
    "try:\n",
    "    value_btc_expr  # may be defined earlier\n",
    "except NameError:\n",
    "    if VALUE_UNITS in (\"sats\", \"sat\", \"satoshis\"):\n",
    "        value_btc_expr = (pl.col(\"value\").cast(pl.Float64) / 1e8)\n",
    "    elif VALUE_UNITS in (\"btc\",):\n",
    "        value_btc_expr = pl.col(\"value\").cast(pl.Float64)\n",
    "    else:\n",
    "        value_btc_expr = (pl.col(\"value\").cast(pl.Float64) / 1e8)\n",
    "\n",
    "# Basic address normalizer; reuse if provided\n",
    "try:\n",
    "    _addr_norm_py\n",
    "except NameError:\n",
    "    def _addr_norm_py(x):\n",
    "        if x is None:\n",
    "            return None\n",
    "        s = str(x).strip()\n",
    "        return s if s else None\n",
    "\n",
    "# Expect 'patterns' to be defined earlier; safe fallback if not\n",
    "try:\n",
    "    patterns\n",
    "except NameError:\n",
    "    PARQUET_DIR = Path(os.getenv(\"PARQUET_DIR\", \".\"))\n",
    "    patterns = {\n",
    "        \"blocks\": str(PARQUET_DIR / \"blocks/day=*/blocks-*.parquet\"),\n",
    "        \"txs\":    str(PARQUET_DIR / \"txs/day=*/txs-*.parquet\"),\n",
    "        \"io\":     str(PARQUET_DIR / \"io/day=*/io-*.parquet\"),\n",
    "    }\n",
    "\n",
    "# ---------- utilities ----------\n",
    "DAY_RE = re.compile(r\"[\\/\\\\]day=(\\d{4}-\\d{2}-\\d{2})[\\/\\\\]\")\n",
    "\n",
    "def _day_from_path(p: str) -> date | None:\n",
    "    m = DAY_RE.search(p)\n",
    "    if not m:\n",
    "        return None\n",
    "    try:\n",
    "        return date.fromisoformat(m.group(1))\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def list_io_files_since(since_day: date) -> list[str]:\n",
    "    all_paths = glob.glob(patterns[\"io\"])\n",
    "    keep: list[str] = []\n",
    "    for p in all_paths:\n",
    "        d = _day_from_path(p)\n",
    "        if d is None or d < since_day:\n",
    "            continue\n",
    "        keep.append(p)\n",
    "    keep.sort()\n",
    "    return keep\n",
    "\n",
    "def chunked_paths(paths: Iterable[str], n: int) -> Iterator[list[str]]:\n",
    "    batch: list[str] = []\n",
    "    for p in paths:\n",
    "        batch.append(p)\n",
    "        if len(batch) >= n:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    if batch:\n",
    "        yield batch\n",
    "\n",
    "# Column aliasing (if your schema varies)\n",
    "ALIASES = {\n",
    "    \"addr\": \"address\",\n",
    "    \"val\": \"value\",\n",
    "    \"tx_id\": \"txid\",\n",
    "    \"prev_tx\": \"prev_txid\",\n",
    "    \"prev_index\": \"prev_vout\",\n",
    "    \"vout\": \"n\",\n",
    "}\n",
    "\n",
    "NEEDED_ANYWAY = [\"dir\", \"txid\", \"n\", \"address\", \"value\",\n",
    "                 \"prev_txid\", \"prev_vout\", \"height\", \"time\"]  # presence optional; we add nulls if missing\n",
    "\n",
    "def scan_with_override(paths: list[str]) -> pl.LazyFrame:\n",
    "    lf = pl.scan_parquet(paths)\n",
    "    have = lf.collect_schema().names()\n",
    "    renames = {src: dst for src, dst in ALIASES.items() if src in have and dst not in have}\n",
    "    if renames:\n",
    "        lf = lf.rename(renames)\n",
    "    have = set(lf.collect_schema().names())\n",
    "    missing = [c for c in NEEDED_ANYWAY if c not in have]\n",
    "    if missing:\n",
    "        lf = lf.with_columns([pl.lit(None).alias(c) for c in missing])\n",
    "    return lf\n",
    "\n",
    "# Small helper: drop obviously bad address tokens\n",
    "def _is_bad_addr_expr(col: str) -> pl.Expr:\n",
    "    return (pl.col(col).is_null() | (pl.col(col) == \"\") |\n",
    "            (pl.col(col).cast(pl.Utf8, strict=False).str.to_lowercase() == \"null\") |\n",
    "            (pl.col(col).cast(pl.Utf8, strict=False).str.to_lowercase() == \"none\"))\n",
    "\n",
    "# ---- PREFIX HELPERS (normalize to lowercase) ----\n",
    "def _prefix_expr(col=\"txid\", n=PFX_LEN) -> pl.Expr:\n",
    "    return pl.col(col).cast(pl.Utf8, strict=False).str.to_lowercase().str.slice(0, n).alias(\"pfx\")\n",
    "\n",
    "def _txid_prefix_expr(col=\"txid\", n=PFX_LEN) -> pl.Expr:\n",
    "    return _prefix_expr(col, n)\n",
    "\n",
    "def _prevtxid_prefix_expr(col=\"prev_txid\", n=PFX_LEN) -> pl.Expr:\n",
    "    return _prefix_expr(col, n)\n",
    "\n",
    "def _normalize_pfx_key(key) -> str:\n",
    "    # Polars group_by iteration may yield ('abc',) for single key\n",
    "    if isinstance(key, tuple) and len(key) == 1:\n",
    "        key = key[0]\n",
    "    return (str(key) if key is not None else \"\").lower()[:PFX_LEN]\n",
    "\n",
    "# ---------- diagnostics ----------\n",
    "def dataset_smoke_check(max_files: int = 8):\n",
    "    counts = {k: len(glob.glob(v)) for k, v in patterns.items()}\n",
    "    print(\"[smoke] file counts:\", counts)\n",
    "\n",
    "    io_paths = list_io_files_since(SAFER_START)\n",
    "    if not io_paths:\n",
    "        print(\"[smoke] no io files at/after\", SAFER_START)\n",
    "        return\n",
    "\n",
    "    sample = io_paths[:max_files]\n",
    "    lf = scan_with_override(sample)\n",
    "    have = lf.collect_schema().names()\n",
    "    print(\"[smoke] checking up to\", len(sample), \"io files...\")\n",
    "    print(\"[smoke] columns after alias:\", have)\n",
    "\n",
    "    if {\"dir\"}.issubset(have):\n",
    "        counts_dir = (\n",
    "            lf.select(\"dir\")\n",
    "              .with_columns(pl.col(\"dir\").cast(pl.Utf8, strict=False))\n",
    "              .group_by(\"dir\").len().sort(\"len\", descending=True)\n",
    "              .collect()\n",
    "        )\n",
    "        print(\"[smoke] dir counts:\\n\", counts_dir)\n",
    "\n",
    "    try:\n",
    "        tminmax = (\n",
    "            lf.select(\n",
    "                pl.col(\"time\").cast(pl.Datetime, strict=False).min().alias(\"min\"),\n",
    "                pl.col(\"time\").cast(pl.Datetime, strict=False).max().alias(\"max\"),\n",
    "            ).collect()\n",
    "        )\n",
    "        print(\"[smoke] time range:\\n\", tminmax)\n",
    "    except Exception as e:\n",
    "        print(\"[smoke] time range skipped:\", repr(e))\n",
    "\n",
    "# ---------- index prefix discovery (handles both naming styles) ----------\n",
    "def _parse_pfx_dirname(base: str) -> str | None:\n",
    "    # base is like \"pfx=abc\" OR \"pfx=('abc',)\"\n",
    "    if not base.startswith(\"pfx=\"):\n",
    "        return None\n",
    "    val = base[4:]\n",
    "    if val.startswith(\"('\") and val.endswith(\"',)\"):\n",
    "        p = val[2:-3]\n",
    "    else:\n",
    "        p = val\n",
    "    if not p:\n",
    "        return None\n",
    "    return p.lower()\n",
    "\n",
    "def list_index_prefixes() -> set[str]:\n",
    "    pfx_set: set[str] = set()\n",
    "    for d in glob.glob(str(OUTPOINTS_DS / \"pfx=*\")):\n",
    "        base = os.path.basename(d)\n",
    "        p = _parse_pfx_dirname(base)\n",
    "        if p:\n",
    "            pfx_set.add(p)\n",
    "    return pfx_set\n",
    "\n",
    "# ============================\n",
    "# 0) Top-K **sources** by “spent outputs”\n",
    "# ============================\n",
    "def topk_sources_chunked(k: int = TOPK_SRC) -> list[str]:\n",
    "    files = list_io_files_since(SAFER_START)\n",
    "    if not files:\n",
    "        print(\"[topk] no io files found via patterns; check `patterns['io']` and START_DAY.\")\n",
    "        return []\n",
    "    partial = None\n",
    "\n",
    "    for paths_chunk in chunked_paths(files, CHUNK_SIZE):\n",
    "        lf = scan_with_override(paths_chunk)\n",
    "        have = lf.collect_schema().names()\n",
    "        need_out = [c for c in (\"dir\",\"txid\",\"n\",\"address\",\"value\") if c in have]\n",
    "        need_in  = [c for c in (\"dir\",\"prev_txid\",\"prev_vout\") if c in have]\n",
    "        if not {\"dir\"}.issubset(set(have)):\n",
    "            continue\n",
    "\n",
    "        outs = (\n",
    "            lf.select(need_out)\n",
    "              .with_columns([\n",
    "                  pl.col(\"dir\").cast(pl.Utf8, strict=False),\n",
    "                  pl.col(\"address\").map_elements(_addr_norm_py, return_dtype=pl.Utf8).alias(\"addr_norm\"),\n",
    "                  value_btc_expr.cast(pl.Float32).alias(\"value_btc\"),\n",
    "              ])\n",
    "              .filter(pl.col(\"dir\").str.to_lowercase().str.contains(\"out\", literal=True))\n",
    "              .filter(~_is_bad_addr_expr(\"addr_norm\"))\n",
    "              .select([\"txid\",\"n\",\"addr_norm\",\"value_btc\"])\n",
    "              .collect()\n",
    "        )\n",
    "        if not outs.height:\n",
    "            continue\n",
    "\n",
    "        ins = (\n",
    "            lf.select(need_in)\n",
    "              .with_columns([\n",
    "                  pl.col(\"dir\").cast(pl.Utf8, strict=False),\n",
    "                  pl.col(\"prev_txid\").cast(pl.Utf8, strict=False).alias(\"ptx\"),\n",
    "                  pl.col(\"prev_vout\").cast(pl.Int64, strict=False).alias(\"pvout\"),\n",
    "              ])\n",
    "              .filter(pl.col(\"dir\").str.to_lowercase().str.contains(\"in\", literal=True)\n",
    "                      & pl.col(\"ptx\").is_not_null() & pl.col(\"pvout\").is_not_null())\n",
    "              .select([\"ptx\",\"pvout\"])\n",
    "              .collect()\n",
    "        )\n",
    "        if not ins.height:\n",
    "            continue\n",
    "\n",
    "        joined = (\n",
    "            pl.from_dataframe(ins)\n",
    "              .join(outs.rename({\"txid\":\"ptx\",\"n\":\"pvout\"}), on=[\"ptx\",\"pvout\"], how=\"inner\")\n",
    "              .group_by(\"addr_norm\")\n",
    "              .len()\n",
    "              .sort(\"len\", descending=True)\n",
    "              .limit(5000)\n",
    "        )\n",
    "        partial = joined if partial is None else (\n",
    "            pl.concat([partial, joined], how=\"vertical_relaxed\")\n",
    "              .group_by(\"addr_norm\")\n",
    "              .agg(pl.col(\"len\").sum().alias(\"len\"))\n",
    "              .sort(\"len\", descending=True)\n",
    "              .limit(20_000)\n",
    "        )\n",
    "        del outs, ins, joined\n",
    "        gc.collect()\n",
    "\n",
    "    if partial is None or not partial.height:\n",
    "        return []\n",
    "    tops = (partial.filter(~_is_bad_addr_expr(\"addr_norm\"))\n",
    "                    .sort(\"len\", descending=True)\n",
    "                    .head(k))\n",
    "    try: display(tops)\n",
    "    except: print(tops)\n",
    "    return tops[\"addr_norm\"].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93feec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 1) Build ON-DISK index (txid,n -> src_addr,value_btc) for TOP sources\n",
    "# ============================\n",
    "def build_top_outpoints_dataset(top_sources: list[str]):\n",
    "    if not top_sources:\n",
    "        raise RuntimeError(\"No top sources.\")\n",
    "    # clean existing shards\n",
    "    for f in glob.glob(str(OUTPOINTS_DS / \"**/*.parquet\"), recursive=True):\n",
    "        try: os.remove(f)\n",
    "        except: pass\n",
    "\n",
    "    files = list_io_files_since(SAFER_START)\n",
    "    wrote = 0\n",
    "    unique_pfx: set[str] = set()\n",
    "\n",
    "    for paths_chunk in chunked_paths(files, CHUNK_SIZE):\n",
    "        lf = scan_with_override(paths_chunk)\n",
    "        have = lf.collect_schema().names()\n",
    "        need = [c for c in (\"dir\",\"txid\",\"n\",\"address\",\"value\") if c in have]\n",
    "        if not {\"dir\",\"txid\",\"n\",\"value\"}.issubset(set(have)):\n",
    "            continue\n",
    "\n",
    "        outp = (\n",
    "            lf.select(need)\n",
    "              .with_columns([\n",
    "                  pl.col(\"dir\").cast(pl.Utf8, strict=False),\n",
    "                  pl.col(\"address\").map_elements(_addr_norm_py, return_dtype=pl.Utf8).alias(\"addr_norm\"),\n",
    "                  value_btc_expr.cast(pl.Float32).alias(\"value_btc\"),\n",
    "                  _txid_prefix_expr(\"txid\", PFX_LEN),\n",
    "              ])\n",
    "              .filter(pl.col(\"dir\").str.to_lowercase().str.contains(\"out\", literal=True)\n",
    "                      & pl.col(\"value_btc\").is_not_null())\n",
    "              .filter(pl.col(\"addr_norm\").is_in(top_sources) & ~_is_bad_addr_expr(\"addr_norm\"))\n",
    "              .select([\"pfx\",\"txid\",\"n\",\"addr_norm\",\"value_btc\"])\n",
    "              .collect()\n",
    "        )\n",
    "        if not outp.height:\n",
    "            continue\n",
    "\n",
    "        # robust grouping: normalize key to lower + write to canonical \"pfx=<pfx>\" dirs\n",
    "        for key, g in outp.group_by(\"pfx\"):\n",
    "            pfx = _normalize_pfx_key(key)\n",
    "            if not pfx:\n",
    "                continue\n",
    "            pdir = OUTPOINTS_DS / f\"pfx={pfx}\"\n",
    "            pdir.mkdir(parents=True, exist_ok=True)\n",
    "            out_path = pdir / f\"outpoints-{np.random.randint(1e9)}.parquet\"\n",
    "            g.drop(\"pfx\").write_parquet(out_path)\n",
    "            wrote += g.height\n",
    "            unique_pfx.add(pfx)\n",
    "\n",
    "        del outp\n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"[index] wrote {wrote:,} outpoint rows for top sources into {OUTPOINTS_DS}\")\n",
    "    print(f\"[index] unique pfx folders (canonical): {len(unique_pfx)} (sample: {sorted(list(unique_pfx))[:12]})\")\n",
    "\n",
    "def _glob_both_styles_for_pfx(pfx: str) -> list[str]:\n",
    "    \"\"\"Return all parquet paths for either folder style: pfx=abc OR pfx=('abc',).\"\"\"\n",
    "    pfx = (pfx or \"\").lower()\n",
    "    paths: list[str] = []\n",
    "    # canonical\n",
    "    d1 = OUTPOINTS_DS / f\"pfx={pfx}\"\n",
    "    if d1.is_dir():\n",
    "        paths.extend(glob.glob(str(d1 / \"*.parquet\")))\n",
    "    # legacy tuple-style\n",
    "    d2 = OUTPOINTS_DS / (\"pfx=('{}',)\".format(pfx))\n",
    "    if d2.is_dir():\n",
    "        paths.extend(glob.glob(str(d2 / \"*.parquet\")))\n",
    "    return paths\n",
    "\n",
    "def _load_outpoints_for_prefixes(prefixes: list[str]) -> pl.DataFrame:\n",
    "    all_paths: list[str] = []\n",
    "    # Only try the prefixes we actually have on disk (fast path)\n",
    "    have_pfx = list_index_prefixes()\n",
    "    req = { (p or \"\").lower() for p in (prefixes or []) }\n",
    "    wanted = sorted(req & have_pfx)\n",
    "    if not wanted:\n",
    "        return pl.DataFrame(schema={\n",
    "            \"txid\": pl.Utf8, \"n\": pl.Int64, \"addr_norm\": pl.Utf8, \"value_btc\": pl.Float32\n",
    "        })\n",
    "    for p in wanted:\n",
    "        all_paths.extend(_glob_both_styles_for_pfx(p))\n",
    "    if not all_paths:\n",
    "        return pl.DataFrame(schema={\n",
    "            \"txid\": pl.Utf8, \"n\": pl.Int64, \"addr_norm\": pl.Utf8, \"value_btc\": pl.Float32\n",
    "        })\n",
    "    df = pl.concat([pl.read_parquet(p) for p in all_paths], how=\"vertical_relaxed\")\n",
    "    return df.with_columns([\n",
    "        pl.col(\"txid\").cast(pl.Utf8, strict=False),\n",
    "        pl.col(\"n\").cast(pl.Int64, strict=False),\n",
    "        pl.col(\"addr_norm\").cast(pl.Utf8, strict=False),\n",
    "        pl.col(\"value_btc\").cast(pl.Float32, strict=False),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e3667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 2) Build flows: Top-K SOURCES → ANY dest (reduce later) — vectorized, faster\n",
    "# ============================\n",
    "def build_flows_top_sources_streaming() -> pd.DataFrame:\n",
    "    files = list_io_files_since(SAFER_START)\n",
    "    if not files:\n",
    "        return pd.DataFrame(columns=[\"src\",\"dst\",\"btc\"])\n",
    "\n",
    "    # clean old edge fragments\n",
    "    for f in glob.glob(str(EDGES_DS / \"*.parquet\")):\n",
    "        try: os.remove(f)\n",
    "        except: pass\n",
    "\n",
    "    have_pfx = list_index_prefixes()\n",
    "    print(f\"[flows] index prefixes available: {len(have_pfx)}\")\n",
    "\n",
    "    total_pairs = 0\n",
    "\n",
    "    with pl.StringCache():\n",
    "        for batch_i, paths_chunk in enumerate(chunked_paths(files, CHUNK_SIZE), start=1):\n",
    "            lf = scan_with_override(paths_chunk)\n",
    "            have = lf.collect_schema().names()\n",
    "\n",
    "            need_in = [c for c in (\"dir\",\"txid\",\"prev_txid\",\"prev_vout\") if c in have]\n",
    "            if not need_in:\n",
    "                print(f\"[chunk {batch_i}] no inputs columns → skip\")\n",
    "                continue\n",
    "\n",
    "            ins = (\n",
    "                lf.select(need_in)\n",
    "                  .with_columns([\n",
    "                      pl.col(\"dir\").cast(pl.Utf8, strict=False),\n",
    "                      pl.col(\"txid\").cast(pl.Utf8, strict=False).alias(\"spend_txid\"),\n",
    "                      pl.col(\"prev_txid\").cast(pl.Utf8, strict=False).alias(\"ptx\"),\n",
    "                      pl.col(\"prev_vout\").cast(pl.Int64, strict=False).alias(\"pvout\"),\n",
    "                      _prevtxid_prefix_expr(\"prev_txid\", PFX_LEN),\n",
    "                  ])\n",
    "                  .filter(pl.col(\"dir\").str.to_lowercase().str.contains(\"in\", literal=True)\n",
    "                          & pl.col(\"ptx\").is_not_null() & pl.col(\"pvout\").is_not_null())\n",
    "                  .select([\"spend_txid\",\"ptx\",\"pvout\",\"pfx\"])\n",
    "                  .collect()\n",
    "            )\n",
    "            if not ins.height:\n",
    "                print(f\"[chunk {batch_i}] ins=0 → skip\")\n",
    "                continue\n",
    "\n",
    "            need_pfx_all = { (p or \"\").lower() for p in ins[\"pfx\"].unique().to_list() }\n",
    "            need_pfx = sorted(need_pfx_all & have_pfx)\n",
    "            if not need_pfx:\n",
    "                print(f\"[chunk {batch_i}] none of {len(need_pfx_all)} needed prefixes exist on disk → skip\")\n",
    "                del ins; gc.collect(); continue\n",
    "\n",
    "            out_idx_df = _load_outpoints_for_prefixes(need_pfx)\n",
    "            if out_idx_df.is_empty():\n",
    "                print(f\"[chunk {batch_i}] out_idx=0 (requested_prefixes={len(need_pfx)} / raw_needed={len(need_pfx_all)}) → skip\")\n",
    "                del ins; gc.collect(); continue\n",
    "\n",
    "            ins_src = (\n",
    "                pl.from_dataframe(ins)\n",
    "                  .join(\n",
    "                      out_idx_df.rename({\n",
    "                          \"txid\":\"ptx\",\"n\":\"pvout\",\n",
    "                          \"addr_norm\":\"src_addr\",\"value_btc\":\"src_value_btc\"\n",
    "                      }),\n",
    "                      on=[\"ptx\",\"pvout\"], how=\"inner\"\n",
    "                  )\n",
    "                  .group_by([\"spend_txid\",\"src_addr\"])\n",
    "                  .agg(pl.col(\"src_value_btc\").sum().alias(\"s_amt\"))\n",
    "            )\n",
    "            if not ins_src.height:\n",
    "                print(f\"[chunk {batch_i}] ins_src=0 after join → skip\")\n",
    "                del ins; gc.collect(); continue\n",
    "\n",
    "            spend_txids = ins_src[\"spend_txid\"].unique().to_list()\n",
    "\n",
    "            need_out = [c for c in (\"dir\",\"txid\",\"address\",\"value\") if c in have]\n",
    "            outs = (\n",
    "                lf.select(need_out)\n",
    "                  .with_columns([\n",
    "                      pl.col(\"dir\").cast(pl.Utf8, strict=False),\n",
    "                      pl.col(\"txid\").cast(pl.Utf8, strict=False),\n",
    "                      pl.col(\"address\").map_elements(_addr_norm_py, return_dtype=pl.Utf8).alias(\"dst_addr\"),\n",
    "                      value_btc_expr.cast(pl.Float32).alias(\"d_amt\"),\n",
    "                  ])\n",
    "                  .filter(pl.col(\"dir\").str.to_lowercase().str.contains(\"out\", literal=True))\n",
    "                  .filter(pl.col(\"txid\").is_in(spend_txids) & ~_is_bad_addr_expr(\"dst_addr\"))\n",
    "                  .select([\"txid\",\"dst_addr\",\"d_amt\"])\n",
    "                  .group_by([\"txid\",\"dst_addr\"])\n",
    "                  .agg(pl.col(\"d_amt\").sum().alias(\"d_amt\"))\n",
    "                  .rename({\"txid\":\"spend_txid\"})\n",
    "                  .collect()\n",
    "            )\n",
    "            if not outs.height:\n",
    "                print(f\"[chunk {batch_i}] outs=0 for spenders → skip\")\n",
    "                del ins, ins_src; gc.collect(); continue\n",
    "\n",
    "            # --- vectorized edge accumulation (no Python tuple loops)\n",
    "            S = ins_src.to_pandas()\n",
    "            D = outs.to_pandas()\n",
    "            gS, gD = S.groupby(\"spend_txid\"), D.groupby(\"spend_txid\")\n",
    "            common_tx = sorted(set(gS.groups.keys()) & set(gD.groups.keys()))\n",
    "            if not common_tx:\n",
    "                print(f\"[chunk {batch_i}] common_tx=0 (ins_src_tx={S['spend_txid'].nunique()}, outs_tx={D['spend_txid'].nunique()})\")\n",
    "                del ins, ins_src, outs, S, D; gc.collect(); continue\n",
    "\n",
    "            # accumulate columns; flush once per chunk\n",
    "            col_src, col_dst, col_btc = [], [], []\n",
    "            pairs_this_chunk = 0\n",
    "\n",
    "            for tx in common_tx:\n",
    "                s = gS.get_group(tx)                   # src_addr, s_amt\n",
    "                d = gD.get_group(tx).copy()            # dst_addr, d_amt\n",
    "                d = d.sort_values(\"d_amt\", ascending=False).head(MAX_DST_PER_TX)\n",
    "\n",
    "                if len(s) * len(d) > MAX_TX_OUTPAIRS:\n",
    "                    continue\n",
    "\n",
    "                s_total = float(s[\"s_amt\"].sum())\n",
    "                d_total = float(d[\"d_amt\"].sum())\n",
    "                if s_total <= 0 or d_total <= 0:\n",
    "                    continue\n",
    "\n",
    "                V = min(s_total, d_total)\n",
    "                s_share = (s[\"s_amt\"].values / s_total)   # shape (S,)\n",
    "                d_share = (d[\"d_amt\"].values / d_total)   # shape (D,)\n",
    "                F = V * np.outer(s_share, d_share)        # (S,D)\n",
    "\n",
    "                if TX_MIN_EDGE_BTC > 0.0:\n",
    "                    mask = F >= TX_MIN_EDGE_BTC\n",
    "                else:\n",
    "                    mask = np.ones_like(F, dtype=bool)\n",
    "\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "\n",
    "                ii, jj = np.nonzero(mask)\n",
    "                weights = F[mask].astype(np.float32)\n",
    "\n",
    "                srcs = s[\"src_addr\"].values\n",
    "                dsts = d[\"dst_addr\"].values\n",
    "\n",
    "                col_src += [srcs[i] for i in ii]\n",
    "                col_dst += [dsts[j] for j in jj]\n",
    "                col_btc += weights.tolist()\n",
    "                pairs_this_chunk += len(weights)\n",
    "\n",
    "            # one Parquet per chunk (fewer shards, faster IO)\n",
    "            if col_src:\n",
    "                shard = EDGES_DS / f\"edges-b{batch_i:05d}.parquet\"\n",
    "                pl.DataFrame(\n",
    "                    {\"src\": col_src, \"dst\": col_dst, \"btc\": col_btc},\n",
    "                    schema={\"src\": pl.Utf8, \"dst\": pl.Utf8, \"btc\": pl.Float32},\n",
    "                ).write_parquet(shard, compression=\"zstd\", compression_level=3, statistics=False)\n",
    "\n",
    "            total_pairs += pairs_this_chunk\n",
    "            print(f\"[chunk {batch_i}] ins={len(ins)} out_idx={len(out_idx_df)} ins_src={len(S)} \"\n",
    "                  f\"outs={len(D)} common_tx={len(common_tx)} pairs_added={pairs_this_chunk}\")\n",
    "\n",
    "            del ins, ins_src, outs, S, D, out_idx_df, col_src, col_dst, col_btc\n",
    "            gc.collect()\n",
    "\n",
    "    shard_paths = glob.glob(str(EDGES_DS / \"*.parquet\"))\n",
    "    if not shard_paths:\n",
    "        print(\"[flows] no edge fragments were written (try: TX_MIN_EDGE_BTC↓, TOPK_SRC↑, MAX_DST_PER_TX↑, PFX_LEN↑).\")\n",
    "        return pd.DataFrame(columns=[\"src\",\"dst\",\"btc\"])\n",
    "    print(f\"[flows] reduce {len(shard_paths)} edge shards (total_pairs={total_pairs}) from {EDGES_DS}\")\n",
    "\n",
    "    reduced = (\n",
    "        pl.scan_parquet(shard_paths)\n",
    "          .group_by([\"src\",\"dst\"])\n",
    "          .agg(pl.col(\"btc\").sum().alias(\"btc\"))\n",
    "          .sort(\"btc\", descending=True)\n",
    "          .collect()\n",
    "    )\n",
    "\n",
    "    # Top-M destinations by inbound BTC\n",
    "    top_dst = (\n",
    "        reduced.group_by(\"dst\")\n",
    "               .agg(pl.col(\"btc\").sum().alias(\"in_btc\"))\n",
    "               .sort(\"in_btc\", descending=True)\n",
    "               .head(TOPM_DST)\n",
    "               .get_column(\"dst\")\n",
    "               .to_list()\n",
    "    )\n",
    "    final_edges = (\n",
    "        reduced.filter(pl.col(\"dst\").is_in(top_dst) & (pl.col(\"btc\") >= FINAL_MIN_EDGE_BTC))\n",
    "               .sort(\"btc\", descending=True)\n",
    "               .to_pandas()\n",
    "    )\n",
    "    return final_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded5b662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# 3) Plot — cleaner, less cluttered\n",
    "# ============================\n",
    "def plot_flow_graph(\n",
    "    edges: pd.DataFrame,\n",
    "    sources: set[str] | None = None,\n",
    "    max_edges: int = SHOW_EDGES,\n",
    "    min_edge_btc: float = FINAL_MIN_EDGE_BTC,\n",
    "    layout: str = \"bipartite\",         # \"bipartite\" | \"spring\" | \"kk\" | \"circular\"\n",
    "    label_top: int = 60,               # only label top-N nodes by strength\n",
    "    min_node_strength: float = 0.0,    # hide nodes with very tiny total flow\n",
    "    figsize=(22, 13),\n",
    "    dpi: int = 160,\n",
    "    save_path: str | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Draws a cleaner flow graph:\n",
    "      - prunes to top edges and labels top nodes\n",
    "      - optional bipartite L->R layout (sources on the left)\n",
    "      - shortened labels for readability\n",
    "    \"\"\"\n",
    "    if edges.empty:\n",
    "        print(\"No edges to draw.\"); return\n",
    "\n",
    "    # ---- prune edges for visibility\n",
    "    e = edges[edges[\"btc\"] >= min_edge_btc].copy()\n",
    "    e = e.sort_values(\"btc\", ascending=False).head(max_edges)\n",
    "    if e.empty:\n",
    "        print(\"No edges reach the visibility threshold.\"); return\n",
    "\n",
    "    # ---- node strengths (weighted degree)\n",
    "    node_strength = {}\n",
    "    for _, r in e.iterrows():\n",
    "        node_strength[r[\"src\"]] = node_strength.get(r[\"src\"], 0.0) + float(r[\"btc\"])\n",
    "        node_strength[r[\"dst\"]] = node_strength.get(r[\"dst\"], 0.0) + float(r[\"btc\"])\n",
    "\n",
    "    # optional node pruning\n",
    "    if min_node_strength > 0:\n",
    "        keep = {n for n, w in node_strength.items() if w >= min_node_strength}\n",
    "        e = e[e[\"src\"].isin(keep) & e[\"dst\"].isin(keep)]\n",
    "        node_strength = {n: w for n, w in node_strength.items() if n in keep}\n",
    "\n",
    "    # ---- build graph\n",
    "    G = nx.DiGraph()\n",
    "    for n, w in node_strength.items():\n",
    "        G.add_node(n, weight=w)\n",
    "    for _, r in e.iterrows():\n",
    "        if r[\"src\"] != r[\"dst\"]:\n",
    "            G.add_edge(r[\"src\"], r[\"dst\"], weight=float(r[\"btc\"]))\n",
    "\n",
    "    if G.number_of_nodes() == 0:\n",
    "        print(\"Nothing to draw after pruning.\"); return\n",
    "\n",
    "    # ---- choose layout\n",
    "    def _bipartite_positions(G, left: set[str]):\n",
    "        left = [n for n in G.nodes() if n in left]\n",
    "        right = [n for n in G.nodes() if n not in left]\n",
    "        pos = {}\n",
    "        if left:\n",
    "            ys = np.linspace(0, 1, len(left), endpoint=True)\n",
    "            for i, n in enumerate(sorted(left, key=lambda x: -G.nodes[x][\"weight\"])):\n",
    "                pos[n] = np.array([0.0, ys[i]])\n",
    "        if right:\n",
    "            ys = np.linspace(0, 1, len(right), endpoint=True)\n",
    "            for i, n in enumerate(sorted(right, key=lambda x: -G.nodes[x][\"weight\"])):\n",
    "                pos[n] = np.array([1.0, ys[i]])\n",
    "        return pos\n",
    "\n",
    "    if layout == \"bipartite\" and sources:\n",
    "        pos = _bipartite_positions(G, set(sources))\n",
    "    elif layout == \"kk\":\n",
    "        pos = nx.kamada_kawai_layout(G, weight=\"weight\")\n",
    "    elif layout == \"circular\":\n",
    "        pos = nx.circular_layout(G)\n",
    "    else:\n",
    "        k = 2.0 / max(1.0, math.sqrt(G.number_of_nodes()))\n",
    "        pos = nx.spring_layout(G, k=k, iterations=300, seed=42, weight=\"weight\")\n",
    "\n",
    "    # ---- node sizes & colors\n",
    "    node_sizes = [max(120, 2000 * math.log10(G.nodes[n][\"weight\"] + 10.0)) for n in G.nodes()]\n",
    "    if sources:\n",
    "        node_colors = [\"#2b8cbe\" if n in sources else \"#a1d99b\" for n in G.nodes()]\n",
    "    else:\n",
    "        node_colors = \"#4c72b0\"\n",
    "\n",
    "    # Optional community tint (best-effort)\n",
    "    try:\n",
    "        from networkx.algorithms.community import greedy_modularity_communities\n",
    "        comms = list(greedy_modularity_communities(G.to_undirected()))\n",
    "        comm_map = {}\n",
    "        for i, c in enumerate(comms):\n",
    "            for n in c: comm_map[n] = i\n",
    "        if not sources:\n",
    "            palette = [\"#4c72b0\",\"#55a868\",\"#c44e52\",\"#8172b3\",\"#ccb974\",\"#64b5cd\"]\n",
    "            node_colors = [palette[comm_map.get(n,0) % len(palette)] for n in G.nodes()]\n",
    "    except Exception:\n",
    "        pass  # community step is optional\n",
    "\n",
    "    # ---- edge widths & alpha by weight\n",
    "    es = np.array([d[\"weight\"] for _,_,d in G.edges(data=True)])\n",
    "    wmin, wmax = float(es.min()), float(es.max())\n",
    "    def scale(x, a, b):\n",
    "        return 0.0 if wmax == wmin else (a + (x - wmin) * (b - a) / (wmax - wmin))\n",
    "    edge_widths = [scale(w, 0.5, 6.0) for w in es]\n",
    "    edge_alphas = [scale(w, 0.15, 0.6) for w in es]\n",
    "\n",
    "    # ---- draw\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.9)\n",
    "    for alpha in sorted(set(round(a,2) for a in edge_alphas)):\n",
    "        idx = [i for i,a in enumerate(edge_alphas) if round(a,2) == alpha]\n",
    "        nx.draw_networkx_edges(\n",
    "            G, pos,\n",
    "            edgelist=[list(G.edges())[i] for i in idx],\n",
    "            width=[edge_widths[i] for i in idx],\n",
    "            alpha=alpha,\n",
    "            arrows=True, arrowsize=8, arrowstyle=\"-|>\",\n",
    "            connectionstyle=\"arc3,rad=0.05\"\n",
    "        )\n",
    "\n",
    "    # ---- label only top nodes by strength (shortened)\n",
    "    def short(addr: str, k: int = 6) -> str:\n",
    "        return addr if len(addr) <= 2*k+1 else f\"{addr[:k]}…{addr[-k:]}\"\n",
    "    top_nodes = sorted(G.nodes(), key=lambda n: G.nodes[n][\"weight\"], reverse=True)[:label_top]\n",
    "    lbl_pos = {n: pos[n] for n in top_nodes}\n",
    "    lbls = {n: short(n) for n in top_nodes}\n",
    "    nx.draw_networkx_labels(G, lbl_pos, labels=lbls, font_size=9, font_weight=\"bold\")\n",
    "\n",
    "    title_side = \" (bipartite)\" if layout == \"bipartite\" else \"\"\n",
    "    plt.title(f\"Flows from Top Sources → Top Destinations{title_side}\\n\"\n",
    "              f\"drawn edges: {len(e):,}  nodes: {G.number_of_nodes():,}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches=\"tight\", dpi=dpi)\n",
    "        print(f\"[plot] saved to {save_path}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6aa6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Run\n",
    "# ============================\n",
    "\n",
    "print(f\"[config] START_DAY={SAFER_START}  VALUE_UNITS={VALUE_UNITS}  CHUNK_SIZE={CHUNK_SIZE}\")\n",
    "print(\"[smoke] file counts:\", {k: len(glob.glob(v)) for k, v in patterns.items()})\n",
    "dataset_smoke_check(max_files=8)  # quick visibility\n",
    "\n",
    "TOP_SOURCES = set(topk_sources_chunked(TOPK_SRC))\n",
    "if not TOP_SOURCES:\n",
    "    raise RuntimeError(\"Could not determine top sources (verify prevout linkage, columns, and START_DAY).\")\n",
    "\n",
    "build_top_outpoints_dataset(list(TOP_SOURCES))\n",
    "# Quick visibility into available index prefixes\n",
    "have_pfx = list_index_prefixes()\n",
    "print(f\"[post-index] have {len(have_pfx)} pfx dirs (sample: {sorted(list(have_pfx))[:16]})\")\n",
    "\n",
    "edges_df = build_flows_top_sources_streaming()\n",
    "try: display(edges_df.head(20))\n",
    "except: print(edges_df.head(20))\n",
    "\n",
    "# Cleaner graph defaults (tweak as you like)\n",
    "plot_flow_graph(\n",
    "    edges_df,\n",
    "    sources=TOP_SOURCES,\n",
    "    layout=\"bipartite\",      # or \"kk\" / \"spring\" / \"circular\"\n",
    "    max_edges=250,           # draw only top-N edges\n",
    "    min_edge_btc=max(1.0, FINAL_MIN_EDGE_BTC),\n",
    "    label_top=60,            # label only strongest nodes\n",
    "    min_node_strength=0.0,   # hide very-weak nodes if you set >0\n",
    "    figsize=(24, 14),\n",
    "    dpi=180,\n",
    "    save_path=None           # or a path like \"flows_bipartite.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dfd4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# VIZ UTILS — pruning + alternates (Sankey / Heatmap)\n",
    "# ============================\n",
    "\n",
    "def _short(addr: str, k: int = 6) -> str:\n",
    "    return addr if len(addr) <= 2*k+1 else f\"{addr[:k]}…{addr[-k:]}\"\n",
    "\n",
    "def condense_edges_for_viz(\n",
    "    edges: pd.DataFrame,\n",
    "    keep_per_source: int = 6,\n",
    "    share_per_source: float = 0.9,   # keep edges until this cumulative share per src\n",
    "    min_edge_btc: float = 0.0,\n",
    "    add_other: bool = True,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reduce clutter by limiting the number of outgoing edges per source.\n",
    "    Keeps top-N and enough edges to reach share_per_source for each src.\n",
    "    Optionally rolls the remainder into a single 'Other (src)' sink.\n",
    "    \"\"\"\n",
    "    if edges.empty:\n",
    "        return edges.copy()\n",
    "\n",
    "    e = edges[edges[\"btc\"] >= min_edge_btc].copy()\n",
    "    out_rows = []\n",
    "    for src, g in e.groupby(\"src\", sort=False):\n",
    "        g = g.sort_values(\"btc\", ascending=False).reset_index(drop=True)\n",
    "        total = g[\"btc\"].sum()\n",
    "        if total <= 0:\n",
    "            continue\n",
    "        g[\"cum_share\"] = g[\"btc\"].cumsum() / total\n",
    "        keep_mask = (g.index < keep_per_source) | (g[\"cum_share\"] <= share_per_source)\n",
    "        kept = g[keep_mask]\n",
    "        out_rows.append(kept[[\"src\",\"dst\",\"btc\"]])\n",
    "\n",
    "        if add_other:\n",
    "            other_sum = g.loc[~keep_mask, \"btc\"].sum()\n",
    "            if other_sum > 0:\n",
    "                out_rows.append(pd.DataFrame(\n",
    "                    {\"src\":[src], \"dst\":[f\"Other({_short(src)})\"], \"btc\":[other_sum]}\n",
    "                ))\n",
    "\n",
    "    if not out_rows:\n",
    "        return pd.DataFrame(columns=[\"src\",\"dst\",\"btc\"])\n",
    "    out = pd.concat(out_rows, ignore_index=True)\n",
    "    # Re-aggregate in case of multiple chunks adding to same (src,dst)\n",
    "    out = (out.groupby([\"src\",\"dst\"], as_index=False)[\"btc\"].sum()\n",
    "              .sort_values(\"btc\", ascending=False).reset_index(drop=True))\n",
    "    return out\n",
    "\n",
    "\n",
    "def plot_sankey_flows(\n",
    "    edges: pd.DataFrame,\n",
    "    sources: set[str] | None = None,\n",
    "    max_nodes_right: int = 80,         # cap # of right-side nodes for readability\n",
    "    keep_per_source: int = 6,          # also applied before building the Sankey\n",
    "    share_per_source: float = 0.9,\n",
    "    min_edge_btc: float = 0.0,\n",
    "    file_html: str | None = \"flows_sankey.html\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot an alluvial/Sankey diagram (great for dense bipartite flows).\n",
    "    Saves an interactive HTML by default.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import plotly.graph_objects as go\n",
    "    except Exception as e:\n",
    "        print(\"[sankey] plotly not installed; `pip install plotly` to enable. Error:\", e)\n",
    "        return\n",
    "\n",
    "    if edges.empty:\n",
    "        print(\"[sankey] no edges\"); return\n",
    "\n",
    "    # 1) prune per source\n",
    "    e = condense_edges_for_viz(edges, keep_per_source, share_per_source, min_edge_btc, add_other=True)\n",
    "\n",
    "    # 2) cap # of right nodes globally (merge tail into 'Other (dest)')\n",
    "    right_totals = e.groupby(\"dst\", as_index=False)[\"btc\"].sum().sort_values(\"btc\", ascending=False)\n",
    "    keep_right = set(right_totals.head(max_nodes_right)[\"dst\"].tolist())\n",
    "    e2 = e.copy()\n",
    "    e2.loc[~e2[\"dst\"].isin(keep_right), \"dst\"] = \"Other (dest)\"\n",
    "\n",
    "    # 3) build node list (sources left, dests right). Keep ordering stable.\n",
    "    left_nodes = sorted(set(e2[\"src\"].tolist()), key=lambda s: (s not in (sources or set()), s))\n",
    "    right_nodes = sorted(set(e2[\"dst\"].tolist()))\n",
    "    nodes = left_nodes + right_nodes\n",
    "    idx = {n:i for i,n in enumerate(nodes)}\n",
    "\n",
    "    # 4) Sankey links\n",
    "    src_idx = e2[\"src\"].map(idx).tolist()\n",
    "    dst_idx = e2[\"dst\"].map(idx).tolist()\n",
    "    vals    = e2[\"btc\"].astype(float).tolist()\n",
    "\n",
    "    # 5) labels (shortened for readability)\n",
    "    labels = [_short(n) for n in nodes]\n",
    "    colors = ([\"#2b8cbe\"]*len(left_nodes)) + ([\"#a1d99b\"]*len(right_nodes))\n",
    "\n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "        arrangement=\"snap\",\n",
    "        node=dict(label=labels, color=colors, pad=12, thickness=18),\n",
    "        link=dict(source=src_idx, target=[i+len(left_nodes) if n in right_nodes else i for i,n in zip(dst_idx, e2[\"dst\"])],\n",
    "                  value=vals, color=\"rgba(0,0,0,0.25)\")\n",
    "    )])\n",
    "\n",
    "    fig.update_layout(\n",
    "        title=f\"Flows from Top Sources → Top Destinations (Sankey)\\n\"\n",
    "              f\"links: {len(vals):,}  left nodes: {len(left_nodes)}  right nodes: {len(right_nodes)}\",\n",
    "        font=dict(size=12),\n",
    "        margin=dict(l=20, r=20, t=70, b=20),\n",
    "        height=750\n",
    "    )\n",
    "\n",
    "    if file_html:\n",
    "        fig.write_html(file_html, include_plotlyjs=\"cdn\")\n",
    "        print(f\"[sankey] wrote {file_html}\")\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def plot_flow_heatmap(\n",
    "    edges: pd.DataFrame,\n",
    "    sources: set[str] | None = None,\n",
    "    top_left: int = 25,\n",
    "    top_right: int = 60,\n",
    "    min_edge_btc: float = 0.0,\n",
    "    figsize=(18, 10),\n",
    "    dpi=140\n",
    "):\n",
    "    \"\"\"\n",
    "    Adjacency matrix heatmap of btc; excellent for scanning patterns without clutter.\n",
    "    \"\"\"\n",
    "    if edges.empty:\n",
    "        print(\"[heatmap] no edges\"); return\n",
    "\n",
    "    e = edges[edges[\"btc\"] >= min_edge_btc].copy()\n",
    "    # pick top src/dst by total volume\n",
    "    top_src = (e.groupby(\"src\", as_index=False)[\"btc\"].sum()\n",
    "                 .sort_values(\"btc\", ascending=False).head(top_left)[\"src\"].tolist())\n",
    "    top_dst = (e.groupby(\"dst\", as_index=False)[\"btc\"].sum()\n",
    "                 .sort_values(\"btc\", ascending=False).head(top_right)[\"dst\"].tolist())\n",
    "\n",
    "    M = (e[e[\"src\"].isin(top_src) & e[\"dst\"].isin(top_dst)]\n",
    "         .pivot_table(index=\"src\", columns=\"dst\", values=\"btc\", aggfunc=\"sum\", fill_value=0.0)\n",
    "         .reindex(index=top_src, columns=top_dst))\n",
    "\n",
    "    if M.empty:\n",
    "        print(\"[heatmap] nothing after filtering\"); return\n",
    "\n",
    "    plt.figure(figsize=figsize, dpi=dpi)\n",
    "    im = plt.imshow(M.values, aspect=\"auto\", interpolation=\"nearest\")\n",
    "    plt.colorbar(im, label=\"BTC\")\n",
    "    plt.yticks(range(len(M.index)), [_short(s) for s in M.index])\n",
    "    plt.xticks(range(len(M.columns)), [_short(d) for d in M.columns], rotation=90)\n",
    "    plt.title(\"Flows heatmap (src × dst)\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78d8d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Much cleaner graph: first condense, then draw bipartite\n",
    "edges_viz = condense_edges_for_viz(\n",
    "    edges_df,\n",
    "    keep_per_source=6,      # try 4–8\n",
    "    share_per_source=0.9,   # keep until 90% of each source's mass\n",
    "    min_edge_btc=1.0,\n",
    "    add_other=True\n",
    ")\n",
    "plot_flow_graph(\n",
    "    edges_viz,\n",
    "    sources=TOP_SOURCES,\n",
    "    layout=\"bipartite\",\n",
    "    max_edges=len(edges_viz),    # we've already pruned\n",
    "    min_edge_btc=0.0,\n",
    "    label_top=50,\n",
    "    figsize=(24, 12),\n",
    "    dpi=160\n",
    ")\n",
    "\n",
    "# 2) Interactive alluvial view (best when many edges):\n",
    "plot_sankey_flows(\n",
    "    edges_df,\n",
    "    sources=TOP_SOURCES,\n",
    "    max_nodes_right=80,      # tune down if still busy\n",
    "    keep_per_source=6,\n",
    "    share_per_source=0.9,\n",
    "    min_edge_btc=1.0,\n",
    "    file_html=\"flows_sankey.html\"\n",
    ")\n",
    "\n",
    "# 3) Quick pattern scan:\n",
    "plot_flow_heatmap(\n",
    "    edges_df,\n",
    "    sources=TOP_SOURCES,\n",
    "    top_left=25,\n",
    "    top_right=60,\n",
    "    min_edge_btc=1.0\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
