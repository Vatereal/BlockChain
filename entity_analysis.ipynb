{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc03378",
   "metadata": {},
   "source": [
    "## What “clusters” (entities) mean\n",
    "\n",
    "A **cluster / entity** is a **connected component** in a graph:\n",
    "\n",
    "- **Nodes = addresses**\n",
    "- **Edges = “same controller” links** inferred from transaction patterns\n",
    "\n",
    "A **cluster size** is simply: **how many addresses became connected** through these inferred links.\n",
    "\n",
    "---\n",
    "\n",
    "## Algorithm overview (step-by-step)\n",
    "\n",
    "### 0) Define two time windows\n",
    "\n",
    "Two overlapping windows are used:\n",
    "\n",
    "- **Index window** `[INDEX_START, ANALYSIS_END)`  \n",
    "  Build an outpoint index so inputs can be resolved to addresses.\n",
    "- **Analysis window** `[ANALYSIS_START, ANALYSIS_END)`  \n",
    "  Apply heuristics and add unions (one year of clustering).\n",
    "\n",
    "Why this is necessary: **Bitcoin inputs do not contain an address**.  \n",
    "An input only references a previous output (`prev_txid`, `prev_vout`). The address lives in that previous output.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Build an outpoint index (outputs → address)\n",
    "\n",
    "For each day/file in the **index window**, take rows with `dir == \"out\"` and store:\n",
    "\n",
    "- key: `(txid, n)` where `n` is output index (vout)\n",
    "- value: `address`\n",
    "\n",
    "This creates a local map: **(prev_txid, prev_vout) → address**, which allows later recovery of input addresses.\n",
    "\n",
    "---\n",
    "\n",
    "### 2) Resolve input addresses (prevouts → address)\n",
    "\n",
    "For each day/file in the **analysis window**, for every row with `dir == \"in\"`:\n",
    "\n",
    "- `spend_txid` = spending transaction id\n",
    "- `(prev_txid, prev_vout)` = the referenced output being spent\n",
    "\n",
    "Then:\n",
    "\n",
    "- count total inputs per spending tx: `input_utxo_count[spend_txid] += 1`\n",
    "- look up `(prev_txid, prev_vout)` in the outpoint index to obtain the **input address**\n",
    "- group resolved input addresses per spending tx:  \n",
    "  `inputs_by_txid[spend_txid] = {addr1, addr2, ...}`\n",
    "\n",
    "Important:\n",
    "- `input_utxo_count` counts **all inputs** (even unresolved)\n",
    "- `inputs_by_txid` contains only **resolved input addresses**\n",
    "\n",
    "---\n",
    "\n",
    "### 3) Group outputs per transaction\n",
    "\n",
    "Still within the analysis window, group `dir == \"out\"` rows by `txid` and collect:\n",
    "\n",
    "- `out_addrs: [address, ...]`\n",
    "- `out_values: [value, ...]`\n",
    "\n",
    "Now each transaction can be analyzed using:\n",
    "- input UTXO count,\n",
    "- resolved input addresses,\n",
    "- output addresses + output amounts.\n",
    "\n",
    "---\n",
    "\n",
    "### 4) Per transaction: apply filters + heuristics → add “same owner” edges\n",
    "\n",
    "For each transaction `txid` that has outputs and at least one input:\n",
    "\n",
    "#### 4.1 Skip transactions with insufficient structure\n",
    "- no outputs → skip\n",
    "- `input_utxo_count[txid] == 0` → skip\n",
    "\n",
    "#### 4.2 Skip collaborative patterns (CoinJoin-like)\n",
    "A simple CoinJoin-ish filter flags a transaction if:\n",
    "- `n_in_utxos >= 3`\n",
    "- `n_out >= 3`\n",
    "- at least 3 outputs share exactly the same amount\n",
    "\n",
    "If flagged:\n",
    "- **do not add any unions**\n",
    "- only mark outputs as “seen” (for later “new output” checks)\n",
    "\n",
    "Goal: avoid linking unrelated participants.\n",
    "\n",
    "#### 4.3 Multi-input heuristic (high-signal link)\n",
    "If `n_in_utxos >= 2`:\n",
    "- take the **resolved unique input addresses** for this tx\n",
    "- if there are ≥2 distinct resolved input addresses → connect (union) them all\n",
    "- if only 1 resolved input address → mark it as participating, but no additional links can be created\n",
    "\n",
    "Interpretation: spending multiple inputs in one transaction usually requires control over all corresponding private keys (or custody over them), so they are treated as belonging to the same entity.\n",
    "\n",
    "#### 4.4 Change heuristic (riskier link)\n",
    "Attempt to identify exactly one output that is likely “change” back to the spender:\n",
    "\n",
    "Candidate output must satisfy all:\n",
    "- output address is **not** one of the resolved input addresses (not a self-send)\n",
    "- output address type matches the **majority input type** (prefix-based: `1...`, `3...`, `bc1...`)\n",
    "- output address is **new as an output** (has not appeared before as an output)\n",
    "\n",
    "If *exactly one* candidate remains:\n",
    "- label it as change\n",
    "- connect (union) it to an input address (e.g., the first resolved input)\n",
    "\n",
    "Finally:\n",
    "- mark all outputs as “seen” to support future “new output” tests.\n",
    "\n",
    "---\n",
    "\n",
    "### 5) After scanning all files: compress unions → entity IDs → cluster sizes\n",
    "\n",
    "After processing the entire analysis window:\n",
    "\n",
    "1) compute each address’s Union-Find root\n",
    "2) assign a contiguous `entity_id` per root\n",
    "3) compute cluster sizes (e.g., via `bincount(entity_id)`)\n",
    "4) report:\n",
    "   - largest cluster size and fraction\n",
    "   - top-K cluster sizes\n",
    "   - quantiles (median/90th/99th)\n",
    "   - heuristic coverage (how many nodes were linked by which heuristic)\n",
    "5) optionally write mapping `(address → entity_id)` to Parquet (ideally in chunks)\n",
    "\n",
    "---\n",
    "\n",
    "## How to interpret “less permissive union” results\n",
    "\n",
    "The typical outcome is **heavy-tailed**:\n",
    "\n",
    "- many clusters of size 1–10\n",
    "- fewer clusters of size 10–1000\n",
    "- very few clusters of size 10k+\n",
    "\n",
    "Sometimes there is a **giant component** (largest cluster far larger than the rest).  \n",
    "This can be:\n",
    "- **plausible**: large custodial/service wallets consolidating many deposits\n",
    "- **a red flag**: false change links “bridge” unrelated clusters, causing cascade merges\n",
    "\n",
    "Tightening change inference typically causes:\n",
    "- fewer unions,\n",
    "- more fragmentation (more clusters),\n",
    "- reduced dominance of the largest cluster (if change links were the main bridge source).\n",
    "\n",
    "If the largest cluster stays huge even with strict change rules, it is often driven by multi-input consolidation patterns of large services, or by collaborative transactions not caught by the filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9e0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Reporting helpers (all prints live in functions below)\n",
    "# =============================================================================\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "OUTPUT_DIR_VALUES = {\"out\", \"vout\", \"output\", \"o\"}\n",
    "INPUT_DIR_VALUES  = {\"in\", \"vin\", \"input\", \"i\"}\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Sanity checks + prevout join sanity\n",
    "# -----------------------------\n",
    "\n",
    "def run_sanity_checks(\n",
    "    n_nodes: int,\n",
    "    node_to_entity: list[int] | np.ndarray | None = None,\n",
    "    cluster_size_counter: Counter | None = None,\n",
    "    prevout_lookups: int | None = None,\n",
    "    prevout_hits: int | None = None,\n",
    "    top_k: int = 20,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prints:\n",
    "      - largest cluster fraction of nodes\n",
    "      - top cluster sizes\n",
    "      - entity count stats\n",
    "      - optional prevout hit-rate\n",
    "    Provide either node_to_entity OR cluster_size_counter.\n",
    "    \"\"\"\n",
    "    if cluster_size_counter is None:\n",
    "        if node_to_entity is None:\n",
    "            raise ValueError(\"Provide either node_to_entity or cluster_size_counter.\")\n",
    "        cluster_size_counter = Counter(list(node_to_entity))\n",
    "\n",
    "    sizes = list(cluster_size_counter.values())\n",
    "    if not sizes:\n",
    "        print(\"[SANITY] No clusters found (sizes empty).\", flush=True)\n",
    "        return\n",
    "\n",
    "    sizes_sorted = sorted(sizes, reverse=True)\n",
    "    total_nodes_from_sizes = sum(sizes_sorted)\n",
    "    largest = sizes_sorted[0]\n",
    "    frac = largest / total_nodes_from_sizes if total_nodes_from_sizes else float(\"nan\")\n",
    "\n",
    "    print(\"\\n[SANITY] Cluster summary\", flush=True)\n",
    "    print(f\"  UF nodes (n_nodes): {n_nodes:,}\", flush=True)\n",
    "    print(f\"  Total nodes from cluster sizes: {total_nodes_from_sizes:,}\", flush=True)\n",
    "    if total_nodes_from_sizes != n_nodes:\n",
    "        print(\"  [WARN] sum(cluster_sizes) != n_nodes  -> mismatch suggests a bug in mapping logic.\", flush=True)\n",
    "    print(f\"  Entities (clusters): {len(cluster_size_counter):,}\", flush=True)\n",
    "    print(f\"  Largest cluster size: {largest:,}\", flush=True)\n",
    "    print(f\"  Largest cluster fraction of nodes: {frac:.2%}\", flush=True)\n",
    "\n",
    "    print(f\"\\n[SANITY] Top {top_k} cluster sizes:\", flush=True)\n",
    "    print(\" \", sizes_sorted[:top_k], flush=True)\n",
    "\n",
    "    def pct(p: float) -> int:\n",
    "        s_asc = sorted(sizes_sorted)\n",
    "        idx = max(0, min(len(s_asc) - 1, math.ceil(p * len(s_asc)) - 1))\n",
    "        return int(s_asc[idx])\n",
    "\n",
    "    med = int(sorted(sizes_sorted)[len(sizes_sorted) // 2])\n",
    "    print(\"\\n[SANITY] Quick distribution stats\", flush=True)\n",
    "    print(f\"  Median cluster size: {med:,}\", flush=True)\n",
    "    print(f\"  90th percentile cluster size: {pct(0.90):,}\", flush=True)\n",
    "    print(f\"  99th percentile cluster size: {pct(0.99):,}\", flush=True)\n",
    "\n",
    "    if prevout_lookups is not None and prevout_hits is not None:\n",
    "        rate = (prevout_hits / prevout_lookups) if prevout_lookups else float(\"nan\")\n",
    "        print(\"\\n[SANITY] Prevout lookup hit-rate (DB)\", flush=True)\n",
    "        print(f\"  Lookups:  {prevout_lookups:,}\", flush=True)\n",
    "        print(f\"  Hits:     {prevout_hits:,}\", flush=True)\n",
    "        print(f\"  Hit-rate: {rate:.2%}\", flush=True)\n",
    "\n",
    "\n",
    "def prevout_join_sanity_polars(one_io_parquet: str) -> None:\n",
    "    \"\"\"\n",
    "    Pure-Polars sanity on a single parquet:\n",
    "      - prev_vout integer-likeness\n",
    "      - duplicate outpoints (txid,n)\n",
    "      - left join vin->vout inside same parquet\n",
    "      - match explosion check\n",
    "    Note: This join only resolves prevouts that point to outputs present in this same file.\n",
    "    Your real pipeline resolves against the outpoint DB across days.\n",
    "    \"\"\"\n",
    "    df = pl.read_parquet(one_io_parquet, columns=[\"dir\",\"txid\",\"n\",\"prev_txid\",\"prev_vout\",\"address\",\"value\"])\n",
    "    df = df.with_columns(pl.col(\"dir\").cast(pl.Utf8, strict=False).str.to_lowercase().alias(\"dir\"))\n",
    "\n",
    "    vout = (\n",
    "        df.filter(pl.col(\"dir\").is_in(list(OUTPUT_DIR_VALUES)))\n",
    "          .select([\n",
    "              pl.col(\"txid\").alias(\"out_txid\"),\n",
    "              pl.col(\"n\").cast(pl.Int64, strict=False).alias(\"out_n\"),\n",
    "              pl.col(\"address\").alias(\"out_address\"),\n",
    "              pl.col(\"value\").alias(\"out_value\"),\n",
    "          ])\n",
    "          .filter(pl.col(\"out_txid\").is_not_null() & pl.col(\"out_n\").is_not_null())\n",
    "    )\n",
    "\n",
    "    vin = (\n",
    "        df.filter(pl.col(\"dir\").is_in(list(INPUT_DIR_VALUES)))\n",
    "          .select([\n",
    "              pl.col(\"txid\").alias(\"spend_txid\"),\n",
    "              pl.col(\"prev_txid\").alias(\"prev_txid\"),\n",
    "              pl.col(\"prev_vout\").alias(\"prev_vout\"),\n",
    "          ])\n",
    "          .filter(pl.col(\"spend_txid\").is_not_null() & pl.col(\"prev_txid\").is_not_null())\n",
    "    )\n",
    "\n",
    "    print(\"\\n[PREVOUT-SANITY | POLARS]\", flush=True)\n",
    "    print(\"File:\", one_io_parquet, flush=True)\n",
    "    print(\"Rows:\", {\"vin\": vin.height, \"vout\": vout.height}, flush=True)\n",
    "\n",
    "    vin_nonnull = vin.filter(pl.col(\"prev_vout\").is_not_null())\n",
    "    if vin_nonnull.height > 0:\n",
    "        frac_integerlike = (\n",
    "            vin_nonnull\n",
    "            .with_columns(((pl.col(\"prev_vout\") - pl.col(\"prev_vout\").floor()).abs() < 1e-9).alias(\"is_intlike\"))\n",
    "            .select(pl.col(\"is_intlike\").mean())\n",
    "            .item()\n",
    "        )\n",
    "        print(f\"prev_vout integer-like fraction: {float(frac_integerlike):.6f}\", flush=True)\n",
    "\n",
    "    vin2 = vin.with_columns(\n",
    "        pl.when(pl.col(\"prev_vout\").is_not_null())\n",
    "          .then(pl.col(\"prev_vout\").cast(pl.Int64, strict=False))\n",
    "          .otherwise(None)\n",
    "          .alias(\"prev_vout_i64\")\n",
    "    )\n",
    "\n",
    "    dupe_outpoints = (\n",
    "        vout.group_by([\"out_txid\",\"out_n\"])\n",
    "            .len()\n",
    "            .filter(pl.col(\"len\") > 1)\n",
    "            .height\n",
    "    )\n",
    "    print(\"Duplicate outpoints (should be 0):\", dupe_outpoints, flush=True)\n",
    "\n",
    "    joined = (\n",
    "        vin2.join(\n",
    "            vout,\n",
    "            left_on=[\"prev_txid\",\"prev_vout_i64\"],\n",
    "            right_on=[\"out_txid\",\"out_n\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    resolved = joined.filter(pl.col(\"out_address\").is_not_null()).height\n",
    "    lookups  = vin.height\n",
    "    print(f\"Prevout resolution hit-rate (within-file): {resolved}/{lookups} = {resolved/lookups:.2%}\" if lookups else \"No vin rows\", flush=True)\n",
    "\n",
    "    if joined.height > 0:\n",
    "        max_matches_per_input = (\n",
    "            joined.group_by([\"spend_txid\",\"prev_txid\",\"prev_vout_i64\"])\n",
    "                  .len()\n",
    "                  .select(pl.col(\"len\").max())\n",
    "                  .item()\n",
    "        )\n",
    "        print(\"Max matches per (spend_txid, prev_txid, prev_vout):\", int(max_matches_per_input) if max_matches_per_input is not None else None, flush=True)\n",
    "\n",
    "    sample_unresolved = (\n",
    "        joined.filter(pl.col(\"out_address\").is_null())\n",
    "              .select([\"spend_txid\",\"prev_txid\",\"prev_vout_i64\"])\n",
    "              .head(5)\n",
    "    )\n",
    "    print(\"\\nSample unresolved inputs (within-file):\", flush=True)\n",
    "    print(sample_unresolved, flush=True)\n",
    "\n",
    "\n",
    "def prevout_join_sanity_db(\n",
    "    conn: sqlite3.Connection,\n",
    "    one_io_parquet: str,\n",
    "    *,\n",
    "    sample_limit: int,\n",
    "    hybrid_threshold: int,\n",
    "    join_chunk: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Sanity-check using your ACTUAL outpoint DB resolution:\n",
    "      - reads vin from a single parquet\n",
    "      - casts prev_vout -> int64\n",
    "      - looks up in DB using lookup_outpoints_hybrid(conn, keys, hybrid_threshold, join_chunk)\n",
    "      - prints DB hit-rate and sample unresolved keys\n",
    "    Uses only a sample of inputs (sample_limit) to keep it cheap.\n",
    "    \"\"\"\n",
    "    df = pl.read_parquet(one_io_parquet, columns=[\"dir\",\"txid\",\"prev_txid\",\"prev_vout\"])\n",
    "    df = df.with_columns(pl.col(\"dir\").cast(pl.Utf8, strict=False).str.to_lowercase().alias(\"dir\"))\n",
    "\n",
    "    vin = (\n",
    "        df.filter(pl.col(\"dir\").is_in(list(INPUT_DIR_VALUES)))\n",
    "          .select([\n",
    "              pl.col(\"txid\").alias(\"spend_txid\"),\n",
    "              pl.col(\"prev_txid\").alias(\"prev_txid\"),\n",
    "              pl.col(\"prev_vout\").alias(\"prev_vout\"),\n",
    "          ])\n",
    "          .filter(pl.col(\"prev_txid\").is_not_null() & pl.col(\"prev_vout\").is_not_null())\n",
    "          .head(sample_limit)\n",
    "    )\n",
    "\n",
    "    print(\"\\n[PREVOUT-SANITY | DB]\", flush=True)\n",
    "    print(\"File:\", one_io_parquet, flush=True)\n",
    "    print(\"Vin sample rows:\", vin.height, flush=True)\n",
    "\n",
    "    if vin.height == 0:\n",
    "        print(\"No vin rows in sample.\", flush=True)\n",
    "        return\n",
    "\n",
    "    frac_integerlike = (\n",
    "        vin.with_columns(((pl.col(\"prev_vout\") - pl.col(\"prev_vout\").floor()).abs() < 1e-9).alias(\"is_intlike\"))\n",
    "           .select(pl.col(\"is_intlike\").mean())\n",
    "           .item()\n",
    "    )\n",
    "    print(f\"prev_vout integer-like fraction (sample): {float(frac_integerlike):.6f}\", flush=True)\n",
    "\n",
    "    vin2 = vin.with_columns(\n",
    "        pl.col(\"prev_vout\").cast(pl.Int64, strict=False).alias(\"prev_vout_i64\")\n",
    "    ).filter(pl.col(\"prev_vout_i64\").is_not_null())\n",
    "\n",
    "    keys = [\n",
    "        (prev_txid, int(prev_n))\n",
    "        for prev_txid, prev_n in vin2.select([\"prev_txid\", \"prev_vout_i64\"]).iter_rows(named=False, buffer_size=200_000)\n",
    "    ]\n",
    "    if not keys:\n",
    "        print(\"No usable keys after casting prev_vout.\", flush=True)\n",
    "        return\n",
    "\n",
    "    # NOTE: pass both knobs (your current version was missing them)\n",
    "    hits = lookup_outpoints_hybrid(conn, keys, hybrid_threshold=hybrid_threshold, join_chunk=join_chunk)\n",
    "    resolved = len(hits)\n",
    "    lookups = len(keys)\n",
    "\n",
    "    print(f\"DB prevout hit-rate (sample): {resolved}/{lookups} = {resolved/lookups:.2%}\", flush=True)\n",
    "\n",
    "    unresolved = []\n",
    "    for k in keys[:5000]:\n",
    "        if k not in hits:\n",
    "            unresolved.append(k)\n",
    "            if len(unresolved) >= 10:\n",
    "                break\n",
    "    if unresolved:\n",
    "        print(\"Sample unresolved (prev_txid, prev_n):\", flush=True)\n",
    "        for k in unresolved:\n",
    "            print(\" \", k, flush=True)\n",
    "    else:\n",
    "        print(\"No unresolved keys in first 5000 keys (good sign).\", flush=True)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Evidence composition helpers\n",
    "# -----------------------------\n",
    "\n",
    "def compute_top_cluster_evidence_composition(\n",
    "    *,\n",
    "    node_to_entity: np.ndarray,\n",
    "    multi_change_flags: bytearray,\n",
    "    top_entity_id: int,\n",
    ") -> dict[str, float | int]:\n",
    "    \"\"\"\n",
    "    Evidence bits (same semantics as address_confidence.parquet evidence_bits):\n",
    "      bit 1 -> multi-input evidence\n",
    "      bit 2 -> change-output evidence\n",
    "      bit 4 -> change-anchor evidence\n",
    "\n",
    "    Returns counts + shares for the top cluster, including bit2-only share.\n",
    "    \"\"\"\n",
    "    n_nodes = int(node_to_entity.size)\n",
    "    if n_nodes == 0:\n",
    "        return {\n",
    "            \"top_entity_id\": int(top_entity_id),\n",
    "            \"n_top\": 0,\n",
    "            \"n_bit1\": 0, \"n_bit2\": 0, \"n_bit4\": 0,\n",
    "            \"n_bit2_only\": 0,\n",
    "            \"share_bit1\": float(\"nan\"),\n",
    "            \"share_bit2\": float(\"nan\"),\n",
    "            \"share_bit4\": float(\"nan\"),\n",
    "            \"share_bit2_only\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    if len(multi_change_flags) < n_nodes:\n",
    "        raise ValueError(f\"multi_change_flags length {len(multi_change_flags)} < n_nodes {n_nodes}\")\n",
    "\n",
    "    bits = np.frombuffer(multi_change_flags, dtype=np.uint8, count=n_nodes)\n",
    "    mask = (node_to_entity == int(top_entity_id))\n",
    "    n_top = int(mask.sum())\n",
    "    if n_top == 0:\n",
    "        return {\n",
    "            \"top_entity_id\": int(top_entity_id),\n",
    "            \"n_top\": 0,\n",
    "            \"n_bit1\": 0, \"n_bit2\": 0, \"n_bit4\": 0,\n",
    "            \"n_bit2_only\": 0,\n",
    "            \"share_bit1\": float(\"nan\"),\n",
    "            \"share_bit2\": float(\"nan\"),\n",
    "            \"share_bit4\": float(\"nan\"),\n",
    "            \"share_bit2_only\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    top_bits = bits[mask]\n",
    "    n_bit1 = int(np.count_nonzero(top_bits & 1))\n",
    "    n_bit2 = int(np.count_nonzero(top_bits & 2))\n",
    "    n_bit4 = int(np.count_nonzero(top_bits & 4))\n",
    "    n_bit2_only = int(np.count_nonzero(top_bits == 2))\n",
    "\n",
    "    share_bit1 = n_bit1 / n_top\n",
    "    share_bit2 = n_bit2 / n_top\n",
    "    share_bit4 = n_bit4 / n_top\n",
    "    share_bit2_only = n_bit2_only / n_top\n",
    "\n",
    "    return {\n",
    "        \"top_entity_id\": int(top_entity_id),\n",
    "        \"n_top\": n_top,\n",
    "        \"n_bit1\": n_bit1,\n",
    "        \"n_bit2\": n_bit2,\n",
    "        \"n_bit4\": n_bit4,\n",
    "        \"n_bit2_only\": n_bit2_only,\n",
    "        \"share_bit1\": float(share_bit1),\n",
    "        \"share_bit2\": float(share_bit2),\n",
    "        \"share_bit4\": float(share_bit4),\n",
    "        \"share_bit2_only\": float(share_bit2_only),\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Final report printer (calls sanity if requested)\n",
    "# -----------------------------\n",
    "\n",
    "def print_final_report(\n",
    "    *,\n",
    "    n_nodes: int,\n",
    "    n_entities: int,\n",
    "    node_to_entity: np.ndarray,\n",
    "    counts: np.ndarray,\n",
    "    cluster_sizes: np.ndarray,\n",
    "    stats: dict[str, int],\n",
    "    multi_change_flags: bytearray,\n",
    "    outputs: OutputConfig,\n",
    "    guards: MergeGuardParams,\n",
    "    enable_constraints_diag: bool,\n",
    "    constraint_unique_pairs: set[tuple[int, int]],\n",
    "    constraint_gate_evals_total: int,\n",
    "    constraint_pairs_repeated: int,\n",
    "    change_merge_votes: dict[tuple[int, int], int],\n",
    "    prevout_lookups: int,\n",
    "    prevout_hits: int,\n",
    "\n",
    "    # sanity hooks\n",
    "    run_sanity: bool,\n",
    "    sanity_top_k: int,\n",
    "    run_prevout_db_sanity: bool,\n",
    "    conn: sqlite3.Connection | None,\n",
    "    sanity_parquet: str | None,\n",
    "    prevout_sanity_sample_limit: int,\n",
    "    prevout_hybrid_threshold: int,\n",
    "    prevout_lookup_chunk: int,\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    All final prints centralized here.\n",
    "    Optionally prints sanity checks and DB prevout sanity.\n",
    "    Returns top_entity_id (largest cluster).\n",
    "    \"\"\"\n",
    "    print(\"\\nFinalizing entity mapping (compressing components)...\", flush=True)\n",
    "    print(f\"Number of unique addresses with UF nodes: {n_nodes:,}\", flush=True)\n",
    "\n",
    "    hit_rate = prevout_hits / max(1, prevout_lookups)\n",
    "    print(f\"[INFO] prevout lookups: {prevout_lookups:,}  hits: {prevout_hits:,}  hit-rate: {hit_rate:.3%}\", flush=True)\n",
    "    print(f\"Number of entities (clusters): {n_entities:,}\", flush=True)\n",
    "\n",
    "    if n_entities == 0:\n",
    "        print(\"[WARN] No entities computed.\", flush=True)\n",
    "        return -1\n",
    "\n",
    "    # Top clusters\n",
    "    top_k = min(outputs.top_k_clusters_print, n_entities)\n",
    "    top_ids = np.argpartition(counts, -top_k)[-top_k:]\n",
    "    top_ids_sorted = top_ids[np.argsort(counts[top_ids])[::-1]]\n",
    "    top_entity_id = int(top_ids_sorted[0])\n",
    "\n",
    "    print(\"\\n[TOP CLUSTERS]\", flush=True)\n",
    "    for rank, eid in enumerate(top_ids_sorted, start=1):\n",
    "        sz = int(counts[eid])\n",
    "        frac = (sz / n_nodes) if n_nodes else float(\"nan\")\n",
    "        print(f\"  #{rank:02d}  entity_id={int(eid):>8d}  size={sz:>10,d}  frac={frac:>7.2%}\", flush=True)\n",
    "\n",
    "    # Cluster stats\n",
    "    largest = int(cluster_sizes.max()) if cluster_sizes.size else 0\n",
    "    largest_frac = (largest / n_nodes) if n_nodes else float(\"nan\")\n",
    "    q50, q90, q99 = np.quantile(cluster_sizes, [0.5, 0.9, 0.99]) if cluster_sizes.size else (0, 0, 0)\n",
    "\n",
    "    print(\"\\n[CLUSTER STATS]\", flush=True)\n",
    "    print(f\"  Entities: {n_entities:,}\", flush=True)\n",
    "    print(f\"  Largest cluster size: {largest:,}\", flush=True)\n",
    "    print(f\"  Largest cluster fraction of nodes: {largest_frac:.2%}\", flush=True)\n",
    "    print(f\"  Median cluster size: {int(q50)}\", flush=True)\n",
    "    print(f\"  90th percentile: {int(q90)}\", flush=True)\n",
    "    print(f\"  99th percentile: {int(q99)}\", flush=True)\n",
    "\n",
    "    # Evidence composition inside top cluster\n",
    "    comp = compute_top_cluster_evidence_composition(\n",
    "        node_to_entity=node_to_entity,\n",
    "        multi_change_flags=multi_change_flags,\n",
    "        top_entity_id=top_entity_id,\n",
    "    )\n",
    "\n",
    "    print(\"\\n[TOP CLUSTER EVIDENCE COMPOSITION]\", flush=True)\n",
    "    print(f\"  Top entity_id: {comp['top_entity_id']:,}\", flush=True)\n",
    "    print(f\"  Addresses in top cluster: {comp['n_top']:,}\", flush=True)\n",
    "    print(f\"  bit1 (multi-input)   : {comp['n_bit1']:,}  share={comp['share_bit1']:.2%}\", flush=True)\n",
    "    print(f\"  bit2 (change-output) : {comp['n_bit2']:,}  share={comp['share_bit2']:.2%}\", flush=True)\n",
    "    print(f\"  bit4 (change-anchor) : {comp['n_bit4']:,}  share={comp['share_bit4']:.2%}\", flush=True)\n",
    "    print(f\"  bit2-only            : {comp['n_bit2_only']:,}  share={comp['share_bit2_only']:.2%}\", flush=True)\n",
    "    print(\"  (collapse signature often: very high bit2-only share + low bit1 share)\", flush=True)\n",
    "\n",
    "    # Heuristic coverage (node-level)\n",
    "    flags_view = multi_change_flags[:n_nodes]\n",
    "    n_addrs_multi = sum(1 for v in flags_view if (v & 1))\n",
    "    n_addrs_change_out = sum(1 for v in flags_view if (v & 2))\n",
    "    n_addrs_change_anchor = sum(1 for v in flags_view if (v & 4))\n",
    "    n_addrs_touched = sum(1 for v in flags_view if (v & 7))\n",
    "\n",
    "    print(\"\\n[HEURISTIC COVERAGE (node-level)]\", flush=True)\n",
    "    print(f\"  Total txs processed (>=1 input UTXO): {stats['n_txs_total']:,}\", flush=True)\n",
    "    print(f\"  Mixing-like skipped: {stats['n_txs_coinjoin_flagged']:,}\", flush=True)\n",
    "    print(f\"  Multi-input txs (SAFE policy applied): {stats['n_txs_with_multiinput']:,}\", flush=True)\n",
    "    print(f\"  Change detected (tight): {stats['n_txs_with_change_detected']:,}\", flush=True)\n",
    "    print(f\"  Nodes marked multi-input: {n_addrs_multi:,}\", flush=True)\n",
    "    print(f\"  Nodes marked change-output: {n_addrs_change_out:,}\", flush=True)\n",
    "    print(f\"  Nodes marked change-anchor: {n_addrs_change_anchor:,}\", flush=True)\n",
    "    print(f\"  Nodes touched by any heuristic: {n_addrs_touched:,}\", flush=True)\n",
    "\n",
    "    # Union diagnostics\n",
    "    print(\"\\n[UNION DIAGNOSTICS]\", flush=True)\n",
    "    print(f\"  Union attempts: {stats['union_attempts']:,}\", flush=True)\n",
    "    print(f\"  Unions applied: {stats['unions_applied']:,}\", flush=True)\n",
    "    print(f\"    - applied via H1:     {stats['unions_applied_h1']:,}\", flush=True)\n",
    "    print(f\"    - applied via CHANGE: {stats['unions_applied_change']:,}\", flush=True)\n",
    "\n",
    "    print(f\"  skip_same_component (already merged): {stats['skip_same_component']:,}\", flush=True)\n",
    "    print(f\"    - same_component via H1:     {stats['skip_same_component_h1']:,}\", flush=True)\n",
    "    print(f\"    - same_component via CHANGE: {stats['skip_same_component_change']:,}\", flush=True)\n",
    "\n",
    "    print(f\"  Unions skipped by guards/votes/constraints: {stats['unions_skipped']:,}\", flush=True)\n",
    "    print(f\"    - skipped (H1):     {stats['unions_skipped_h1']:,}\", flush=True)\n",
    "    print(f\"    - skipped (CHANGE): {stats['unions_skipped_change']:,}\", flush=True)\n",
    "\n",
    "    print(\"  Skip breakdown:\", flush=True)\n",
    "    print(f\"    - skip_abs_cap: {stats['skip_abs_cap']:,}  (H1={stats['skip_abs_cap_h1']:,}, CHANGE={stats['skip_abs_cap_change']:,})\", flush=True)\n",
    "    print(f\"    - skip_ratio_guard: {stats['skip_ratio_guard']:,}  (CHANGE-only)\", flush=True)\n",
    "    print(f\"    - skip_constraint: {stats['skip_constraint']:,}  (CHANGE-only; vote pending)\", flush=True)\n",
    "    print(f\"    - skip_degree_guard: {stats['skip_degree_guard']:,}  (CHANGE-only)\", flush=True)\n",
    "    print(f\"    - skip_vote_pending: {stats['skip_vote_pending']:,}\", flush=True)\n",
    "    print(f\"    - skip_vote_failed:  {stats['skip_vote_failed']:,}\", flush=True)\n",
    "\n",
    "    # Constraint diagnostics (optional)\n",
    "    if enable_constraints_diag:\n",
    "        print(\"\\n[CONSTRAINT DIAGNOSTICS]\", flush=True)\n",
    "        print(f\"  Vote-gating pairs seen (unique): {len(constraint_unique_pairs):,}\", flush=True)\n",
    "        print(f\"  Vote-gating evaluations (total): {constraint_gate_evals_total:,}\", flush=True)\n",
    "        print(f\"  Vote-gating pairs that repeated: {constraint_pairs_repeated:,}\", flush=True)\n",
    "        active_pairs = sum(1 for _k, v in change_merge_votes.items() if v > 0)\n",
    "        print(f\"  Vote pairs tracked (total): {active_pairs:,}\", flush=True)\n",
    "\n",
    "    # sanity checks invoked here\n",
    "    if run_sanity:\n",
    "        run_sanity_checks(\n",
    "            n_nodes=n_nodes,\n",
    "            node_to_entity=node_to_entity,\n",
    "            prevout_lookups=prevout_lookups,\n",
    "            prevout_hits=prevout_hits,\n",
    "            top_k=sanity_top_k,\n",
    "        )\n",
    "\n",
    "    if run_prevout_db_sanity:\n",
    "        if conn is None or sanity_parquet is None:\n",
    "            print(\"[SANITY] prevout DB sanity requested but conn or sanity_parquet is None; skipping.\", flush=True)\n",
    "        else:\n",
    "            prevout_join_sanity_db(\n",
    "                conn,\n",
    "                sanity_parquet,\n",
    "                sample_limit=prevout_sanity_sample_limit,\n",
    "                hybrid_threshold=prevout_hybrid_threshold,\n",
    "                join_chunk=prevout_lookup_chunk,\n",
    "            )\n",
    "\n",
    "    return top_entity_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0ed893",
   "metadata": {},
   "source": [
    "### PathsConfig\n",
    "\n",
    "- `parquet_io_glob = \"/media/vatereal/Main/parquet/io/day=*/io-*.parquet\"`\n",
    "  1) **Name + value set:** `parquet_io_glob = \"/media/vatereal/Main/parquet/io/day=*/io-*.parquet\"`\n",
    "  2) **Reflects:** Input discovery pattern (which parquet files exist in-scope).\n",
    "  3) **Update higher/lower:** Not numeric; change when dataset layout/naming changes (different root, partitions, filenames).\n",
    "\n",
    "- `outputs_dir = \"/media/vatereal/Main/outputs\"`\n",
    "  1) **Name + value set:** `outputs_dir = \"/media/vatereal/Main/outputs\"`\n",
    "  2) **Reflects:** Where all artifacts are written (SQLite DB, parquet mapping, logs).\n",
    "  3) **Update higher/lower:** Not numeric; move to faster storage (NVMe/SSD) for major speedups in SQLite + Parquet writes.\n",
    "\n",
    "- `outpoint_db_path = \"/media/vatereal/Main/outputs/outpoints_2014.sqlite\"`\n",
    "  1) **Name + value set:** `outpoint_db_path = outputs_dir / \"outpoints_2014.sqlite\"`\n",
    "  2) **Reflects:** SQLite cache of outpoints used for prevout resolution (drives hit-rate and speed).\n",
    "  3) **Update higher/lower:** Not numeric; keep year/window-specific; place on fast disk; avoid reusing across different settings.\n",
    "\n",
    "- `entity_map_out_path = \"/media/vatereal/Main/outputs/entities_multiinput_change_2014.parquet\"`\n",
    "  1) **Name + value set:** `entity_map_out_path = outputs_dir / \"entities_multiinput_change_2014.parquet\"`\n",
    "  2) **Reflects:** Final artifact: `address -> entity_id` mapping output.\n",
    "  3) **Update higher/lower:** Not numeric; version outputs when heuristics/guards change to preserve comparability.\n",
    "\n",
    "---\n",
    "\n",
    "### AnalysisConfig\n",
    "\n",
    "- `analysis_start = date(2014, 1, 1)`\n",
    "  1) **Name + value set:** `analysis_start = 2014-01-01`\n",
    "  2) **Reflects:** Inclusive start of clustering window.\n",
    "  3) **Update higher/lower:** Not numeric; move earlier/later to target a different time period.\n",
    "\n",
    "- `analysis_end_exclusive = date(2015, 1, 1)`\n",
    "  1) **Name + value set:** `analysis_end_exclusive = 2015-01-01 (exclusive)`\n",
    "  2) **Reflects:** Exclusive end of clustering window.\n",
    "  3) **Update higher/lower:** Not numeric; set to `year+1` for 1-year runs; extend for multi-year runs.\n",
    "\n",
    "- `outpoint_db_lookback_days = 365`\n",
    "  1) **Name + value set:** `outpoint_db_lookback_days = 365`\n",
    "  2) **Reflects:** How far back to preload outputs so analysis inputs can resolve prevouts.\n",
    "  3) **Update higher/lower:**\n",
    "     - **Higher:** improves prevout hit-rate / change eligibility when UTXOs are old.\n",
    "     - **Lower:** faster preload but more unresolved inputs (more skipped txs).\n",
    "\n",
    "- `value_unit_mode = \"infer\"`\n",
    "  1) **Name + value set:** `value_unit_mode = \"infer\"`\n",
    "  2) **Reflects:** Whether `value` is interpreted as BTC vs sats (infer by magnitude).\n",
    "  3) **Update higher/lower:**\n",
    "     - Use `\"btc\"` / `\"sats\"` if schema is known (safer).\n",
    "     - Keep `\"infer\"` only if dataset is consistent and you trust inference.\n",
    "\n",
    "---\n",
    "\n",
    "### OutputConfig\n",
    "\n",
    "- `reset_outpoint_db = True`\n",
    "  1) **Name + value set:** `reset_outpoint_db = True`\n",
    "  2) **Reflects:** Whether DB is rebuilt from scratch (reproducibility).\n",
    "  3) **Update higher/lower:**\n",
    "     - **False:** only when reusing a DB built from the exact same input/window/lookback/settings.\n",
    "     - **True:** when iterating heuristics or debugging.\n",
    "\n",
    "- `write_entity_mapping = True`\n",
    "  1) **Name + value set:** `write_entity_mapping = True`\n",
    "  2) **Reflects:** Whether to write `address -> entity_id` parquet.\n",
    "  3) **Update higher/lower:**\n",
    "     - **False:** faster diagnostics runs.\n",
    "     - **True:** final outputs.\n",
    "\n",
    "- `entity_map_compression = \"zstd\"`\n",
    "  1) **Name + value set:** `entity_map_compression = \"zstd\"`\n",
    "  2) **Reflects:** Output size vs write speed tradeoff.\n",
    "  3) **Update higher/lower:**\n",
    "     - `\"snappy\"` for faster writes / larger file.\n",
    "     - `\"zstd\"` for smaller files / usually fine speed.\n",
    "\n",
    "- `entity_write_batch = 1_000_000`\n",
    "  1) **Name + value set:** `entity_write_batch = 1,000,000`\n",
    "  2) **Reflects:** Rows buffered per parquet write (speed vs memory).\n",
    "  3) **Update higher/lower:**\n",
    "     - **Higher:** fewer write calls (faster) if RAM allows.\n",
    "     - **Lower:** reduces memory spikes.\n",
    "\n",
    "- `top_k_clusters_print = 20`\n",
    "  1) **Name + value set:** `top_k_clusters_print = 20`\n",
    "  2) **Reflects:** Log verbosity for top clusters.\n",
    "  3) **Update higher/lower:** Higher for debugging; lower for cleaner logs.\n",
    "\n",
    "---\n",
    "\n",
    "### PlotConfig\n",
    "\n",
    "- `enable_plots = True`\n",
    "  1) **Name + value set:** `enable_plots = True`\n",
    "  2) **Reflects:** Whether to run matplotlib diagnostics (no clustering effect).\n",
    "  3) **Update higher/lower:** False for headless/throughput runs; True when iterating heuristics.\n",
    "\n",
    "- `zipf_top_k = 200_000`\n",
    "  1) **Name + value set:** `zipf_top_k = 200,000`\n",
    "  2) **Reflects:** How many top clusters appear in the rank-size plot.\n",
    "  3) **Update higher/lower:** Higher for more tail detail; lower if plotting is slow.\n",
    "\n",
    "- `focus_max_size = 2048`\n",
    "  1) **Name + value set:** `focus_max_size = 2048`\n",
    "  2) **Reflects:** Upper cutoff for the “zoomed” histogram region.\n",
    "  3) **Update higher/lower:** Lower to zoom in on small wallets; higher to inspect mid-sized clusters.\n",
    "\n",
    "- `log_bins = 40`\n",
    "  1) **Name + value set:** `log_bins = 40`\n",
    "  2) **Reflects:** Histogram resolution in log-space.\n",
    "  3) **Update higher/lower:** Higher for finer detail (can get noisy); lower for smoother plots.\n",
    "\n",
    "---\n",
    "\n",
    "### PerfConfig\n",
    "\n",
    "- `outpoint_commit_every_rows = 500_000`\n",
    "  1) **Name + value set:** `outpoint_commit_every_rows = 500,000`\n",
    "  2) **Reflects:** SQLite commit granularity (throughput vs stall/WAL behavior).\n",
    "  3) **Update higher/lower:**\n",
    "     - **Higher:** fewer commits (often faster) if stable.\n",
    "     - **Lower:** smaller transactions if WAL growth/stalls appear.\n",
    "\n",
    "- `prevout_hybrid_threshold = 5_000`\n",
    "  1) **Name + value set:** `prevout_hybrid_threshold = 5,000`\n",
    "  2) **Reflects:** Legacy lookup mode switch (mostly relevant to sanity helper).\n",
    "  3) **Update higher/lower:** Usually leave as-is under Patch 3; only matters if legacy path is used.\n",
    "\n",
    "- `prevout_lookup_chunk = 50_000`\n",
    "  1) **Name + value set:** `prevout_lookup_chunk = 50,000`\n",
    "  2) **Reflects:** Chunk size for legacy join buffer (sanity helper).\n",
    "  3) **Update higher/lower:** Increase if using legacy path and RAM allows; decrease if SQLite temp memory pressure.\n",
    "\n",
    "- `iter_buffer_in = 200_000`\n",
    "  1) **Name + value set:** `iter_buffer_in = 200,000`\n",
    "  2) **Reflects:** Polars→Python iteration chunk size for VIN extraction.\n",
    "  3) **Update higher/lower:** Higher for speed if RAM allows; lower to reduce memory spikes.\n",
    "\n",
    "- `iter_buffer_out = 200_000`\n",
    "  1) **Name + value set:** `iter_buffer_out = 200,000`\n",
    "  2) **Reflects:** Polars→Python iteration chunk size for VOUT/outpoint indexing.\n",
    "  3) **Update higher/lower:** Same as `iter_buffer_in`.\n",
    "\n",
    "- `iter_buffer_grouped = 50_000`\n",
    "  1) **Name + value set:** `iter_buffer_grouped = 50,000`\n",
    "  2) **Reflects:** Iteration chunk size for per-tx joined rows.\n",
    "  3) **Update higher/lower:** Higher for speed if stable; lower if joined rows are heavy.\n",
    "\n",
    "- `gc_every_n_files = 100`\n",
    "  1) **Name + value set:** `gc_every_n_files = 100`\n",
    "  2) **Reflects:** How often to trigger GC to control RSS.\n",
    "  3) **Update higher/lower:** Lower if memory creeps upward; higher if GC pauses are noticeable.\n",
    "\n",
    "- `tqdm_mininterval = 2.0`\n",
    "  1) **Name + value set:** `tqdm_mininterval = 2.0`\n",
    "  2) **Reflects:** Min seconds between tqdm refreshes.\n",
    "  3) **Update higher/lower:** Higher reduces overhead; lower increases UI responsiveness.\n",
    "\n",
    "- `tqdm_miniters = 50`\n",
    "  1) **Name + value set:** `tqdm_miniters = 50`\n",
    "  2) **Reflects:** Min iterations between tqdm refreshes.\n",
    "  3) **Update higher/lower:** Higher reduces overhead; lower updates more frequently.\n",
    "\n",
    "- `tqdm_postfix_every = 250`\n",
    "  1) **Name + value set:** `tqdm_postfix_every = 250`\n",
    "  2) **Reflects:** How often postfix status is updated.\n",
    "  3) **Update higher/lower:** Higher reduces overhead; lower gives more live context.\n",
    "\n",
    "- `use_sql_prevout_join = True`\n",
    "  1) **Name + value set:** `use_sql_prevout_join = True`\n",
    "  2) **Reflects:** Patch 3: set-based vinbuf→outpoints join in SQLite.\n",
    "  3) **Update higher/lower:** Keep True for this build’s correctness/perf; False not supported in this patched version.\n",
    "\n",
    "- `vinbuf_insert_chunk = 1_000_000`\n",
    "  1) **Name + value set:** `vinbuf_insert_chunk = 1,000,000`\n",
    "  2) **Reflects:** Batch size for inserting VIN rows into SQLite temp table.\n",
    "  3) **Update higher/lower:** Higher for speed if RAM allows; lower if temp memory pressure/stalls.\n",
    "\n",
    "- `vinbuf_fetch_chunk = 1_000_000`\n",
    "  1) **Name + value set:** `vinbuf_fetch_chunk = 1,000,000`\n",
    "  2) **Reflects:** Batch size for fetching aggregated join results from SQLite.\n",
    "  3) **Update higher/lower:** Higher reduces fetch loops; lower reduces peak memory during transport.\n",
    "\n",
    "---\n",
    "\n",
    "### SanityConfig\n",
    "\n",
    "- `run_sanity_checks = True`\n",
    "  1) **Name + value set:** `run_sanity_checks = True`\n",
    "  2) **Reflects:** Prints cluster summaries and prevout hit-rate diagnostics.\n",
    "  3) **Update higher/lower:** Disable for max throughput once stable; enable during tuning.\n",
    "\n",
    "- `run_prevout_db_sanity = True`\n",
    "  1) **Name + value set:** `run_prevout_db_sanity = True`\n",
    "  2) **Reflects:** Additional sampled prevout hit-rate test.\n",
    "  3) **Update higher/lower:** Disable to save time once DB logic is trusted.\n",
    "\n",
    "- `prevout_sanity_sample_limit = 200_000`\n",
    "  1) **Name + value set:** `prevout_sanity_sample_limit = 200,000`\n",
    "  2) **Reflects:** Sample size (confidence vs time).\n",
    "  3) **Update higher/lower:** Higher for stronger confidence; lower for quick smoke checks.\n",
    "\n",
    "- `prevout_sanity_parquet = None`\n",
    "  1) **Name + value set:** `prevout_sanity_parquet = None`\n",
    "  2) **Reflects:** Which parquet file is used for sanity sampling (None = auto-selected).\n",
    "  3) **Update higher/lower:** Not numeric; set explicitly to compare runs consistently.\n",
    "\n",
    "---\n",
    "\n",
    "### HeuristicToggles\n",
    "\n",
    "- `enable_coinjoin_filter = True`\n",
    "  1) **Name + value set:** `enable_coinjoin_filter = True`\n",
    "  2) **Reflects:** Whether mixing-like transactions are excluded from clustering.\n",
    "  3) **Update higher/lower:** Disable for maximum linkage (more false merges); enable for conservative clustering.\n",
    "\n",
    "- `enable_multi_input = True`\n",
    "  1) **Name + value set:** `enable_multi_input = True`\n",
    "  2) **Reflects:** Whether H1 multi-input unions are applied.\n",
    "  3) **Update higher/lower:** Disable to study change-only behavior; enable for additional linkage.\n",
    "\n",
    "- `enable_change = True`\n",
    "  1) **Name + value set:** `enable_change = True`\n",
    "  2) **Reflects:** Whether change heuristic unions are applied.\n",
    "  3) **Update higher/lower:** Disable to study pure H1; enable for stronger wallet growth linkage.\n",
    "\n",
    "- `enable_merge_guards = True`\n",
    "  1) **Name + value set:** `enable_merge_guards = True`\n",
    "  2) **Reflects:** Whether guardrails/caps/ratio/votes are enforced.\n",
    "  3) **Update higher/lower:** Disable only for experiments; enable to avoid catastrophic bridging.\n",
    "\n",
    "- `precreate_nodes_for_all_output_addrs = False`\n",
    "  1) **Name + value set:** `precreate_nodes_for_all_output_addrs = False`\n",
    "  2) **Reflects:** Whether every output address gets a UF node regardless of later resolution.\n",
    "  3) **Update higher/lower:** True for maximal node universe; False to reduce UF size/RAM.\n",
    "\n",
    "- `create_nodes_for_all_resolved_inputs = True`\n",
    "  1) **Name + value set:** `create_nodes_for_all_resolved_inputs = True`\n",
    "  2) **Reflects:** Preserve node universe for all resolved input addresses.\n",
    "  3) **Update higher/lower:** Keep True for comparability/completeness; False only to reduce memory.\n",
    "\n",
    "---\n",
    "\n",
    "### HeuristicParams\n",
    "\n",
    "- `dust_sats = 546`\n",
    "  1) **Name + value set:** `dust_sats = 546`\n",
    "  2) **Reflects:** Dust cutoff for “spendable” output logic.\n",
    "  3) **Update higher/lower:** Higher if dust spam pollutes patterns; lower rarely.\n",
    "\n",
    "- `max_fee_abs_sats = 50_000_000`\n",
    "  1) **Name + value set:** `max_fee_abs_sats = 50,000,000`\n",
    "  2) **Reflects:** Absolute fee sanity cap for change detection.\n",
    "  3) **Update higher/lower:** Higher if high-fee regimes; lower for stricter filtering.\n",
    "\n",
    "- `max_fee_frac = 0.05`\n",
    "  1) **Name + value set:** `max_fee_frac = 0.05`\n",
    "  2) **Reflects:** Fee cap as fraction of total input value.\n",
    "  3) **Update higher/lower:** Higher if fee spikes expected; lower to be conservative.\n",
    "\n",
    "- `max_change_inputs_utxos = 10`\n",
    "  1) **Name + value set:** `max_change_inputs_utxos = 10`\n",
    "  2) **Reflects:** Max input UTXOs to allow change heuristic.\n",
    "  3) **Update higher/lower:** Higher includes consolidations (riskier); lower makes change stricter.\n",
    "\n",
    "- `max_change_spendable_outs = 2`\n",
    "  1) **Name + value set:** `max_change_spendable_outs = 2`\n",
    "  2) **Reflects:** Max spendable outputs allowed for change heuristic.\n",
    "  3) **Update higher/lower:** Higher increases coverage but risk; lower reduces false positives and coverage.\n",
    "\n",
    "- `change_require_2_outputs = True`\n",
    "  1) **Name + value set:** `change_require_2_outputs = True`\n",
    "  2) **Reflects:** Require exactly 2 spendable outputs for change detection.\n",
    "  3) **Update higher/lower:** False to allow 3-output logic; True for tightest behavior.\n",
    "\n",
    "- `multi_input_policy = \"one_output\"`\n",
    "  1) **Name + value set:** `multi_input_policy = \"one_output\"`\n",
    "  2) **Reflects:** H1 fires only if there is exactly 1 spendable output (very conservative).\n",
    "  3) **Update higher/lower:** `\"one_or_two_nonmix\"` increases coverage but raises false-positive risk.\n",
    "\n",
    "- `enable_optimal_change_constraint = True`\n",
    "  1) **Name + value set:** `enable_optimal_change_constraint = True`\n",
    "  2) **Reflects:** Extra constraint intended to mimic coin selection optimality.\n",
    "  3) **Update higher/lower:** Disable if too restrictive; enable for conservative change inference.\n",
    "\n",
    "- `optimal_change_slack_sats = 0`\n",
    "  1) **Name + value set:** `optimal_change_slack_sats = 0`\n",
    "  2) **Reflects:** Tolerance around optimal-change inequality.\n",
    "  3) **Update higher/lower:** Increase slightly if near-miss rejects due to rounding/fees; keep 0 for strictness.\n",
    "\n",
    "---\n",
    "\n",
    "### MergeGuardParams\n",
    "\n",
    "- `max_merged_component_size = 1_500_000`\n",
    "  1) **Name + value set:** `max_merged_component_size = 1,500,000`\n",
    "  2) **Reflects:** Hard fail-safe cap on entity size (limits worst-case damage).\n",
    "  3) **Update higher/lower:** Higher to allow very large services; lower to reduce catastrophic merge impact.\n",
    "\n",
    "- `merge_ratio_guard = True`\n",
    "  1) **Name + value set:** `merge_ratio_guard = True`\n",
    "  2) **Reflects:** Enables lopsided-merge protection on CHANGE unions.\n",
    "  3) **Update higher/lower:** Disable only if you accept bridging risk; otherwise keep enabled.\n",
    "\n",
    "- `merge_ratio_max = 200.0`\n",
    "  1) **Name + value set:** `merge_ratio_max = 200.0`\n",
    "  2) **Reflects:** Maximum `s_big/s_small` allowed when ratio guard applies.\n",
    "  3) **Update higher/lower:** Higher allows more growth (more risk); lower blocks more merges (more conservative).\n",
    "\n",
    "- `merge_ratio_big_cluster_min = 50_000`\n",
    "  1) **Name + value set:** `merge_ratio_big_cluster_min = 50,000`\n",
    "  2) **Reflects:** Only apply ratio guard after a cluster is “big”.\n",
    "  3) **Update higher/lower:** Higher delays ratio checks; lower/0 applies ratio checks earlier.\n",
    "\n",
    "- `merge_ratio_small_floor = 50`\n",
    "  1) **Name + value set:** `merge_ratio_small_floor = 50`\n",
    "  2) **Reflects:** Patch 1: ratio guard ignores tiny components (prevents freezing on singleton change).\n",
    "  3) **Update higher/lower:** Higher reduces freezing further; lower makes guard stricter against small attachments.\n",
    "\n",
    "- `enable_change_merge_votes = True`\n",
    "  1) **Name + value set:** `enable_change_merge_votes = True`\n",
    "  2) **Reflects:** Enables voting gate for big–big CHANGE merges (bridging defense).\n",
    "  3) **Update higher/lower:** Disable for maximum merge permissiveness; enable to protect large clusters.\n",
    "\n",
    "- `change_vote_min_side = 25_000`\n",
    "  1) **Name + value set:** `change_vote_min_side = 25,000`\n",
    "  2) **Reflects:** Votes trigger only if `min(sa, sb)` exceeds this (both sides big).\n",
    "  3) **Update higher/lower:** Higher limits voting to very large merges; lower protects medium clusters too.\n",
    "\n",
    "- `change_vote_merged_trigger = 0`\n",
    "  1) **Name + value set:** `change_vote_merged_trigger = 0`\n",
    "  2) **Reflects:** Optional trigger by merged size (0 = disabled).\n",
    "  3) **Update higher/lower:** Set >0 (e.g., 100k) if you want voting on huge merged results even with uneven sides.\n",
    "\n",
    "- `change_votes_required = 2`\n",
    "  1) **Name + value set:** `change_votes_required = 2`\n",
    "  2) **Reflects:** Number of repeated observations required before allowing a voted merge.\n",
    "  3) **Update higher/lower:** Higher = more conservative; lower = faster merging (riskier).\n",
    "\n",
    "- `ratio_guard_sample_n = 1000`\n",
    "  1) **Name + value set:** `ratio_guard_sample_n = 1000`\n",
    "  2) **Reflects:** How many ratio-guard events to log for diagnostics (no behavior change).\n",
    "  3) **Update higher/lower:** Higher yields more evidence (bigger logs); 0 disables logging overhead.\n",
    "\n",
    "- `ratio_guard_sample_path = \"/media/vatereal/Main/outputs/ratio_guard_samples_2014.tsv\"`\n",
    "  1) **Name + value set:** `ratio_guard_sample_path = outputs_dir / \"ratio_guard_samples_2014.tsv\"`\n",
    "  2) **Reflects:** Where diagnostic ratio-guard TSV is written.\n",
    "  3) **Update higher/lower:** Not numeric; make run-specific to avoid overwriting/comparing mixed runs.\n",
    "\n",
    "- `debug_large_change_merges = False`\n",
    "  1) **Name + value set:** `debug_large_change_merges = False`\n",
    "  2) **Reflects:** Extra logging for large CHANGE merges (debug-only).\n",
    "  3) **Update higher/lower:** Enable only when investigating suspicious merges.\n",
    "\n",
    "- `debug_change_merge_min = 50_000`\n",
    "  1) **Name + value set:** `debug_change_merge_min = 50,000`\n",
    "  2) **Reflects:** Threshold for debug log entries when debug is enabled.\n",
    "  3) **Update higher/lower:** Lower captures more events; higher captures only very large events.\n",
    "\n",
    "- `debug_change_merge_log_path = \"/media/vatereal/Main/outputs/large_change_merges_2014.tsv\"`\n",
    "  1) **Name + value set:** `debug_change_merge_log_path = outputs_dir / \"large_change_merges_2014.tsv\"`\n",
    "  2) **Reflects:** Where large-merge debug logs are written.\n",
    "  3) **Update higher/lower:** Not numeric; keep run-specific to avoid mixing logs across experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77943b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sqlite3\n",
    "import sys\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Literal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Determinism helpers\n",
    "# =============================================================================\n",
    "\n",
    "def set_determinism(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Makes the pipeline *more reproducible* across runs.\n",
    "\n",
    "    Notes:\n",
    "      - This pipeline is fundamentally deterministic given a deterministic stream order.\n",
    "      - The main nondeterminism you observed comes from:\n",
    "          * SQLite group_concat(DISTINCT ...) ordering (fixed by sorting lists post-transport)\n",
    "          * Polars unique() order (fixed by sorting before insertion)\n",
    "      - We still set seeds and env to reduce incidental variation.\n",
    "    \"\"\"\n",
    "    os.environ.setdefault(\"PYTHONHASHSEED\", str(seed))\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Config objects (NO DEFAULTS — everything must be explicit)\n",
    "# =============================================================================\n",
    "\n",
    "ValueUnitMode = Literal[\"infer\", \"btc\", \"sats\"]\n",
    "MultiInputPolicy = Literal[\"one_output\", \"one_or_two_nonmix\"]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PathsConfig:\n",
    "    parquet_io_glob: str\n",
    "    outputs_dir: Path\n",
    "    outpoint_db_path: Path\n",
    "    entity_map_out_path: Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AnalysisConfig:\n",
    "    analysis_start: date\n",
    "    analysis_end_exclusive: date\n",
    "    outpoint_db_lookback_days: int\n",
    "    value_unit_mode: ValueUnitMode\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OutputConfig:\n",
    "    reset_outpoint_db: bool\n",
    "    write_entity_mapping: bool\n",
    "    entity_map_compression: str\n",
    "    entity_write_batch: int\n",
    "    top_k_clusters_print: int\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PlotConfig:\n",
    "    enable_plots: bool\n",
    "    zipf_top_k: int\n",
    "    focus_max_size: int\n",
    "    log_bins: int\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PerfConfig:\n",
    "    outpoint_commit_every_rows: int\n",
    "    prevout_hybrid_threshold: int\n",
    "    prevout_lookup_chunk: int\n",
    "    iter_buffer_in: int\n",
    "    iter_buffer_out: int\n",
    "    iter_buffer_grouped: int\n",
    "    gc_every_n_files: int\n",
    "    tqdm_mininterval: float\n",
    "    tqdm_miniters: int\n",
    "    tqdm_postfix_every: int\n",
    "\n",
    "    # Patch 3: set-based prevout join path (SQLite join + SQLite aggregates + Polars transport)\n",
    "    use_sql_prevout_join: bool\n",
    "    vinbuf_insert_chunk: int\n",
    "    vinbuf_fetch_chunk: int\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SanityConfig:\n",
    "    run_sanity_checks: bool\n",
    "    run_prevout_db_sanity: bool\n",
    "    prevout_sanity_sample_limit: int\n",
    "    prevout_sanity_parquet: str | None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class HeuristicToggles:\n",
    "    enable_coinjoin_filter: bool\n",
    "    enable_multi_input: bool\n",
    "    enable_change: bool\n",
    "    enable_merge_guards: bool\n",
    "    precreate_nodes_for_all_output_addrs: bool\n",
    "    create_nodes_for_all_resolved_inputs: bool\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class HeuristicParams:\n",
    "    dust_sats: int\n",
    "    max_fee_abs_sats: int\n",
    "    max_fee_frac: float\n",
    "    max_change_inputs_utxos: int\n",
    "    max_change_spendable_outs: int\n",
    "    change_require_2_outputs: bool\n",
    "    multi_input_policy: MultiInputPolicy\n",
    "    enable_optimal_change_constraint: bool\n",
    "    optimal_change_slack_sats: int\n",
    "\n",
    "\n",
    "UltraVoteRule = tuple[int, int]  # (threshold, votes_required)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class MergeGuardParams:\n",
    "    # Patch A: Safety ceiling (raised; still recommended as a fuse)\n",
    "    max_merged_component_size: int\n",
    "\n",
    "    # Ratio guard (CHANGE only) + ignore tiny components (singleton change)\n",
    "    merge_ratio_guard: bool\n",
    "    merge_ratio_max: float\n",
    "    merge_ratio_big_cluster_min: int\n",
    "    merge_ratio_small_floor: int\n",
    "\n",
    "    # Big–big gating (CHANGE only): require multiple \"confirmations\" per component-pair\n",
    "    # (Patch A: strengthened tiers above 250k/500k/1M via ultra_change_vote_rules)\n",
    "    enable_change_merge_votes: bool\n",
    "    change_vote_min_side: int\n",
    "    change_vote_merged_trigger: int\n",
    "    change_votes_required: int\n",
    "    ultra_change_vote_rules: tuple[UltraVoteRule, ...]\n",
    "\n",
    "    # Patch B: alternative confirmation signal when pair repetition is rare\n",
    "    # \"change component attempts to attach to too many distinct large anchors\"\n",
    "    enable_change_degree_guard: bool\n",
    "    change_degree_large_min: int\n",
    "    change_degree_small_min: int\n",
    "    change_degree_max_distinct_large: int\n",
    "    change_degree_set_cap: int\n",
    "\n",
    "    # Diagnostics: ratio-guard sample log\n",
    "    ratio_guard_sample_n: int\n",
    "    ratio_guard_sample_path: Path\n",
    "\n",
    "    # Patch B: constraint event log (vote pending + degree guard)\n",
    "    constraint_log_n: int\n",
    "    constraint_log_path: Path\n",
    "\n",
    "    # Debug logging of large merges (optional)\n",
    "    debug_large_change_merges: bool\n",
    "    debug_change_merge_min: int\n",
    "    debug_change_merge_log_path: Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ConfidenceConfig:\n",
    "    \"\"\"\n",
    "    Proxy \"probability\" that an address is *meaningfully clustered* (diagnostic / downstream weight).\n",
    "    This does NOT claim statistical calibration; it's an evidence-based score in [0,1].\n",
    "    \"\"\"\n",
    "    enable_confidence_proxy: bool\n",
    "    confidence_out_path: Path\n",
    "    confidence_compression: str\n",
    "    confidence_write_batch: int\n",
    "\n",
    "    # scoring knobs\n",
    "    size_norm: int\n",
    "    size_bonus_max: float\n",
    "    p_singleton: float\n",
    "    p_base_0: float\n",
    "    p_base_1: float\n",
    "    p_base_2plus: float\n",
    "\n",
    "    include_cluster_size: bool\n",
    "    include_evidence_bits: bool\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Utility helpers\n",
    "# =============================================================================\n",
    "\n",
    "def extract_day_from_path(path: str) -> date | None:\n",
    "    p = Path(path)\n",
    "    for part in p.parts:\n",
    "        if part.startswith(\"day=\"):\n",
    "            day_str = part.split(\"=\", 1)[1]\n",
    "            try:\n",
    "                return date.fromisoformat(day_str)\n",
    "            except ValueError:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def addr_type(addr: str) -> str:\n",
    "    if addr.startswith(\"1\"):\n",
    "        return \"p2pkh\"\n",
    "    if addr.startswith(\"3\"):\n",
    "        return \"p2sh\"\n",
    "    if addr.startswith(\"bc1q\"):\n",
    "        return \"bech32_p2wpkh\"\n",
    "    if addr.startswith(\"bc1p\"):\n",
    "        return \"taproot\"\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "def infer_value_unit_from_sample(sample_vals: list[float]) -> str:\n",
    "    if not sample_vals:\n",
    "        return \"btc\"\n",
    "    mx = max(sample_vals)\n",
    "    return \"sats\" if mx >= 1e6 else \"btc\"\n",
    "\n",
    "\n",
    "class UnionFind:\n",
    "    \"\"\"\n",
    "    Union-Find with rank, path compression, component sizes,\n",
    "    plus Patch B metadata: per-root set of distinct \"large anchors\" that a CHANGE component tried to attach to.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.parent: list[int] = []\n",
    "        self.rank: list[int] = []\n",
    "        self.size: list[int] = []\n",
    "\n",
    "        # Patch B: root -> set(anchor_root) (only meaningful for \"change side\" components)\n",
    "        self.change_large_anchor_roots: dict[int, set[int]] = {}\n",
    "\n",
    "    def make_set(self) -> int:\n",
    "        idx = len(self.parent)\n",
    "        self.parent.append(idx)\n",
    "        self.rank.append(0)\n",
    "        self.size.append(1)\n",
    "        return idx\n",
    "\n",
    "    def find(self, x: int) -> int:\n",
    "        parent = self.parent\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "\n",
    "    def union_roots(self, rx: int, ry: int, *, change_degree_set_cap: int) -> int:\n",
    "        \"\"\"\n",
    "        Union roots (rx, ry) and return new root.\n",
    "        Merges Patch B sets deterministically, capping to change_degree_set_cap.\n",
    "        \"\"\"\n",
    "        if rx == ry:\n",
    "            return rx\n",
    "\n",
    "        parent = self.parent\n",
    "        rank = self.rank\n",
    "        size = self.size\n",
    "\n",
    "        # union by rank; deterministic tie-break already given by call order in your stream\n",
    "        if rank[rx] < rank[ry]:\n",
    "            parent[rx] = ry\n",
    "            size[ry] += size[rx]\n",
    "            new_root, old_root = ry, rx\n",
    "        elif rank[rx] > rank[ry]:\n",
    "            parent[ry] = rx\n",
    "            size[rx] += size[ry]\n",
    "            new_root, old_root = rx, ry\n",
    "        else:\n",
    "            parent[ry] = rx\n",
    "            rank[rx] += 1\n",
    "            size[rx] += size[ry]\n",
    "            new_root, old_root = rx, ry\n",
    "\n",
    "        # Patch B: merge anchor-root sets\n",
    "        sa = self.change_large_anchor_roots.get(new_root)\n",
    "        sb = self.change_large_anchor_roots.get(old_root)\n",
    "\n",
    "        if sa is None and sb is None:\n",
    "            return new_root\n",
    "\n",
    "        merged: set[int] = set()\n",
    "        if sa:\n",
    "            merged |= sa\n",
    "        if sb:\n",
    "            merged |= sb\n",
    "\n",
    "        if len(merged) > change_degree_set_cap:\n",
    "            merged = set(sorted(merged)[:change_degree_set_cap])\n",
    "\n",
    "        self.change_large_anchor_roots[new_root] = merged\n",
    "        self.change_large_anchor_roots.pop(old_root, None)\n",
    "        return new_root\n",
    "\n",
    "\n",
    "def _count_dupe_values(values: list[int], dust_sats: int) -> tuple[int, int]:\n",
    "    vals = [v for v in values if v >= dust_sats]\n",
    "    if not vals:\n",
    "        return 0, 0\n",
    "    cnt = Counter(vals)\n",
    "    return max(cnt.values()), len(cnt)\n",
    "\n",
    "\n",
    "def detect_mixing_like(n_in_utxos: int, spendable_outs: list[tuple[str, int]], dust_sats: int) -> bool:\n",
    "    # Conservative \"mixing-like\" flags: equal-amount patterns and symmetry.\n",
    "    n_out = len(spendable_outs)\n",
    "    if n_in_utxos < 3 or n_out < 3:\n",
    "        return False\n",
    "\n",
    "    out_vals = [v for (_a, v) in spendable_outs]\n",
    "    out_types = [addr_type(a) for (a, _v) in spendable_outs if a is not None]\n",
    "    unique_vals = len(set(out_vals))\n",
    "    unique_types = len(set(out_types)) if out_types else 0\n",
    "    max_dupe, distinct_vals_non_dust = _count_dupe_values(out_vals, dust_sats)\n",
    "\n",
    "    if max_dupe >= 2 and distinct_vals_non_dust >= 2:\n",
    "        return True\n",
    "    if abs(n_in_utxos - n_out) <= 1 and min(n_in_utxos, n_out) >= 3:\n",
    "        return True\n",
    "    if n_out >= 4 and unique_vals <= (n_out // 2):\n",
    "        return True\n",
    "    if n_out >= 4 and unique_types == 1 and max_dupe >= 2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def multi_input_is_safe(policy: MultiInputPolicy, n_in_utxos: int, n_spendable_out: int, is_mixing_like: bool) -> bool:\n",
    "    if n_in_utxos < 2:\n",
    "        return False\n",
    "    if policy == \"one_output\":\n",
    "        return n_spendable_out == 1\n",
    "    if policy == \"one_or_two_nonmix\":\n",
    "        if n_spendable_out == 1:\n",
    "            return True\n",
    "        if n_spendable_out == 2 and not is_mixing_like:\n",
    "            return True\n",
    "        return False\n",
    "    raise ValueError(f\"Unknown multi_input_policy={policy}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SQLite outpoint DB + TEMP join buffers\n",
    "# =============================================================================\n",
    "\n",
    "def open_outpoint_db(db_path: Path, reset: bool) -> sqlite3.Connection:\n",
    "    if reset and db_path.exists():\n",
    "        db_path.unlink()\n",
    "\n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    cur.execute(\"PRAGMA synchronous=OFF;\")\n",
    "    cur.execute(\"PRAGMA temp_store=MEMORY;\")\n",
    "    cur.execute(\"PRAGMA cache_size=-2000000;\")   # ~2GB cache (negative => KB)\n",
    "    cur.execute(\"PRAGMA wal_autocheckpoint=5000;\")\n",
    "\n",
    "    conn.isolation_level = None\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS outpoints (\n",
    "            txid TEXT NOT NULL,\n",
    "            n    INTEGER NOT NULL,\n",
    "            address TEXT NOT NULL,\n",
    "            value_sats INTEGER NOT NULL,\n",
    "            PRIMARY KEY(txid, n)\n",
    "        ) WITHOUT ROWID;\n",
    "        \"\"\"\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "\n",
    "def init_lookup_tables(conn: sqlite3.Connection) -> None:\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"DROP TABLE IF EXISTS keybuf;\")\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TEMP TABLE keybuf (\n",
    "            txid TEXT NOT NULL,\n",
    "            n    INTEGER NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    cur.execute(\"DROP TABLE IF EXISTS vinbuf;\")\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TEMP TABLE vinbuf (\n",
    "            spend_txid TEXT NOT NULL,\n",
    "            prev_txid  TEXT NOT NULL,\n",
    "            prev_n     INTEGER NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def reset_vinbuf(conn: sqlite3.Connection) -> None:\n",
    "    conn.execute(\"DELETE FROM vinbuf;\")\n",
    "\n",
    "\n",
    "def insert_outpoints_no_commit(conn: sqlite3.Connection, rows: list[tuple[str, int, str, int]]) -> None:\n",
    "    if rows:\n",
    "        conn.executemany(\n",
    "            \"INSERT OR IGNORE INTO outpoints(txid, n, address, value_sats) VALUES (?, ?, ?, ?);\",\n",
    "            rows,\n",
    "        )\n",
    "\n",
    "\n",
    "# Legacy hybrid lookup kept for sanity check helper\n",
    "def lookup_outpoints_or(conn: sqlite3.Connection, keys: list[tuple[str, int]], chunk_size: int) -> dict[tuple[str, int], tuple[str, int]]:\n",
    "    if not keys:\n",
    "        return {}\n",
    "    keys = list(dict.fromkeys(keys))\n",
    "    cur = conn.cursor()\n",
    "    out: dict[tuple[str, int], tuple[str, int]] = {}\n",
    "\n",
    "    for i in range(0, len(keys), chunk_size):\n",
    "        chunk = keys[i : i + chunk_size]\n",
    "        where = \" OR \".join([\"(txid=? AND n=?)\"] * len(chunk))\n",
    "        params = [p for k in chunk for p in k]\n",
    "        cur.execute(f\"SELECT txid, n, address, value_sats FROM outpoints WHERE {where};\", params)\n",
    "        for txid, n, address, value_sats in cur.fetchall():\n",
    "            out[(txid, int(n))] = (address, int(value_sats))\n",
    "    return out\n",
    "\n",
    "\n",
    "def lookup_outpoints_join(conn: sqlite3.Connection, keys: list[tuple[str, int]], chunk_size: int) -> dict[tuple[str, int], tuple[str, int]]:\n",
    "    if not keys:\n",
    "        return {}\n",
    "    keys = list(dict.fromkeys(keys))\n",
    "    cur = conn.cursor()\n",
    "    out: dict[tuple[str, int], tuple[str, int]] = {}\n",
    "\n",
    "    for i in range(0, len(keys), chunk_size):\n",
    "        chunk = keys[i : i + chunk_size]\n",
    "        cur.execute(\"DELETE FROM keybuf;\")\n",
    "        cur.executemany(\"INSERT INTO keybuf(txid, n) VALUES (?, ?);\", chunk)\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT k.txid, k.n, o.address, o.value_sats\n",
    "            FROM keybuf k\n",
    "            JOIN outpoints o\n",
    "              ON o.txid = k.txid AND o.n = k.n;\n",
    "            \"\"\"\n",
    "        )\n",
    "        for txid, n, address, value_sats in cur.fetchall():\n",
    "            out[(txid, int(n))] = (address, int(value_sats))\n",
    "    return out\n",
    "\n",
    "\n",
    "def lookup_outpoints_hybrid(conn: sqlite3.Connection, keys: list[tuple[str, int]], hybrid_threshold: int, join_chunk: int) -> dict[tuple[str, int], tuple[str, int]]:\n",
    "    if not keys:\n",
    "        return {}\n",
    "    if len(keys) < hybrid_threshold:\n",
    "        return lookup_outpoints_or(conn, keys, chunk_size=500)\n",
    "    return lookup_outpoints_join(conn, keys, chunk_size=join_chunk)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Patch 3: Resolve + aggregate inputs inside SQLite, transport to Polars\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_prevouts_aggs_polars(conn: sqlite3.Connection, fetch_chunk: int) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns one row per spend_txid:\n",
    "\n",
    "      txid: str\n",
    "      resolved_cnt: int\n",
    "      sum_inputs_sats: int\n",
    "      min_input_sats: int\n",
    "      in_addrs: list[str]   (DISTINCT addresses among resolved inputs for that tx)\n",
    "\n",
    "    Note: group_concat() uses commas; Bitcoin addresses do not contain commas.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        SELECT\n",
    "            v.spend_txid                                 AS txid,\n",
    "            COUNT(*)                                      AS resolved_cnt,\n",
    "            SUM(o.value_sats)                             AS sum_inputs_sats,\n",
    "            MIN(o.value_sats)                             AS min_input_sats,\n",
    "            group_concat(DISTINCT o.address)              AS in_addrs_csv\n",
    "        FROM vinbuf v\n",
    "        JOIN outpoints o\n",
    "          ON o.txid = v.prev_txid AND o.n = v.prev_n\n",
    "        GROUP BY v.spend_txid;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    dfs: list[pl.DataFrame] = []\n",
    "    while True:\n",
    "        rows = cur.fetchmany(fetch_chunk)\n",
    "        if not rows:\n",
    "            break\n",
    "        dfs.append(\n",
    "            pl.DataFrame(\n",
    "                rows,\n",
    "                schema=[\"txid\", \"resolved_cnt\", \"sum_inputs_sats\", \"min_input_sats\", \"in_addrs_csv\"],\n",
    "                orient=\"row\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if not dfs:\n",
    "        return pl.DataFrame(\n",
    "            schema={\n",
    "                \"txid\": pl.Utf8,\n",
    "                \"resolved_cnt\": pl.Int64,\n",
    "                \"sum_inputs_sats\": pl.Int64,\n",
    "                \"min_input_sats\": pl.Int64,\n",
    "                \"in_addrs\": pl.List(pl.Utf8),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pl.concat(dfs, how=\"vertical\", rechunk=True)\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"in_addrs_csv\")\n",
    "        .fill_null(\"\")\n",
    "        .str.split(\",\")\n",
    "        .alias(\"in_addrs\")\n",
    "    ).drop(\"in_addrs_csv\")\n",
    "\n",
    "    # Edge-case: NULL -> \"\" -> [\"\"] should become []\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"in_addrs\") == [\"\"])\n",
    "        .then(pl.lit([], dtype=pl.List(pl.Utf8)))\n",
    "        .otherwise(pl.col(\"in_addrs\"))\n",
    "        .alias(\"in_addrs\")\n",
    "    )\n",
    "\n",
    "    # Determinism: sort the per-tx list of input addresses\n",
    "    df = df.with_columns(pl.col(\"in_addrs\").list.sort().alias(\"in_addrs\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Reporting helpers (prints centralized below)\n",
    "# =============================================================================\n",
    "\n",
    "OUTPUT_DIR_VALUES = {\"out\", \"vout\", \"output\", \"o\"}\n",
    "INPUT_DIR_VALUES  = {\"in\", \"vin\", \"input\", \"i\"}\n",
    "\n",
    "\n",
    "def run_sanity_checks(\n",
    "    n_nodes: int,\n",
    "    node_to_entity: list[int] | np.ndarray | None = None,\n",
    "    cluster_size_counter: Counter | None = None,\n",
    "    prevout_lookups: int | None = None,\n",
    "    prevout_hits: int | None = None,\n",
    "    top_k: int = 20,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Prints:\n",
    "      - largest cluster fraction of nodes\n",
    "      - top cluster sizes\n",
    "      - entity count stats\n",
    "      - optional prevout hit-rate\n",
    "    Provide either node_to_entity OR cluster_size_counter.\n",
    "    \"\"\"\n",
    "    if cluster_size_counter is None:\n",
    "        if node_to_entity is None:\n",
    "            raise ValueError(\"Provide either node_to_entity or cluster_size_counter.\")\n",
    "        cluster_size_counter = Counter(list(node_to_entity))\n",
    "\n",
    "    sizes = list(cluster_size_counter.values())\n",
    "    if not sizes:\n",
    "        print(\"[SANITY] No clusters found (sizes empty).\", flush=True)\n",
    "        return\n",
    "\n",
    "    sizes_sorted = sorted(sizes, reverse=True)\n",
    "    total_nodes_from_sizes = sum(sizes_sorted)\n",
    "    largest = sizes_sorted[0]\n",
    "    frac = largest / total_nodes_from_sizes if total_nodes_from_sizes else float(\"nan\")\n",
    "\n",
    "    print(\"\\n[SANITY] Cluster summary\", flush=True)\n",
    "    print(f\"  UF nodes (n_nodes): {n_nodes:,}\", flush=True)\n",
    "    print(f\"  Total nodes from cluster sizes: {total_nodes_from_sizes:,}\", flush=True)\n",
    "    if total_nodes_from_sizes != n_nodes:\n",
    "        print(\"  [WARN] sum(cluster_sizes) != n_nodes  -> mismatch suggests a bug in mapping logic.\", flush=True)\n",
    "    print(f\"  Entities (clusters): {len(cluster_size_counter):,}\", flush=True)\n",
    "    print(f\"  Largest cluster size: {largest:,}\", flush=True)\n",
    "    print(f\"  Largest cluster fraction of nodes: {frac:.2%}\", flush=True)\n",
    "\n",
    "    print(f\"\\n[SANITY] Top {top_k} cluster sizes:\", flush=True)\n",
    "    print(\" \", sizes_sorted[:top_k], flush=True)\n",
    "\n",
    "    def pct(p: float) -> int:\n",
    "        s_asc = sorted(sizes_sorted)\n",
    "        idx = max(0, min(len(s_asc) - 1, math.ceil(p * len(s_asc)) - 1))\n",
    "        return int(s_asc[idx])\n",
    "\n",
    "    med = int(sorted(sizes_sorted)[len(sizes_sorted) // 2])\n",
    "    print(\"\\n[SANITY] Quick distribution stats\", flush=True)\n",
    "    print(f\"  Median cluster size: {med:,}\", flush=True)\n",
    "    print(f\"  90th percentile cluster size: {pct(0.90):,}\", flush=True)\n",
    "    print(f\"  99th percentile cluster size: {pct(0.99):,}\", flush=True)\n",
    "\n",
    "    if prevout_lookups is not None and prevout_hits is not None:\n",
    "        rate = (prevout_hits / prevout_lookups) if prevout_lookups else float(\"nan\")\n",
    "        print(\"\\n[SANITY] Prevout lookup hit-rate (DB)\", flush=True)\n",
    "        print(f\"  Lookups:  {prevout_lookups:,}\", flush=True)\n",
    "        print(f\"  Hits:     {prevout_hits:,}\", flush=True)\n",
    "        print(f\"  Hit-rate: {rate:.2%}\", flush=True)\n",
    "\n",
    "\n",
    "def prevout_join_sanity_polars(one_io_parquet: str) -> None:\n",
    "    \"\"\"\n",
    "    Pure-Polars sanity on a single parquet:\n",
    "      - prev_vout integer-likeness\n",
    "      - duplicate outpoints (txid,n)\n",
    "      - left join vin->vout inside same parquet\n",
    "      - match explosion check\n",
    "    Note: This join only resolves prevouts that point to outputs present in this same file.\n",
    "    Your real pipeline resolves against the outpoint DB across days.\n",
    "    \"\"\"\n",
    "    df = pl.read_parquet(one_io_parquet, columns=[\"dir\",\"txid\",\"n\",\"prev_txid\",\"prev_vout\",\"address\",\"value\"])\n",
    "    df = df.with_columns(pl.col(\"dir\").cast(pl.Utf8, strict=False).str.to_lowercase().alias(\"dir\"))\n",
    "\n",
    "    vout = (\n",
    "        df.filter(pl.col(\"dir\").is_in(list(OUTPUT_DIR_VALUES)))\n",
    "          .select([\n",
    "              pl.col(\"txid\").alias(\"out_txid\"),\n",
    "              pl.col(\"n\").cast(pl.Int64, strict=False).alias(\"out_n\"),\n",
    "              pl.col(\"address\").alias(\"out_address\"),\n",
    "              pl.col(\"value\").alias(\"out_value\"),\n",
    "          ])\n",
    "          .filter(pl.col(\"out_txid\").is_not_null() & pl.col(\"out_n\").is_not_null())\n",
    "    )\n",
    "\n",
    "    vin = (\n",
    "        df.filter(pl.col(\"dir\").is_in(list(INPUT_DIR_VALUES)))\n",
    "          .select([\n",
    "              pl.col(\"txid\").alias(\"spend_txid\"),\n",
    "              pl.col(\"prev_txid\").alias(\"prev_txid\"),\n",
    "              pl.col(\"prev_vout\").alias(\"prev_vout\"),\n",
    "          ])\n",
    "          .filter(pl.col(\"spend_txid\").is_not_null() & pl.col(\"prev_txid\").is_not_null())\n",
    "    )\n",
    "\n",
    "    print(\"\\n[PREVOUT-SANITY | POLARS]\", flush=True)\n",
    "    print(\"File:\", one_io_parquet, flush=True)\n",
    "    print(\"Rows:\", {\"vin\": vin.height, \"vout\": vout.height}, flush=True)\n",
    "\n",
    "    vin_nonnull = vin.filter(pl.col(\"prev_vout\").is_not_null())\n",
    "    if vin_nonnull.height > 0:\n",
    "        frac_integerlike = (\n",
    "            vin_nonnull\n",
    "            .with_columns(((pl.col(\"prev_vout\") - pl.col(\"prev_vout\").floor()).abs() < 1e-9).alias(\"is_intlike\"))\n",
    "            .select(pl.col(\"is_intlike\").mean())\n",
    "            .item()\n",
    "        )\n",
    "        print(f\"prev_vout integer-like fraction: {float(frac_integerlike):.6f}\", flush=True)\n",
    "\n",
    "    vin2 = vin.with_columns(\n",
    "        pl.when(pl.col(\"prev_vout\").is_not_null())\n",
    "          .then(pl.col(\"prev_vout\").cast(pl.Int64, strict=False))\n",
    "          .otherwise(None)\n",
    "          .alias(\"prev_vout_i64\")\n",
    "    )\n",
    "\n",
    "    dupe_outpoints = (\n",
    "        vout.group_by([\"out_txid\",\"out_n\"])\n",
    "            .len()\n",
    "            .filter(pl.col(\"len\") > 1)\n",
    "            .height\n",
    "    )\n",
    "    print(\"Duplicate outpoints (should be 0):\", dupe_outpoints, flush=True)\n",
    "\n",
    "    joined = (\n",
    "        vin2.join(\n",
    "            vout,\n",
    "            left_on=[\"prev_txid\",\"prev_vout_i64\"],\n",
    "            right_on=[\"out_txid\",\"out_n\"],\n",
    "            how=\"left\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    resolved = joined.filter(pl.col(\"out_address\").is_not_null()).height\n",
    "    lookups  = vin.height\n",
    "    print(f\"Prevout resolution hit-rate (within-file): {resolved}/{lookups} = {resolved/lookups:.2%}\" if lookups else \"No vin rows\", flush=True)\n",
    "\n",
    "    if joined.height > 0:\n",
    "        max_matches_per_input = (\n",
    "            joined.group_by([\"spend_txid\",\"prev_txid\",\"prev_vout_i64\"])\n",
    "                  .len()\n",
    "                  .select(pl.col(\"len\").max())\n",
    "                  .item()\n",
    "        )\n",
    "        print(\"Max matches per (spend_txid, prev_txid, prev_vout):\", int(max_matches_per_input) if max_matches_per_input is not None else None, flush=True)\n",
    "\n",
    "    sample_unresolved = (\n",
    "        joined.filter(pl.col(\"out_address\").is_null())\n",
    "              .select([\"spend_txid\",\"prev_txid\",\"prev_vout_i64\"])\n",
    "              .head(5)\n",
    "    )\n",
    "    print(\"\\nSample unresolved inputs (within-file):\", flush=True)\n",
    "    print(sample_unresolved, flush=True)\n",
    "\n",
    "\n",
    "def prevout_join_sanity_db(\n",
    "    conn: sqlite3.Connection,\n",
    "    one_io_parquet: str,\n",
    "    *,\n",
    "    sample_limit: int,\n",
    "    hybrid_threshold: int,\n",
    "    join_chunk: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Sanity-check using your ACTUAL outpoint DB resolution:\n",
    "      - reads vin from a single parquet\n",
    "      - casts prev_vout -> int64\n",
    "      - looks up in DB using lookup_outpoints_hybrid(conn, keys, hybrid_threshold, join_chunk)\n",
    "      - prints DB hit-rate and sample unresolved keys\n",
    "    Uses only a sample of inputs (sample_limit) to keep it cheap.\n",
    "    \"\"\"\n",
    "    df = pl.read_parquet(one_io_parquet, columns=[\"dir\",\"txid\",\"prev_txid\",\"prev_vout\"])\n",
    "    df = df.with_columns(pl.col(\"dir\").cast(pl.Utf8, strict=False).str.to_lowercase().alias(\"dir\"))\n",
    "\n",
    "    vin = (\n",
    "        df.filter(pl.col(\"dir\").is_in(list(INPUT_DIR_VALUES)))\n",
    "          .select([\n",
    "              pl.col(\"txid\").alias(\"spend_txid\"),\n",
    "              pl.col(\"prev_txid\").alias(\"prev_txid\"),\n",
    "              pl.col(\"prev_vout\").alias(\"prev_vout\"),\n",
    "          ])\n",
    "          .filter(pl.col(\"prev_txid\").is_not_null() & pl.col(\"prev_vout\").is_not_null())\n",
    "          .head(sample_limit)\n",
    "    )\n",
    "\n",
    "    print(\"\\n[PREVOUT-SANITY | DB]\", flush=True)\n",
    "    print(\"File:\", one_io_parquet, flush=True)\n",
    "    print(\"Vin sample rows:\", vin.height, flush=True)\n",
    "\n",
    "    if vin.height == 0:\n",
    "        print(\"No vin rows in sample.\", flush=True)\n",
    "        return\n",
    "\n",
    "    frac_integerlike = (\n",
    "        vin.with_columns(((pl.col(\"prev_vout\") - pl.col(\"prev_vout\").floor()).abs() < 1e-9).alias(\"is_intlike\"))\n",
    "           .select(pl.col(\"is_intlike\").mean())\n",
    "           .item()\n",
    "    )\n",
    "    print(f\"prev_vout integer-like fraction (sample): {float(frac_integerlike):.6f}\", flush=True)\n",
    "\n",
    "    vin2 = vin.with_columns(\n",
    "        pl.col(\"prev_vout\").cast(pl.Int64, strict=False).alias(\"prev_vout_i64\")\n",
    "    ).filter(pl.col(\"prev_vout_i64\").is_not_null())\n",
    "\n",
    "    keys = [\n",
    "        (prev_txid, int(prev_n))\n",
    "        for prev_txid, prev_n in vin2.select([\"prev_txid\", \"prev_vout_i64\"]).iter_rows(named=False, buffer_size=200_000)\n",
    "    ]\n",
    "    if not keys:\n",
    "        print(\"No usable keys after casting prev_vout.\", flush=True)\n",
    "        return\n",
    "\n",
    "    hits = lookup_outpoints_hybrid(conn, keys, hybrid_threshold=hybrid_threshold, join_chunk=join_chunk)\n",
    "    resolved = len(hits)\n",
    "    lookups = len(keys)\n",
    "\n",
    "    print(f\"DB prevout hit-rate (sample): {resolved}/{lookups} = {resolved/lookups:.2%}\", flush=True)\n",
    "\n",
    "    unresolved = []\n",
    "    for k in keys[:5000]:\n",
    "        if k not in hits:\n",
    "            unresolved.append(k)\n",
    "            if len(unresolved) >= 10:\n",
    "                break\n",
    "    if unresolved:\n",
    "        print(\"Sample unresolved (prev_txid, prev_n):\", flush=True)\n",
    "        for k in unresolved:\n",
    "            print(\" \", k, flush=True)\n",
    "    else:\n",
    "        print(\"No unresolved keys in first 5000 keys (good sign).\", flush=True)\n",
    "\n",
    "\n",
    "def compute_top_cluster_evidence_composition(\n",
    "    *,\n",
    "    node_to_entity: np.ndarray,\n",
    "    multi_change_flags: bytearray,\n",
    "    top_entity_id: int,\n",
    ") -> dict[str, float | int]:\n",
    "    \"\"\"\n",
    "    Evidence bits:\n",
    "      bit 1 -> multi-input evidence\n",
    "      bit 2 -> change-output evidence\n",
    "      bit 4 -> change-anchor evidence\n",
    "\n",
    "    Returns counts + shares for the top cluster, including bit2-only share.\n",
    "    \"\"\"\n",
    "    n_nodes = int(node_to_entity.size)\n",
    "    if n_nodes == 0:\n",
    "        return {\n",
    "            \"top_entity_id\": int(top_entity_id),\n",
    "            \"n_top\": 0,\n",
    "            \"n_bit1\": 0, \"n_bit2\": 0, \"n_bit4\": 0,\n",
    "            \"n_bit2_only\": 0,\n",
    "            \"share_bit1\": float(\"nan\"),\n",
    "            \"share_bit2\": float(\"nan\"),\n",
    "            \"share_bit4\": float(\"nan\"),\n",
    "            \"share_bit2_only\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    if len(multi_change_flags) < n_nodes:\n",
    "        raise ValueError(f\"multi_change_flags length {len(multi_change_flags)} < n_nodes {n_nodes}\")\n",
    "\n",
    "    bits = np.frombuffer(multi_change_flags, dtype=np.uint8, count=n_nodes)\n",
    "    mask = (node_to_entity == int(top_entity_id))\n",
    "    n_top = int(mask.sum())\n",
    "    if n_top == 0:\n",
    "        return {\n",
    "            \"top_entity_id\": int(top_entity_id),\n",
    "            \"n_top\": 0,\n",
    "            \"n_bit1\": 0, \"n_bit2\": 0, \"n_bit4\": 0,\n",
    "            \"n_bit2_only\": 0,\n",
    "            \"share_bit1\": float(\"nan\"),\n",
    "            \"share_bit2\": float(\"nan\"),\n",
    "            \"share_bit4\": float(\"nan\"),\n",
    "            \"share_bit2_only\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    top_bits = bits[mask]\n",
    "    n_bit1 = int(np.count_nonzero(top_bits & 1))\n",
    "    n_bit2 = int(np.count_nonzero(top_bits & 2))\n",
    "    n_bit4 = int(np.count_nonzero(top_bits & 4))\n",
    "    n_bit2_only = int(np.count_nonzero(top_bits == 2))\n",
    "\n",
    "    share_bit1 = n_bit1 / n_top\n",
    "    share_bit2 = n_bit2 / n_top\n",
    "    share_bit4 = n_bit4 / n_top\n",
    "    share_bit2_only = n_bit2_only / n_top\n",
    "\n",
    "    return {\n",
    "        \"top_entity_id\": int(top_entity_id),\n",
    "        \"n_top\": n_top,\n",
    "        \"n_bit1\": n_bit1,\n",
    "        \"n_bit2\": n_bit2,\n",
    "        \"n_bit4\": n_bit4,\n",
    "        \"n_bit2_only\": n_bit2_only,\n",
    "        \"share_bit1\": float(share_bit1),\n",
    "        \"share_bit2\": float(share_bit2),\n",
    "        \"share_bit4\": float(share_bit4),\n",
    "        \"share_bit2_only\": float(share_bit2_only),\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Plot helpers\n",
    "# =============================================================================\n",
    "\n",
    "def _log_bins(max_val: int, bins: int) -> np.ndarray:\n",
    "    max_val = max(1, int(max_val))\n",
    "    return np.logspace(0, math.log10(max_val), num=bins)\n",
    "\n",
    "\n",
    "def plot_hist_logbins(sizes: np.ndarray, title: str, color: str, bins: int, max_x: int | None) -> None:\n",
    "    if sizes.size == 0:\n",
    "        print(f\"[PLOT] Skipping empty plot: {title}\", flush=True)\n",
    "        return\n",
    "    if max_x is None:\n",
    "        max_x = int(sizes.max())\n",
    "    b = _log_bins(max_x, bins)\n",
    "    plt.figure(figsize=(11, 6))\n",
    "    plt.hist(sizes, bins=b, log=True, color=color, edgecolor=\"black\", linewidth=0.7)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Cluster size (addresses per entity)\")\n",
    "    plt.ylabel(\"Frequency (log scale)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.35, alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_zipf(sizes: np.ndarray, title: str, color: str, top_k: int) -> None:\n",
    "    if sizes.size == 0:\n",
    "        print(f\"[PLOT] Skipping empty plot: {title}\", flush=True)\n",
    "        return\n",
    "    top_k = min(top_k, sizes.size)\n",
    "    top = np.partition(sizes, -top_k)[-top_k:]\n",
    "    top_sorted = np.sort(top)[::-1]\n",
    "    ranks = np.arange(1, top_sorted.size + 1)\n",
    "\n",
    "    plt.figure(figsize=(11, 6))\n",
    "    plt.plot(ranks, top_sorted, color=color, linewidth=1.3)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Rank (log scale)\")\n",
    "    plt.ylabel(\"Cluster size (log scale)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.35, alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main executor (explicit-parameter entrypoint)\n",
    "# =============================================================================\n",
    "\n",
    "def run_entity_clustering(\n",
    "    paths: PathsConfig,\n",
    "    analysis: AnalysisConfig,\n",
    "    outputs: OutputConfig,\n",
    "    plots: PlotConfig,\n",
    "    perf: PerfConfig,\n",
    "    sanity: SanityConfig,\n",
    "    toggles: HeuristicToggles,\n",
    "    params: HeuristicParams,\n",
    "    guards: MergeGuardParams,\n",
    "    confidence: ConfidenceConfig,\n",
    "    *,\n",
    "    determinism_seed: int,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Address clustering with:\n",
    "      - H1 multi-input heuristic under a configurable safety policy\n",
    "      - Change heuristic (2-output default)\n",
    "      - Mixing-like filter\n",
    "      - Merge guards:\n",
    "          * absolute max component size (all unions)\n",
    "          * ratio guard ONLY for CHANGE unions (+ small-floor to avoid freezing on singletons)\n",
    "          * big–big vote-gating with ultra-large strengthening (Patch A)\n",
    "          * change-degree guard (Patch B) for bridge-y change components\n",
    "      - Patch 3:\n",
    "          * prevout resolution via set-based SQLite JOIN (vinbuf → outpoints)\n",
    "          * per-tx input aggregates computed in SQLite, transported to Polars\n",
    "      - Node universe:\n",
    "          * create UF nodes for all resolved input addresses (toggle)\n",
    "      - Diagnostics:\n",
    "          * skip reason counters\n",
    "          * ratio-guard sample log\n",
    "          * constraint event log (vote pending + degree guard) + uniqueness/repeat diagnostics (Patch B)\n",
    "      - Confidence proxy output (optional)\n",
    "    \"\"\"\n",
    "    set_determinism(determinism_seed)\n",
    "\n",
    "    try:\n",
    "        sys.stdout.reconfigure(line_buffering=True)\n",
    "        sys.stderr.reconfigure(line_buffering=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    pl.Config.set_tbl_rows(20)\n",
    "    pl.Config.set_fmt_str_lengths(80)\n",
    "\n",
    "    paths.outputs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    io_paths = sorted(glob.glob(paths.parquet_io_glob))\n",
    "    print(\"Parquet file counts:\", {\"io\": len(io_paths)}, flush=True)\n",
    "    print(f\"[INFO] Analysis window: {analysis.analysis_start} .. {analysis.analysis_end_exclusive} (exclusive)\", flush=True)\n",
    "\n",
    "    index_start = analysis.analysis_start - timedelta(days=analysis.outpoint_db_lookback_days)\n",
    "    print(f\"[INFO] Outpoint DB preload start: {index_start} .. {analysis.analysis_end_exclusive} (exclusive)\", flush=True)\n",
    "    print(f\"[INFO] Outpoint DB path: {paths.outpoint_db_path} (reset={outputs.reset_outpoint_db})\", flush=True)\n",
    "\n",
    "    eligible: list[tuple[str, date]] = []\n",
    "    for path in io_paths:\n",
    "        d = extract_day_from_path(path)\n",
    "        if d is None:\n",
    "            continue\n",
    "        if index_start <= d < analysis.analysis_end_exclusive:\n",
    "            eligible.append((path, d))\n",
    "\n",
    "    n_preload_total = sum(1 for _p, d in eligible if d < analysis.analysis_start)\n",
    "    n_analysis_total = len(eligible) - n_preload_total\n",
    "    print(f\"[INFO] Eligible files in window: {len(eligible)}  (preload={n_preload_total}, analysis={n_analysis_total})\", flush=True)\n",
    "\n",
    "    if not eligible:\n",
    "        print(\"[WARN] No eligible parquet files found for the requested window.\", flush=True)\n",
    "        return\n",
    "\n",
    "    # Pick sanity parquet\n",
    "    sanity_parquet = sanity.prevout_sanity_parquet\n",
    "    if sanity_parquet is None:\n",
    "        for p, d in eligible:\n",
    "            if analysis.analysis_start <= d < analysis.analysis_end_exclusive:\n",
    "                sanity_parquet = p\n",
    "                break\n",
    "        if sanity_parquet is None and eligible:\n",
    "            sanity_parquet = eligible[0][0]\n",
    "    if sanity.run_sanity_checks and sanity_parquet:\n",
    "        print(f\"[INFO] Sanity parquet selected: {sanity_parquet}\", flush=True)\n",
    "\n",
    "    conn = open_outpoint_db(paths.outpoint_db_path, reset=outputs.reset_outpoint_db)\n",
    "    init_lookup_tables(conn)\n",
    "\n",
    "    conn.execute(\"BEGIN;\")\n",
    "    pending_outpoint_rows = 0\n",
    "\n",
    "    uf = UnionFind()\n",
    "    addr_to_id: dict[str, int] = {}\n",
    "\n",
    "    # Flags (diagnostic)\n",
    "    # bit 1: multi-input evidence\n",
    "    # bit 2: change-output evidence\n",
    "    # bit 4: change-anchor evidence\n",
    "    multi_change_flags = bytearray()\n",
    "    seen_output_flags = bytearray()\n",
    "\n",
    "    seen_output_addrs: set[str] | None\n",
    "    if toggles.precreate_nodes_for_all_output_addrs:\n",
    "        seen_output_addrs = None\n",
    "    else:\n",
    "        seen_output_addrs = set()\n",
    "\n",
    "    def ensure_flag_capacity(idx: int) -> None:\n",
    "        needed = idx + 1\n",
    "        cur = len(seen_output_flags)\n",
    "        if cur < needed:\n",
    "            delta = needed - cur\n",
    "            seen_output_flags.extend(b\"\\x00\" * delta)\n",
    "            multi_change_flags.extend(b\"\\x00\" * delta)\n",
    "\n",
    "    def get_addr_id(addr: str) -> int:\n",
    "        idx = addr_to_id.get(addr)\n",
    "        if idx is None:\n",
    "            idx = uf.make_set()\n",
    "            addr_to_id[addr] = idx\n",
    "            ensure_flag_capacity(idx)\n",
    "        return idx\n",
    "\n",
    "    # value unit mode\n",
    "    if analysis.value_unit_mode == \"btc\":\n",
    "        value_unit: str | None = \"btc\"\n",
    "    elif analysis.value_unit_mode == \"sats\":\n",
    "        value_unit = \"sats\"\n",
    "    elif analysis.value_unit_mode == \"infer\":\n",
    "        value_unit = None\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown value_unit_mode={analysis.value_unit_mode}\")\n",
    "\n",
    "    def value_expr_to_sats() -> pl.Expr:\n",
    "        nonlocal value_unit\n",
    "        if value_unit is None:\n",
    "            return (pl.col(\"value\").cast(pl.Float64, strict=False) * 100_000_000).round(0).cast(pl.Int64, strict=False)\n",
    "        if value_unit == \"sats\":\n",
    "            return pl.col(\"value\").cast(pl.Float64, strict=False).round(0).cast(pl.Int64, strict=False)\n",
    "        return (pl.col(\"value\").cast(pl.Float64, strict=False) * 100_000_000).round(0).cast(pl.Int64, strict=False)\n",
    "\n",
    "    stats = {\n",
    "        \"n_files_indexed\": 0,\n",
    "        \"n_files_analyzed\": 0,\n",
    "        \"n_txs_total\": 0,\n",
    "        \"n_txs_coinjoin_flagged\": 0,\n",
    "        \"n_txs_with_multiinput\": 0,\n",
    "        \"n_txs_with_change_detected\": 0,\n",
    "        \"n_prevout_lookups\": 0,\n",
    "        \"n_prevout_hits\": 0,\n",
    "\n",
    "        \"union_attempts\": 0,\n",
    "        \"unions_applied\": 0,\n",
    "        \"unions_applied_h1\": 0,\n",
    "        \"unions_applied_change\": 0,\n",
    "\n",
    "        \"skip_same_component\": 0,\n",
    "        \"skip_same_component_h1\": 0,\n",
    "        \"skip_same_component_change\": 0,\n",
    "\n",
    "        \"unions_skipped\": 0,\n",
    "        \"unions_skipped_h1\": 0,\n",
    "        \"unions_skipped_change\": 0,\n",
    "\n",
    "        \"skip_abs_cap\": 0,\n",
    "        \"skip_abs_cap_h1\": 0,\n",
    "        \"skip_abs_cap_change\": 0,\n",
    "\n",
    "        \"skip_ratio_guard\": 0,\n",
    "        \"skip_ratio_guard_change\": 0,\n",
    "\n",
    "        # vote-gating\n",
    "        \"skip_constraint\": 0,\n",
    "        \"skip_constraint_change\": 0,\n",
    "\n",
    "        # Patch B: degree-guard\n",
    "        \"skip_degree_guard\": 0,\n",
    "        \"skip_degree_guard_change\": 0,\n",
    "\n",
    "        \"skip_vote_pending\": 0,\n",
    "        \"skip_vote_failed\": 0,\n",
    "    }\n",
    "\n",
    "    # vote-gating store (unordered (root_a, root_b))\n",
    "    change_merge_votes: dict[tuple[int, int], int] = {}\n",
    "\n",
    "    # Patch B: constraint diagnostics\n",
    "    constraints_added = 0\n",
    "    constraint_gate_evals_total = 0\n",
    "    constraint_pair_counts: dict[tuple[int, int], int] = {}\n",
    "    constraint_unique_pairs: set[tuple[int, int]] = set()\n",
    "    constraint_pairs_repeated = 0\n",
    "\n",
    "    # ratio-guard sample log state\n",
    "    ratio_guard_samples_written = 0\n",
    "    ratio_guard_sample_fh = None\n",
    "    try:\n",
    "        if guards.ratio_guard_sample_n > 0:\n",
    "            ratio_guard_sample_fh = open(guards.ratio_guard_sample_path, \"w\", encoding=\"utf-8\")\n",
    "            ratio_guard_sample_fh.write(\n",
    "                \"txid\\tsa\\tsb\\ts_big\\ts_small\\tratio\\tchange_side\\tbig_side\\tbig_is_change\\tchange_comp_size\\tinput_comp_size\\n\"\n",
    "            )\n",
    "    except Exception:\n",
    "        ratio_guard_sample_fh = None\n",
    "\n",
    "    # constraint log state (Patch B)\n",
    "    constraint_log_written = 0\n",
    "    constraint_log_fh = None\n",
    "    try:\n",
    "        if guards.constraint_log_n > 0:\n",
    "            constraint_log_fh = open(guards.constraint_log_path, \"w\", encoding=\"utf-8\")\n",
    "            constraint_log_fh.write(\n",
    "                \"txid\\tevent\\treason\\tsa\\tsb\\tmerged\\tmin_side\\tbig_side\\tratio\\t\"\n",
    "                \"votes\\tvotes_required\\tkey_r0\\tkey_r1\\ttriggered_by\\tbig_is_change\\t\"\n",
    "                \"pair_seen\\tis_repeat\\tchange_root\\tanchor_root\\tchange_distinct_large\\n\"\n",
    "            )\n",
    "    except Exception:\n",
    "        constraint_log_fh = None\n",
    "\n",
    "    def _votes_required_for_sizes(min_side: int, big_side: int, merged: int) -> int:\n",
    "        # base requirement\n",
    "        req = guards.change_votes_required\n",
    "        # Patch A: ultra tiers (take maximum requirement among triggered thresholds)\n",
    "        for thr, req_thr in guards.ultra_change_vote_rules:\n",
    "            if big_side >= thr or merged >= thr:\n",
    "                if req_thr > req:\n",
    "                    req = req_thr\n",
    "        return req\n",
    "\n",
    "    def _log_constraint_event(\n",
    "        *,\n",
    "        txid: str | None,\n",
    "        event: str,  # \"VOTE_PENDING\" | \"DEGREE_GUARD\"\n",
    "        reason: str,  # \"CHANGE\" or \"H1\" (here: only CHANGE)\n",
    "        sa: int,\n",
    "        sb: int,\n",
    "        merged: int,\n",
    "        min_side: int,\n",
    "        big_side: int,\n",
    "        ratio: float,\n",
    "        votes: int,\n",
    "        votes_required: int,\n",
    "        key_r0: int,\n",
    "        key_r1: int,\n",
    "        triggered_by: str,\n",
    "        big_is_change: int,\n",
    "        pair_seen: int,\n",
    "        is_repeat: int,\n",
    "        change_root: int,\n",
    "        anchor_root: int,\n",
    "        change_distinct_large: int,\n",
    "    ) -> None:\n",
    "        nonlocal constraint_log_written\n",
    "        if (\n",
    "            constraint_log_fh is None\n",
    "            or constraint_log_written >= guards.constraint_log_n\n",
    "            or txid is None\n",
    "        ):\n",
    "            return\n",
    "        constraint_log_fh.write(\n",
    "            f\"{txid}\\t{event}\\t{reason}\\t{sa}\\t{sb}\\t{merged}\\t{min_side}\\t{big_side}\\t{ratio}\\t\"\n",
    "            f\"{votes}\\t{votes_required}\\t{key_r0}\\t{key_r1}\\t{triggered_by}\\t{big_is_change}\\t\"\n",
    "            f\"{pair_seen}\\t{is_repeat}\\t{change_root}\\t{anchor_root}\\t{change_distinct_large}\\n\"\n",
    "        )\n",
    "        constraint_log_written += 1\n",
    "\n",
    "    def union_guarded(a: int, b: int, reason: Literal[\"H1\", \"CHANGE\"], txid: str | None) -> bool:\n",
    "        nonlocal ratio_guard_samples_written, constraint_log_written\n",
    "        nonlocal constraints_added, constraint_gate_evals_total, constraint_pairs_repeated\n",
    "\n",
    "        stats[\"union_attempts\"] += 1\n",
    "        ra = uf.find(a)\n",
    "        rb = uf.find(b)\n",
    "        if ra == rb:\n",
    "            stats[\"skip_same_component\"] += 1\n",
    "            if reason == \"H1\":\n",
    "                stats[\"skip_same_component_h1\"] += 1\n",
    "            else:\n",
    "                stats[\"skip_same_component_change\"] += 1\n",
    "            return False\n",
    "\n",
    "        sa = uf.size[ra]\n",
    "        sb = uf.size[rb]\n",
    "        merged = sa + sb\n",
    "\n",
    "        # No guards path\n",
    "        if not toggles.enable_merge_guards:\n",
    "            uf.union_roots(ra, rb, change_degree_set_cap=guards.change_degree_set_cap)\n",
    "            stats[\"unions_applied\"] += 1\n",
    "            if reason == \"H1\":\n",
    "                stats[\"unions_applied_h1\"] += 1\n",
    "            else:\n",
    "                stats[\"unions_applied_change\"] += 1\n",
    "            return True\n",
    "\n",
    "        # Absolute cap (all unions)\n",
    "        if merged > guards.max_merged_component_size:\n",
    "            stats[\"unions_skipped\"] += 1\n",
    "            if reason == \"H1\":\n",
    "                stats[\"unions_skipped_h1\"] += 1\n",
    "                stats[\"skip_abs_cap_h1\"] += 1\n",
    "            else:\n",
    "                stats[\"unions_skipped_change\"] += 1\n",
    "                stats[\"skip_abs_cap_change\"] += 1\n",
    "            stats[\"skip_abs_cap\"] += 1\n",
    "            return False\n",
    "\n",
    "        # Ratio guard ONLY for CHANGE (with small-floor)\n",
    "        if guards.merge_ratio_guard and reason == \"CHANGE\":\n",
    "            s_big = sa if sa >= sb else sb\n",
    "            s_small = sb if sa >= sb else sa\n",
    "\n",
    "            if s_small >= guards.merge_ratio_small_floor and s_big >= guards.merge_ratio_big_cluster_min:\n",
    "                ratio = (s_big / s_small) if s_small > 0 else float(\"inf\")\n",
    "                if ratio > guards.merge_ratio_max:\n",
    "                    stats[\"unions_skipped\"] += 1\n",
    "                    stats[\"unions_skipped_change\"] += 1\n",
    "                    stats[\"skip_ratio_guard\"] += 1\n",
    "                    stats[\"skip_ratio_guard_change\"] += 1\n",
    "\n",
    "                    # Sample log (assumes CHANGE call is (anchor=a, change=b))\n",
    "                    if (\n",
    "                        ratio_guard_sample_fh is not None\n",
    "                        and ratio_guard_samples_written < guards.ratio_guard_sample_n\n",
    "                        and txid is not None\n",
    "                    ):\n",
    "                        big_root = ra if sa >= sb else rb\n",
    "                        big_is_change = 1 if big_root == rb else 0\n",
    "                        big_side = \"b\" if big_is_change else \"a\"\n",
    "\n",
    "                        ratio_guard_sample_fh.write(\n",
    "                            f\"{txid}\\t{sa}\\t{sb}\\t{s_big}\\t{s_small}\\t{ratio}\\t\"\n",
    "                            f\"b\\t{big_side}\\t{big_is_change}\\t{sb}\\t{sa}\\n\"\n",
    "                        )\n",
    "                        ratio_guard_samples_written += 1\n",
    "                    return False\n",
    "\n",
    "        # Patch B: change-degree guard (CHANGE-only)\n",
    "        if guards.enable_change_degree_guard and reason == \"CHANGE\":\n",
    "            # Convention in pipeline: a=anchor, b=change\n",
    "            anchor_root = ra\n",
    "            change_root = rb\n",
    "            anchor_size = sa\n",
    "            change_size = sb\n",
    "\n",
    "            change_distinct_large = 0\n",
    "            if (anchor_size >= guards.change_degree_large_min) and (change_size >= guards.change_degree_small_min):\n",
    "                s = uf.change_large_anchor_roots.get(change_root)\n",
    "                if s is None:\n",
    "                    s = set()\n",
    "                    uf.change_large_anchor_roots[change_root] = s\n",
    "                s.add(anchor_root)\n",
    "                if len(s) > guards.change_degree_set_cap:\n",
    "                    uf.change_large_anchor_roots[change_root] = set(sorted(s)[:guards.change_degree_set_cap])\n",
    "                    s = uf.change_large_anchor_roots[change_root]\n",
    "                change_distinct_large = len(s)\n",
    "\n",
    "                if change_distinct_large > guards.change_degree_max_distinct_large:\n",
    "                    stats[\"unions_skipped\"] += 1\n",
    "                    stats[\"unions_skipped_change\"] += 1\n",
    "                    stats[\"skip_degree_guard\"] += 1\n",
    "                    stats[\"skip_degree_guard_change\"] += 1\n",
    "\n",
    "                    s_big = max(sa, sb)\n",
    "                    s_small = min(sa, sb)\n",
    "                    ratio = (s_big / s_small) if s_small > 0 else float(\"inf\")\n",
    "                    big_root = ra if sa >= sb else rb\n",
    "                    big_is_change = 1 if big_root == rb else 0\n",
    "                    r0, r1 = (anchor_root, change_root) if anchor_root < change_root else (change_root, anchor_root)\n",
    "\n",
    "                    _log_constraint_event(\n",
    "                        txid=txid,\n",
    "                        event=\"DEGREE_GUARD\",\n",
    "                        reason=reason,\n",
    "                        sa=sa, sb=sb,\n",
    "                        merged=merged,\n",
    "                        min_side=min(sa, sb),\n",
    "                        big_side=max(sa, sb),\n",
    "                        ratio=ratio,\n",
    "                        votes=0,\n",
    "                        votes_required=0,\n",
    "                        key_r0=r0,\n",
    "                        key_r1=r1,\n",
    "                        triggered_by=\"degree_guard\",\n",
    "                        big_is_change=big_is_change,\n",
    "                        pair_seen=0,\n",
    "                        is_repeat=0,\n",
    "                        change_root=change_root,\n",
    "                        anchor_root=anchor_root,\n",
    "                        change_distinct_large=change_distinct_large,\n",
    "                    )\n",
    "                    return False\n",
    "\n",
    "        # Big–big vote-gating ONLY for CHANGE\n",
    "        if guards.enable_change_merge_votes and reason == \"CHANGE\":\n",
    "            min_side = sa if sa <= sb else sb\n",
    "            big_side = sa if sa >= sb else sb\n",
    "\n",
    "            triggered = False\n",
    "            triggered_by = \"none\"\n",
    "\n",
    "            if guards.change_vote_min_side > 0 and min_side >= guards.change_vote_min_side:\n",
    "                triggered = True\n",
    "                triggered_by = \"min_side\"\n",
    "            if (not triggered) and guards.change_vote_merged_trigger > 0 and merged >= guards.change_vote_merged_trigger:\n",
    "                triggered = True\n",
    "                triggered_by = \"merged\"\n",
    "\n",
    "            if triggered:\n",
    "                r0, r1 = (ra, rb) if ra < rb else (rb, ra)\n",
    "                k = (r0, r1)\n",
    "\n",
    "                constraint_gate_evals_total += 1\n",
    "                prev_seen = constraint_pair_counts.get(k, 0)\n",
    "                pair_seen = prev_seen + 1\n",
    "                constraint_pair_counts[k] = pair_seen\n",
    "                if pair_seen == 1:\n",
    "                    constraints_added += 1\n",
    "                    constraint_unique_pairs.add(k)\n",
    "                elif pair_seen == 2:\n",
    "                    constraint_pairs_repeated += 1\n",
    "                is_repeat = 1 if pair_seen > 1 else 0\n",
    "\n",
    "                prev_votes = change_merge_votes.get(k, 0)\n",
    "                votes = prev_votes + 1\n",
    "                change_merge_votes[k] = votes\n",
    "\n",
    "                votes_required = _votes_required_for_sizes(min_side=min_side, big_side=big_side, merged=merged)\n",
    "\n",
    "                if votes < votes_required:\n",
    "                    stats[\"unions_skipped\"] += 1\n",
    "                    stats[\"unions_skipped_change\"] += 1\n",
    "                    stats[\"skip_constraint\"] += 1\n",
    "                    stats[\"skip_constraint_change\"] += 1\n",
    "                    stats[\"skip_vote_pending\"] += 1\n",
    "\n",
    "                    s_big = max(sa, sb)\n",
    "                    s_small = min(sa, sb)\n",
    "                    ratio = (s_big / s_small) if s_small > 0 else float(\"inf\")\n",
    "                    big_root = ra if sa >= sb else rb\n",
    "                    big_is_change = 1 if big_root == rb else 0\n",
    "\n",
    "                    _log_constraint_event(\n",
    "                        txid=txid,\n",
    "                        event=\"VOTE_PENDING\",\n",
    "                        reason=reason,\n",
    "                        sa=sa, sb=sb,\n",
    "                        merged=merged,\n",
    "                        min_side=min_side,\n",
    "                        big_side=big_side,\n",
    "                        ratio=ratio,\n",
    "                        votes=votes,\n",
    "                        votes_required=votes_required,\n",
    "                        key_r0=r0,\n",
    "                        key_r1=r1,\n",
    "                        triggered_by=triggered_by,\n",
    "                        big_is_change=big_is_change,\n",
    "                        pair_seen=pair_seen,\n",
    "                        is_repeat=is_repeat,\n",
    "                        change_root=rb,\n",
    "                        anchor_root=ra,\n",
    "                        change_distinct_large=0,\n",
    "                    )\n",
    "                    return False\n",
    "\n",
    "        # Debug log of large CHANGE merges that pass all guards (optional)\n",
    "        if guards.debug_large_change_merges and reason == \"CHANGE\" and merged >= guards.debug_change_merge_min and txid is not None:\n",
    "            try:\n",
    "                with open(guards.debug_change_merge_log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(f\"{txid}\\t{sa}\\t{sb}\\t{merged}\\n\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        uf.union_roots(ra, rb, change_degree_set_cap=guards.change_degree_set_cap)\n",
    "        stats[\"unions_applied\"] += 1\n",
    "        if reason == \"H1\":\n",
    "            stats[\"unions_applied_h1\"] += 1\n",
    "        else:\n",
    "            stats[\"unions_applied_change\"] += 1\n",
    "        return True\n",
    "\n",
    "    # tqdm: force stdout\n",
    "    pbar = tqdm(\n",
    "        eligible,\n",
    "        total=len(eligible),\n",
    "        unit=\"file\",\n",
    "        dynamic_ncols=True,\n",
    "        mininterval=perf.tqdm_mininterval,\n",
    "        miniters=perf.tqdm_miniters,\n",
    "        smoothing=0,\n",
    "        desc=\"Processing parquet\",\n",
    "        file=sys.stdout,\n",
    "        leave=True,\n",
    "    )\n",
    "\n",
    "    DIR_NEEDS_NORMALIZATION: bool | None = None\n",
    "\n",
    "    def log(msg: str) -> None:\n",
    "        tqdm.write(msg, file=sys.stdout)\n",
    "\n",
    "    def mark_outputs_seen(outputs_list: list[tuple[str | None, int]]) -> None:\n",
    "        if toggles.precreate_nodes_for_all_output_addrs:\n",
    "            for a, _v in outputs_list:\n",
    "                if a is None:\n",
    "                    continue\n",
    "                a_id = get_addr_id(a)\n",
    "                seen_output_flags[a_id] = 1\n",
    "        else:\n",
    "            assert seen_output_addrs is not None\n",
    "            for a, _v in outputs_list:\n",
    "                if a is not None:\n",
    "                    seen_output_addrs.add(a)\n",
    "\n",
    "    for i, (path, file_day) in enumerate(pbar, start=1):\n",
    "        in_analysis = (analysis.analysis_start <= file_day < analysis.analysis_end_exclusive)\n",
    "\n",
    "        if (i % perf.tqdm_postfix_every) == 0:\n",
    "            pbar.set_postfix_str(f\"{'analysis' if in_analysis else 'preload'} day={file_day}\")\n",
    "\n",
    "        if in_analysis:\n",
    "            stats[\"n_files_analyzed\"] += 1\n",
    "            if stats[\"n_files_analyzed\"] <= 5:\n",
    "                log(f\"[{stats['n_files_analyzed']}] Processing {path} (day={file_day}) ...\")\n",
    "            elif (stats[\"n_files_analyzed\"] % 250) == 0:\n",
    "                log(f\"[{stats['n_files_analyzed']}] Processing ... (day={file_day})\")\n",
    "        else:\n",
    "            stats[\"n_files_indexed\"] += 1\n",
    "            if stats[\"n_files_indexed\"] <= 3:\n",
    "                log(f\"[preload {stats['n_files_indexed']}] Indexing outputs only: {path} (day={file_day}) ...\")\n",
    "\n",
    "        df = pl.read_parquet(path, columns=[\"dir\", \"txid\", \"n\", \"prev_txid\", \"prev_vout\", \"address\", \"value\"])\n",
    "\n",
    "        if DIR_NEEDS_NORMALIZATION is None:\n",
    "            dir_uniques = df.select(pl.col(\"dir\").cast(pl.Utf8, strict=False).unique()).to_series().to_list()\n",
    "            DIR_NEEDS_NORMALIZATION = any(\n",
    "                (d is not None and str(d).lower() in (\"in\", \"out\") and str(d) not in (\"in\", \"out\")) for d in dir_uniques\n",
    "            )\n",
    "            if in_analysis and stats[\"n_files_analyzed\"] == 1:\n",
    "                log(f\"  [debug] distinct dir values in first processed file: {dir_uniques}\")\n",
    "\n",
    "        if DIR_NEEDS_NORMALIZATION:\n",
    "            df = df.with_columns(pl.col(\"dir\").cast(pl.Utf8).str.to_lowercase().alias(\"dir\"))\n",
    "\n",
    "        # Infer unit once if needed\n",
    "        if value_unit is None:\n",
    "            sample = (\n",
    "                df.filter(pl.col(\"dir\") == \"out\")\n",
    "                .select(pl.col(\"value\").cast(pl.Float64, strict=False))\n",
    "                .drop_nulls()\n",
    "                .head(5000)\n",
    "                .to_series()\n",
    "                .to_list()\n",
    "            )\n",
    "            value_unit = infer_value_unit_from_sample(sample)\n",
    "            log(f\"[INFO] Detected value unit: {value_unit}  (all internal amount logic uses satoshis)\")\n",
    "\n",
    "        # 1) Index outputs into outpoints DB\n",
    "        out_all = (\n",
    "            df.filter(pl.col(\"dir\") == \"out\")\n",
    "            .filter(pl.col(\"txid\").is_not_null())\n",
    "            .filter(pl.col(\"n\").is_not_null())\n",
    "            .select(\n",
    "                pl.col(\"txid\"),\n",
    "                pl.col(\"n\").cast(pl.Int64, strict=False).alias(\"n\"),\n",
    "                pl.col(\"address\"),\n",
    "                value_expr_to_sats().alias(\"value_sats\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        out_for_db = out_all.filter(pl.col(\"address\").is_not_null()).filter(pl.col(\"value_sats\").is_not_null())\n",
    "        if out_for_db.height > 0:\n",
    "            rows = [\n",
    "                (txid, int(n), addr, int(vs))\n",
    "                for txid, n, addr, vs in out_for_db.select([\"txid\", \"n\", \"address\", \"value_sats\"]).iter_rows(\n",
    "                    named=False, buffer_size=perf.iter_buffer_out\n",
    "                )\n",
    "            ]\n",
    "            insert_outpoints_no_commit(conn, rows)\n",
    "            pending_outpoint_rows += len(rows)\n",
    "\n",
    "            if pending_outpoint_rows >= perf.outpoint_commit_every_rows:\n",
    "                conn.execute(\"COMMIT;\")\n",
    "                conn.execute(\"BEGIN;\")\n",
    "                pending_outpoint_rows = 0\n",
    "\n",
    "            if toggles.precreate_nodes_for_all_output_addrs:\n",
    "                # Determinism: sort unique output addrs before node creation\n",
    "                out_u = out_for_db.select(pl.col(\"address\").unique()).to_series().to_list()\n",
    "                for addr in sorted(out_u):\n",
    "                    _ = get_addr_id(addr)\n",
    "\n",
    "        if not in_analysis:\n",
    "            del df, out_all, out_for_db\n",
    "            if (i % perf.gc_every_n_files) == 0:\n",
    "                gc.collect()\n",
    "            continue\n",
    "\n",
    "        # 2) Build VIN (spend_txid, prev_txid, prev_n)\n",
    "        in_df = (\n",
    "            df.filter(pl.col(\"dir\") == \"in\")\n",
    "            .filter(pl.col(\"txid\").is_not_null())\n",
    "            .filter(pl.col(\"prev_txid\").is_not_null())\n",
    "            .filter(pl.col(\"prev_vout\").is_not_null())\n",
    "            .select(\n",
    "                pl.col(\"txid\").alias(\"spend_txid\"),\n",
    "                pl.col(\"prev_txid\"),\n",
    "                pl.col(\"prev_vout\").cast(pl.Int64, strict=False).alias(\"prev_n\"),\n",
    "            )\n",
    "            .filter(pl.col(\"prev_n\").is_not_null())\n",
    "        )\n",
    "\n",
    "        del df\n",
    "        if (i % perf.gc_every_n_files) == 0:\n",
    "            gc.collect()\n",
    "\n",
    "        if in_df.height == 0:\n",
    "            del in_df\n",
    "            del out_all, out_for_db\n",
    "            if (i % perf.gc_every_n_files) == 0:\n",
    "                gc.collect()\n",
    "            continue\n",
    "\n",
    "        # n_in_utxos per spend tx\n",
    "        in_counts = (\n",
    "            in_df.group_by(\"spend_txid\")\n",
    "            .len()\n",
    "            .rename({\"spend_txid\": \"txid\", \"len\": \"n_in_utxos\"})\n",
    "        )\n",
    "\n",
    "        # 2a) Resolve prevouts via SQLite join + SQLite aggregates\n",
    "        if not perf.use_sql_prevout_join:\n",
    "            raise RuntimeError(\"perf.use_sql_prevout_join=False is not supported in this build; set it True.\")\n",
    "\n",
    "        reset_vinbuf(conn)\n",
    "\n",
    "        cur = conn.cursor()\n",
    "        insert_sql = \"INSERT INTO vinbuf(spend_txid, prev_txid, prev_n) VALUES (?, ?, ?);\"\n",
    "        batch: list[tuple[str, str, int]] = []\n",
    "        total_vin = 0\n",
    "\n",
    "        for spend_txid, prev_txid, prev_n in in_df.select([\"spend_txid\", \"prev_txid\", \"prev_n\"]).iter_rows(\n",
    "            named=False, buffer_size=perf.iter_buffer_in\n",
    "        ):\n",
    "            batch.append((str(spend_txid), str(prev_txid), int(prev_n)))\n",
    "            if len(batch) >= perf.vinbuf_insert_chunk:\n",
    "                cur.executemany(insert_sql, batch)\n",
    "                total_vin += len(batch)\n",
    "                batch.clear()\n",
    "\n",
    "        if batch:\n",
    "            cur.executemany(insert_sql, batch)\n",
    "            total_vin += len(batch)\n",
    "            batch.clear()\n",
    "\n",
    "        stats[\"n_prevout_lookups\"] += total_vin\n",
    "\n",
    "        resolved_aggs = fetch_prevouts_aggs_polars(conn, fetch_chunk=perf.vinbuf_fetch_chunk)\n",
    "\n",
    "        if resolved_aggs.height > 0:\n",
    "            resolved_inputs_sum = resolved_aggs.select(pl.col(\"resolved_cnt\").sum()).item()\n",
    "            stats[\"n_prevout_hits\"] += int(resolved_inputs_sum) if resolved_inputs_sum is not None else 0\n",
    "\n",
    "        del in_df\n",
    "\n",
    "        # Node universe patch: create nodes for ALL resolved input addresses\n",
    "        if toggles.create_nodes_for_all_resolved_inputs and resolved_aggs.height > 0:\n",
    "            u = (\n",
    "                resolved_aggs\n",
    "                .select(pl.col(\"in_addrs\").explode().drop_nulls().unique())\n",
    "                .to_series()\n",
    "                .to_list()\n",
    "            )\n",
    "            for a in sorted(u):\n",
    "                _ = get_addr_id(a)\n",
    "\n",
    "        # 3) Outputs grouped per tx\n",
    "        if out_all.height == 0:\n",
    "            del out_all, out_for_db, in_counts, resolved_aggs\n",
    "            if (i % perf.gc_every_n_files) == 0:\n",
    "                gc.collect()\n",
    "            continue\n",
    "\n",
    "        vout_grouped = (\n",
    "            out_all.group_by(\"txid\")\n",
    "            .agg(\n",
    "                pl.col(\"address\").alias(\"out_addrs_all\"),\n",
    "                pl.col(\"value_sats\").alias(\"out_values_sats_all\"),\n",
    "            )\n",
    "        )\n",
    "        del out_all, out_for_db\n",
    "\n",
    "        # Join tx-level inputs + resolved stats to outputs\n",
    "        tx_joined = (\n",
    "            vout_grouped\n",
    "            .join(in_counts, on=\"txid\", how=\"inner\")\n",
    "            .join(resolved_aggs, on=\"txid\", how=\"left\")\n",
    "        )\n",
    "        del vout_grouped, in_counts, resolved_aggs\n",
    "\n",
    "        for (\n",
    "            txid,\n",
    "            out_addrs_all,\n",
    "            out_values_all,\n",
    "            n_in_utxos,\n",
    "            resolved_cnt,\n",
    "            sum_inputs_sats,\n",
    "            min_input_sats,\n",
    "            in_addrs,\n",
    "        ) in tx_joined.select(\n",
    "            [\"txid\", \"out_addrs_all\", \"out_values_sats_all\", \"n_in_utxos\", \"resolved_cnt\", \"sum_inputs_sats\", \"min_input_sats\", \"in_addrs\"]\n",
    "        ).iter_rows(named=False, buffer_size=perf.iter_buffer_grouped):\n",
    "\n",
    "            if not out_addrs_all or not out_values_all:\n",
    "                continue\n",
    "\n",
    "            n_in_utxos_i = int(n_in_utxos) if n_in_utxos is not None else 0\n",
    "            if n_in_utxos_i == 0:\n",
    "                continue\n",
    "\n",
    "            stats[\"n_txs_total\"] += 1\n",
    "\n",
    "            outputs_list: list[tuple[str | None, int]] = []\n",
    "            sum_outputs_sats = 0\n",
    "            for a, v in zip(out_addrs_all, out_values_all):\n",
    "                if v is None:\n",
    "                    continue\n",
    "                vi = int(v)\n",
    "                outputs_list.append((a, vi))\n",
    "                sum_outputs_sats += vi\n",
    "\n",
    "            spendable = [(a, v) for (a, v) in outputs_list if a is not None and v >= params.dust_sats]\n",
    "            if not spendable:\n",
    "                mark_outputs_seen(outputs_list)\n",
    "                continue\n",
    "\n",
    "            # Mixing-like filter\n",
    "            is_mixing_like = False\n",
    "            if toggles.enable_coinjoin_filter:\n",
    "                is_mixing_like = detect_mixing_like(n_in_utxos_i, spendable, dust_sats=params.dust_sats)\n",
    "                if is_mixing_like:\n",
    "                    stats[\"n_txs_coinjoin_flagged\"] += 1\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "            if in_addrs is None:\n",
    "                mark_outputs_seen(outputs_list)\n",
    "                continue\n",
    "\n",
    "            in_addrs_list = list(in_addrs)  # already list-sorted in fetch_prevouts_aggs_polars()\n",
    "            if not in_addrs_list:\n",
    "                mark_outputs_seen(outputs_list)\n",
    "                continue\n",
    "\n",
    "            # H1 multi-input unions\n",
    "            if toggles.enable_multi_input:\n",
    "                safe = multi_input_is_safe(params.multi_input_policy, n_in_utxos_i, len(spendable), is_mixing_like)\n",
    "                if safe:\n",
    "                    stats[\"n_txs_with_multiinput\"] += 1\n",
    "                    if len(in_addrs_list) >= 2:\n",
    "                        in_ids = [get_addr_id(a) for a in in_addrs_list]\n",
    "                        for idx in in_ids:\n",
    "                            multi_change_flags[idx] |= 1\n",
    "                        first_id = in_ids[0]\n",
    "                        for idx in in_ids[1:]:\n",
    "                            union_guarded(first_id, idx, reason=\"H1\", txid=str(txid))\n",
    "                    else:\n",
    "                        idx = get_addr_id(in_addrs_list[0])\n",
    "                        multi_change_flags[idx] |= 1\n",
    "\n",
    "            # CHANGE unions\n",
    "            if toggles.enable_change:\n",
    "                n_spendable_out = len(spendable)\n",
    "\n",
    "                if params.change_require_2_outputs and n_spendable_out != 2:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "                if n_in_utxos_i > params.max_change_inputs_utxos:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "                if n_spendable_out > params.max_change_spendable_outs:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                resolved_cnt_i = int(resolved_cnt) if resolved_cnt is not None else 0\n",
    "                if resolved_cnt_i != n_in_utxos_i:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                in_types = [addr_type(a) for a in in_addrs_list]\n",
    "                if len(set(in_types)) != 1:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "                majority_type = in_types[0]\n",
    "\n",
    "                spendable_addrs = [a for (a, _v) in spendable]\n",
    "                if len(spendable_addrs) != len(set(spendable_addrs)):\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                in_addr_set_fast = set(in_addrs_list)\n",
    "                candidates: list[tuple[str, int]] = []\n",
    "                for a, v in spendable:\n",
    "                    if a in in_addr_set_fast:\n",
    "                        continue\n",
    "                    if addr_type(a) != majority_type:\n",
    "                        continue\n",
    "                    if toggles.precreate_nodes_for_all_output_addrs:\n",
    "                        a_id = get_addr_id(a)\n",
    "                        if seen_output_flags[a_id]:\n",
    "                            continue\n",
    "                    else:\n",
    "                        assert seen_output_addrs is not None\n",
    "                        if a in seen_output_addrs:\n",
    "                            continue\n",
    "                    candidates.append((a, v))\n",
    "\n",
    "                if len(candidates) != 1:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                change_addr, change_val = candidates[0]\n",
    "\n",
    "                sum_in_i = int(sum_inputs_sats) if sum_inputs_sats is not None else 0\n",
    "                fee = sum_in_i - sum_outputs_sats\n",
    "                if sum_in_i <= 0 or fee < 0:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "                if fee > params.max_fee_abs_sats or fee > int(params.max_fee_frac * sum_in_i):\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                spend_vals = [v for (_a, v) in spendable]\n",
    "                if n_spendable_out == 2:\n",
    "                    other_val = max(v for (_a, v) in spendable if _a != change_addr)\n",
    "                    if not (change_val < other_val):\n",
    "                        mark_outputs_seen(outputs_list)\n",
    "                        continue\n",
    "                elif n_spendable_out == 3:\n",
    "                    if change_val != min(spend_vals):\n",
    "                        mark_outputs_seen(outputs_list)\n",
    "                        continue\n",
    "                else:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                if params.enable_optimal_change_constraint:\n",
    "                    mn = int(min_input_sats) if min_input_sats is not None else None\n",
    "                    if mn is None:\n",
    "                        mark_outputs_seen(outputs_list)\n",
    "                        continue\n",
    "                    if change_val > (mn - fee + params.optimal_change_slack_sats):\n",
    "                        mark_outputs_seen(outputs_list)\n",
    "                        continue\n",
    "\n",
    "                change_id = get_addr_id(change_addr)\n",
    "                multi_change_flags[change_id] |= 2\n",
    "\n",
    "                anchor_id = get_addr_id(in_addrs_list[0])\n",
    "                multi_change_flags[anchor_id] |= 4\n",
    "\n",
    "                stats[\"n_txs_with_change_detected\"] += 1\n",
    "                union_guarded(anchor_id, change_id, reason=\"CHANGE\", txid=str(txid))\n",
    "\n",
    "            mark_outputs_seen(outputs_list)\n",
    "\n",
    "        del tx_joined\n",
    "        if (i % perf.gc_every_n_files) == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    pbar.close()\n",
    "    conn.execute(\"COMMIT;\")\n",
    "\n",
    "    # Close diagnostic file handles\n",
    "    try:\n",
    "        if ratio_guard_sample_fh is not None:\n",
    "            ratio_guard_sample_fh.close()\n",
    "            log(f\"[DIAG] Wrote ratio-guard sample log: {guards.ratio_guard_sample_path}  (n={ratio_guard_samples_written})\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if constraint_log_fh is not None:\n",
    "            constraint_log_fh.close()\n",
    "            log(f\"[DIAG] Wrote constraint-event log: {guards.constraint_log_path}  (n={constraint_log_written})\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Finalize entity mapping (compute only), then report via print_final_report()\n",
    "    n_nodes = len(addr_to_id)\n",
    "    if n_nodes == 0:\n",
    "        print(\"No union-find nodes created.\", flush=True)\n",
    "        conn.close()\n",
    "        return\n",
    "\n",
    "    node_to_entity = np.empty(n_nodes, dtype=np.int32)\n",
    "    root_to_entity: dict[int, int] = {}\n",
    "    next_entity_id = 0\n",
    "    for node in range(n_nodes):\n",
    "        root = uf.find(node)\n",
    "        ent = root_to_entity.get(root)\n",
    "        if ent is None:\n",
    "            ent = next_entity_id\n",
    "            root_to_entity[root] = ent\n",
    "            next_entity_id += 1\n",
    "        node_to_entity[node] = ent\n",
    "\n",
    "    n_entities = next_entity_id\n",
    "    counts = np.bincount(node_to_entity, minlength=n_entities).astype(np.int64)\n",
    "    cluster_sizes = counts[counts > 0]\n",
    "\n",
    "    _ = print_final_report(\n",
    "        n_nodes=n_nodes,\n",
    "        n_entities=n_entities,\n",
    "        node_to_entity=node_to_entity,\n",
    "        counts=counts,\n",
    "        cluster_sizes=cluster_sizes,\n",
    "        stats=stats,\n",
    "        multi_change_flags=multi_change_flags,\n",
    "        outputs=outputs,\n",
    "        guards=guards,\n",
    "        enable_constraints_diag=(guards.enable_change_merge_votes or guards.enable_change_degree_guard),\n",
    "        constraint_unique_pairs=constraint_unique_pairs,\n",
    "        constraint_gate_evals_total=constraint_gate_evals_total,\n",
    "        constraint_pairs_repeated=constraint_pairs_repeated,\n",
    "        change_merge_votes=change_merge_votes,\n",
    "        prevout_lookups=stats[\"n_prevout_lookups\"],\n",
    "        prevout_hits=stats[\"n_prevout_hits\"],\n",
    "        run_sanity=sanity.run_sanity_checks,\n",
    "        sanity_top_k=20,\n",
    "        run_prevout_db_sanity=(sanity.run_prevout_db_sanity and sanity_parquet is not None),\n",
    "        conn=conn,\n",
    "        sanity_parquet=sanity_parquet,\n",
    "        prevout_sanity_sample_limit=sanity.prevout_sanity_sample_limit,\n",
    "        prevout_hybrid_threshold=perf.prevout_hybrid_threshold,\n",
    "        prevout_lookup_chunk=perf.prevout_lookup_chunk,\n",
    "    )\n",
    "\n",
    "    # Write mapping (address -> entity_id)\n",
    "    if outputs.write_entity_mapping:\n",
    "        print(f\"\\n[WRITE] Writing entity mapping to: {paths.entity_map_out_path}\", flush=True)\n",
    "        if paths.entity_map_out_path.exists():\n",
    "            paths.entity_map_out_path.unlink()\n",
    "\n",
    "        writer: pq.ParquetWriter | None = None\n",
    "        batch_addrs: list[str] = []\n",
    "        batch_eids: list[int] = []\n",
    "        written = 0\n",
    "\n",
    "        for addr, node_id in addr_to_id.items():\n",
    "            batch_addrs.append(addr)\n",
    "            batch_eids.append(int(node_to_entity[node_id]))\n",
    "\n",
    "            if len(batch_addrs) >= outputs.entity_write_batch:\n",
    "                table = pa.table({\"address\": batch_addrs, \"entity_id\": batch_eids})\n",
    "                if writer is None:\n",
    "                    writer = pq.ParquetWriter(\n",
    "                        str(paths.entity_map_out_path),\n",
    "                        table.schema,\n",
    "                        compression=outputs.entity_map_compression,\n",
    "                        use_dictionary=True,\n",
    "                    )\n",
    "                writer.write_table(table)\n",
    "                written += len(batch_addrs)\n",
    "                batch_addrs.clear()\n",
    "                batch_eids.clear()\n",
    "                print(f\"  [WRITE] rows written: {written:,}\", flush=True)\n",
    "\n",
    "        if batch_addrs:\n",
    "            table = pa.table({\"address\": batch_addrs, \"entity_id\": batch_eids})\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(\n",
    "                    str(paths.entity_map_out_path),\n",
    "                    table.schema,\n",
    "                    compression=outputs.entity_map_compression,\n",
    "                    use_dictionary=True,\n",
    "                )\n",
    "            writer.write_table(table)\n",
    "            written += len(batch_addrs)\n",
    "            print(f\"  [WRITE] rows written: {written:,}\", flush=True)\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "        print(\"[WRITE] Done.\", flush=True)\n",
    "\n",
    "    # Confidence proxy output (optional)\n",
    "    if confidence.enable_confidence_proxy:\n",
    "        print(f\"\\n[CONF] Writing confidence proxy to: {confidence.confidence_out_path}\", flush=True)\n",
    "        if confidence.confidence_out_path.exists():\n",
    "            confidence.confidence_out_path.unlink()\n",
    "\n",
    "        def proxy_prob(node_id: int) -> float:\n",
    "            eid = int(node_to_entity[node_id])\n",
    "            sz = int(counts[eid])\n",
    "            if sz <= 1:\n",
    "                return float(confidence.p_singleton)\n",
    "\n",
    "            bits = int(multi_change_flags[node_id])\n",
    "            e = 0\n",
    "            if bits & 1:\n",
    "                e += 1\n",
    "            if bits & 2:\n",
    "                e += 1\n",
    "            if bits & 4:\n",
    "                e += 1\n",
    "\n",
    "            if e <= 0:\n",
    "                base = float(confidence.p_base_0)\n",
    "            elif e == 1:\n",
    "                base = float(confidence.p_base_1)\n",
    "            else:\n",
    "                base = float(confidence.p_base_2plus)\n",
    "\n",
    "            denom = math.log1p(max(1, confidence.size_norm))\n",
    "            frac = (math.log1p(sz) / denom) if denom > 0 else 1.0\n",
    "            frac = min(1.0, max(0.0, frac))\n",
    "            p = base + float(confidence.size_bonus_max) * frac\n",
    "            return float(min(0.999, max(0.001, p)))\n",
    "\n",
    "        writer2: pq.ParquetWriter | None = None\n",
    "        b_addr: list[str] = []\n",
    "        b_eid: list[int] = []\n",
    "        b_p: list[float] = []\n",
    "        b_sz: list[int] = []\n",
    "        b_bits: list[int] = []\n",
    "        written2 = 0\n",
    "\n",
    "        for addr, node_id in addr_to_id.items():\n",
    "            eid = int(node_to_entity[node_id])\n",
    "            b_addr.append(addr)\n",
    "            b_eid.append(eid)\n",
    "            b_p.append(proxy_prob(node_id))\n",
    "            if confidence.include_cluster_size:\n",
    "                b_sz.append(int(counts[eid]))\n",
    "            if confidence.include_evidence_bits:\n",
    "                b_bits.append(int(multi_change_flags[node_id]))\n",
    "\n",
    "            if len(b_addr) >= confidence.confidence_write_batch:\n",
    "                data = {\"address\": b_addr, \"entity_id\": b_eid, \"p_clustered_proxy\": b_p}\n",
    "                if confidence.include_cluster_size:\n",
    "                    data[\"cluster_size\"] = b_sz\n",
    "                if confidence.include_evidence_bits:\n",
    "                    data[\"evidence_bits\"] = b_bits\n",
    "                table = pa.table(data)\n",
    "                if writer2 is None:\n",
    "                    writer2 = pq.ParquetWriter(\n",
    "                        str(confidence.confidence_out_path),\n",
    "                        table.schema,\n",
    "                        compression=confidence.confidence_compression,\n",
    "                        use_dictionary=True,\n",
    "                    )\n",
    "                writer2.write_table(table)\n",
    "                written2 += len(b_addr)\n",
    "                b_addr.clear()\n",
    "                b_eid.clear()\n",
    "                b_p.clear()\n",
    "                b_sz.clear()\n",
    "                b_bits.clear()\n",
    "                print(f\"  [CONF] rows written: {written2:,}\", flush=True)\n",
    "\n",
    "        if b_addr:\n",
    "            data = {\"address\": b_addr, \"entity_id\": b_eid, \"p_clustered_proxy\": b_p}\n",
    "            if confidence.include_cluster_size:\n",
    "                data[\"cluster_size\"] = b_sz\n",
    "            if confidence.include_evidence_bits:\n",
    "                data[\"evidence_bits\"] = b_bits\n",
    "            table = pa.table(data)\n",
    "            if writer2 is None:\n",
    "                writer2 = pq.ParquetWriter(\n",
    "                    str(confidence.confidence_out_path),\n",
    "                    table.schema,\n",
    "                    compression=confidence.confidence_compression,\n",
    "                    use_dictionary=True,\n",
    "                )\n",
    "            writer2.write_table(table)\n",
    "            written2 += len(b_addr)\n",
    "            print(f\"  [CONF] rows written: {written2:,}\", flush=True)\n",
    "\n",
    "        if writer2 is not None:\n",
    "            writer2.close()\n",
    "\n",
    "        print(\"[CONF] Done.\", flush=True)\n",
    "\n",
    "    # Plots\n",
    "    if plots.enable_plots:\n",
    "        print(\"\\n[PLOT] Generating plots...\", flush=True)\n",
    "\n",
    "        largest = int(cluster_sizes.max()) if cluster_sizes.size else 0\n",
    "\n",
    "        plot_hist_logbins(\n",
    "            sizes=cluster_sizes,\n",
    "            title=f\"Entity Cluster Size Distribution ({analysis.analysis_start.year}) — log bins (all clusters)\",\n",
    "            color=\"tab:blue\",\n",
    "            bins=plots.log_bins,\n",
    "            max_x=None,\n",
    "        )\n",
    "\n",
    "        sizes_excl = cluster_sizes[cluster_sizes != largest] if largest > 0 else cluster_sizes\n",
    "        plot_hist_logbins(\n",
    "            sizes=sizes_excl,\n",
    "            title=f\"Entity Cluster Size Distribution ({analysis.analysis_start.year}) — excluding largest cluster\",\n",
    "            color=\"tab:orange\",\n",
    "            bins=plots.log_bins,\n",
    "            max_x=None,\n",
    "        )\n",
    "\n",
    "        sizes_focus = cluster_sizes[cluster_sizes <= plots.focus_max_size]\n",
    "        plot_hist_logbins(\n",
    "            sizes=sizes_focus,\n",
    "            title=f\"Entity Cluster Sizes ≤ {plots.focus_max_size} ({analysis.analysis_start.year}) — log bins\",\n",
    "            color=\"tab:green\",\n",
    "            bins=plots.log_bins,\n",
    "            max_x=plots.focus_max_size,\n",
    "        )\n",
    "\n",
    "        plot_zipf(\n",
    "            sizes=cluster_sizes,\n",
    "            title=f\"Rank–Size (Zipf) Plot ({analysis.analysis_start.year}) — top {min(plots.zipf_top_k, cluster_sizes.size):,} clusters\",\n",
    "            color=\"tab:red\",\n",
    "            top_k=plots.zipf_top_k,\n",
    "        )\n",
    "\n",
    "    conn.close()\n",
    "    print(\"[INFO] Done.\", flush=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Paths (explicit) ---\n",
    "    outputs_dir = Path(\"/media/vatereal/Main/outputs\")\n",
    "    analysis_year = 2014\n",
    "\n",
    "\n",
    "    paths = PathsConfig(\n",
    "        parquet_io_glob=str(Path(\"/media/vatereal/Main/parquet\") / \"io/day=*/io-*.parquet\"),\n",
    "        outputs_dir=outputs_dir,\n",
    "        outpoint_db_path=outputs_dir / f\"outpoints_{analysis_year}.sqlite\",\n",
    "        entity_map_out_path=outputs_dir / f\"entities_multiinput_change_{analysis_year}.parquet\",\n",
    "    )\n",
    "\n",
    "    # --- Analysis window (explicit) ---\n",
    "    analysis = AnalysisConfig(\n",
    "        analysis_start=date(analysis_year, 1, 1),\n",
    "        analysis_end_exclusive=date(analysis_year + 1, 1, 1),\n",
    "        outpoint_db_lookback_days=365,\n",
    "        value_unit_mode=\"infer\",\n",
    "    )\n",
    "\n",
    "    # --- Output behavior (explicit) ---\n",
    "    outputs = OutputConfig(\n",
    "        reset_outpoint_db=True,\n",
    "        write_entity_mapping=True,\n",
    "        entity_map_compression=\"zstd\",\n",
    "        entity_write_batch=1_000_000,\n",
    "        top_k_clusters_print=20,\n",
    "    )\n",
    "\n",
    "    # --- Plot behavior (explicit) ---\n",
    "    plots = PlotConfig(\n",
    "        enable_plots=True,\n",
    "        zipf_top_k=200_000,\n",
    "        focus_max_size=2048,\n",
    "        log_bins=40,\n",
    "    )\n",
    "\n",
    "    # --- Performance knobs (explicit) ---\n",
    "    perf = PerfConfig(\n",
    "        outpoint_commit_every_rows=500_000,\n",
    "        prevout_hybrid_threshold=5_000,\n",
    "        prevout_lookup_chunk=50_000,\n",
    "        iter_buffer_in=200_000,\n",
    "        iter_buffer_out=200_000,\n",
    "        iter_buffer_grouped=50_000,\n",
    "        gc_every_n_files=100,\n",
    "        tqdm_mininterval=2.0,\n",
    "        tqdm_miniters=50,\n",
    "        tqdm_postfix_every=250,\n",
    "\n",
    "        # Patch 3 settings (required):\n",
    "        use_sql_prevout_join=True,\n",
    "        vinbuf_insert_chunk=1_000_000,\n",
    "        vinbuf_fetch_chunk=1_000_000,\n",
    "    )\n",
    "\n",
    "    # --- Sanity checks (explicit) ---\n",
    "    sanity = SanityConfig(\n",
    "        run_sanity_checks=True,\n",
    "        run_prevout_db_sanity=True,\n",
    "        prevout_sanity_sample_limit=200_000,\n",
    "        prevout_sanity_parquet=None,\n",
    "    )\n",
    "\n",
    "    # --- Heuristic toggles (explicit) ---\n",
    "    toggles = HeuristicToggles(\n",
    "        enable_coinjoin_filter=True,\n",
    "        enable_multi_input=True,\n",
    "        enable_change=True,\n",
    "        enable_merge_guards=True,\n",
    "        precreate_nodes_for_all_output_addrs=False,\n",
    "        create_nodes_for_all_resolved_inputs=True,\n",
    "    )\n",
    "\n",
    "    # --- Heuristic params (explicit) ---\n",
    "    params = HeuristicParams(\n",
    "        dust_sats=546,\n",
    "        max_fee_abs_sats=50_000_000,\n",
    "        max_fee_frac=0.05,\n",
    "        max_change_inputs_utxos=10,\n",
    "        max_change_spendable_outs=2,\n",
    "        change_require_2_outputs=True,\n",
    "        multi_input_policy=\"one_output\",\n",
    "        enable_optimal_change_constraint=True,\n",
    "        optimal_change_slack_sats=0,\n",
    "    )\n",
    "\n",
    "    # --- Merge guards (explicit) ---\n",
    "    guards = MergeGuardParams(\n",
    "        # Patch A: allow mega-entities (still a ceiling/fuse; tune as desired)\n",
    "        max_merged_component_size=10_000_000,\n",
    "\n",
    "        # Ratio guard ignores tiny components\n",
    "        merge_ratio_guard=True,\n",
    "        merge_ratio_max=200.0,\n",
    "        merge_ratio_big_cluster_min=50_000,\n",
    "        merge_ratio_small_floor=50,\n",
    "\n",
    "        # Big–big gating (CHANGE only): min_side is \"both sides >= this\"\n",
    "        enable_change_merge_votes=True,\n",
    "        change_vote_min_side=25_000,\n",
    "        change_vote_merged_trigger=0,\n",
    "        change_votes_required=2,\n",
    "\n",
    "        # Patch A: stricter votes above thresholds (250k/500k/1M)\n",
    "        ultra_change_vote_rules=((250_000, 3), (500_000, 4), (1_000_000, 5)),\n",
    "\n",
    "        # Patch B: degree guard (alternative confirmation signal)\n",
    "        enable_change_degree_guard=True,\n",
    "        change_degree_large_min=25_000,\n",
    "        change_degree_small_min=50,\n",
    "        change_degree_max_distinct_large=3,\n",
    "        change_degree_set_cap=64,\n",
    "\n",
    "        # Ratio-guard sample log\n",
    "        ratio_guard_sample_n=1000,\n",
    "        ratio_guard_sample_path=outputs_dir / f\"ratio_guard_samples_{analysis_year}.tsv\",\n",
    "\n",
    "        # Patch B: constraint event log (vote pending + degree guard)\n",
    "        constraint_log_n=200_000,\n",
    "        constraint_log_path=outputs_dir / f\"constraint_events_{analysis_year}.tsv\",\n",
    "\n",
    "        debug_large_change_merges=False,\n",
    "        debug_change_merge_min=250_000,\n",
    "        debug_change_merge_log_path=outputs_dir / f\"large_change_merges_{analysis_year}.tsv\",\n",
    "    )\n",
    "\n",
    "    # --- Confidence proxy output (explicit) ---\n",
    "    confidence = ConfidenceConfig(\n",
    "        enable_confidence_proxy=True,\n",
    "        confidence_out_path=outputs_dir / f\"address_confidence_{analysis_year}.parquet\",\n",
    "        confidence_compression=\"zstd\",\n",
    "        confidence_write_batch=1_000_000,\n",
    "\n",
    "        size_norm=100_000,\n",
    "        size_bonus_max=0.15,\n",
    "        p_singleton=0.02,\n",
    "        p_base_0=0.25,\n",
    "        p_base_1=0.65,\n",
    "        p_base_2plus=0.85,\n",
    "\n",
    "        include_cluster_size=True,\n",
    "        include_evidence_bits=True,\n",
    "    )\n",
    "\n",
    "    run_entity_clustering(\n",
    "        paths=paths,\n",
    "        analysis=analysis,\n",
    "        outputs=outputs,\n",
    "        plots=plots,\n",
    "        perf=perf,\n",
    "        sanity=sanity,\n",
    "        toggles=toggles,\n",
    "        params=params,\n",
    "        guards=guards,\n",
    "        confidence=confidence,\n",
    "        determinism_seed=1337,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7e568f",
   "metadata": {},
   "source": [
    "## Logistic change model: implementation + feature set\n",
    "\n",
    "### Objective\n",
    "The pipeline uses a **binary classifier** to decide, for a given transaction, **which candidate output is the change output** (if any). It replaces brittle hard rules with:\n",
    "\n",
    "- a **feature extractor** (transaction → feature vector),\n",
    "- a **logistic regression scorer** (features → probability),\n",
    "- a **decision policy** (probability thresholds + abstention).\n",
    "\n",
    "The output of this decision drives **CHANGE unions** in the union-find clustering.\n",
    "\n",
    "---\n",
    "\n",
    "## Where the model is used in the pipeline\n",
    "\n",
    "For each transaction eligible for change detection:\n",
    "\n",
    "1. **Candidate generation**  \n",
    "   Restrict to spendable outputs that:\n",
    "   - are not one of the input addresses,\n",
    "   - have output address type matching the (uniform) input type,\n",
    "   - satisfy the “2-output” structure (if enabled),\n",
    "   - are distinct and non-dust.\n",
    "\n",
    "2. **Score candidates**  \n",
    "   For each candidate output $c$, compute features $x(c)$, then score with logit:\n",
    "   \n",
    "   $$p(c) = \\sigma\\left(b + \\sum_i w_i x_i(c)\\right)$$\n",
    "   \n",
    "   where $\\sigma(z) = \\frac{1}{1+e^{-z}}$.\n",
    "\n",
    "3. **Select best + abstain if ambiguous**  \n",
    "   Let $p_{best}$ be max probability and $p_{2nd}$ runner-up.\n",
    "   - If $p_{best} - p_{2nd} < \\text{min\\_p\\_gap}$, abstain (no change union).\n",
    "\n",
    "4. **Apply dynamic acceptance threshold**  \n",
    "   The threshold increases with the current **anchor cluster size**:\n",
    "   \n",
    "   $$p_{\\text{accept}} = \\text{clamp}\\left(p_0 + \\alpha \\log(1 + S_{anchor}),\\ p_{\\min},\\ p_{\\max}\\right)$$\n",
    "   \n",
    "   If $p_{best} < p_{\\text{accept}}$, abstain.\n",
    "\n",
    "5. **If accepted: perform CHANGE union**  \n",
    "   - Mark evidence bits:\n",
    "     - change output: bit2\n",
    "     - change anchor: bit4\n",
    "   - Union(anchor\\_addr, change\\_addr) with CHANGE reason (subject to guards).\n",
    "\n",
    "---\n",
    "\n",
    "## Logistic regression implementation details\n",
    "\n",
    "### Scoring function\n",
    "The model computes a linear score:\n",
    "\n",
    "$$z = \\text{intercept} + \\sum_i w_i x_i$$\n",
    "\n",
    "Then converts to probability with a numerically stable sigmoid:\n",
    "\n",
    "- If $z \\ge 0$: $\\sigma(z)=\\frac{1}{1+e^{-z}}$\n",
    "- Else: $\\sigma(z)=\\frac{e^z}{1+e^z}$\n",
    "\n",
    "This avoids overflow for large $|z|$.\n",
    "\n",
    "### Missing weights\n",
    "If a feature exists in the extracted vector but has no coefficient entry, it contributes **0**.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature extraction\n",
    "\n",
    "Each candidate output gets its own feature vector, derived from transaction context + candidate-vs-other comparisons.\n",
    "\n",
    "### Core structural features\n",
    "\n",
    "| Feature name | Type | Meaning |\n",
    "|---|---:|---|\n",
    "| `n_in` | float | Number of input UTXOs spent by the tx |\n",
    "| `n_out` | float | Number of spendable outputs considered |\n",
    "| `in_type_uniform` | {0,1} | All input address types are identical |\n",
    "| `out_type_match_in` | {0,1} | Candidate output type matches input type |\n",
    "| `is_mixing_like` | {0,1} | Mixing-like pattern detected (penalized) |\n",
    "\n",
    "> Note: The pipeline *already* tends to filter out some non-uniform / mixing-like cases before scoring; but the feature is kept so the model can encode this as a strong negative signal if such cases slip through.\n",
    "\n",
    "---\n",
    "\n",
    "### Fee / conservation features\n",
    "\n",
    "| Feature name | Type | Meaning |\n",
    "|---|---:|---|\n",
    "| `fee_frac` | float | $\\frac{\\text{fee}}{\\text{sum\\_inputs}}$ |\n",
    "| `log_fee` | float | normalized $\\log(1+\\text{fee})$ |\n",
    "| `sum_in_gt_sum_out` | {0,1} | sanity indicator (inputs exceed outputs) |\n",
    "\n",
    "**Normalization used**  \n",
    "To keep magnitudes stable across time/scale:\n",
    "\n",
    "$$\\text{log\\_fee} = \\frac{\\log(1+\\text{fee})}{\\log(1+\\text{fee\\_norm})}$$\n",
    "\n",
    "with `fee_norm = cfg.log1p_fee_norm` (example: \\(10^6\\)).\n",
    "\n",
    "---\n",
    "\n",
    "### Candidate-specific features\n",
    "\n",
    "| Feature name | Type | Meaning |\n",
    "|---|---:|---|\n",
    "| `cand_new` | {0,1} | candidate address has not been seen as an output before (“fresh”) |\n",
    "| `cand_is_min_out` | {0,1} | candidate value is the smaller of the two |\n",
    "| `cand_lt_other` | {0,1} | candidate value strictly less than the other output |\n",
    "| `log_cand` | float | normalized \\( \\log(1+\\text{candidate value}) \\) |\n",
    "| `log_other` | float | normalized \\( \\log(1+\\text{other output value}) \\) |\n",
    "\n",
    "Normalization:\n",
    "$$\\text{log\\_cand} = \\frac{\\log(1+\\text{cand\\_value})}{\\log(1+\\text{value\\_norm})}$$\n",
    "\n",
    "$$\\text{log\\_other} = \\frac{\\log(1+\\text{other\\_value})}{\\log(1+\\text{value\\_norm})}$$\n",
    "\n",
    "with `value_norm = cfg.log1p_value_norm` (example: \\(10^8\\)).\n",
    "\n",
    "---\n",
    "\n",
    "### “Optimal change” feature (optional but implemented)\n",
    "\n",
    "| Feature name | Type | Meaning |\n",
    "|---|---:|---|\n",
    "| `optimal_ok` | {0,1} | candidate satisfies a soft “optimal change” inequality |\n",
    "\n",
    "Definition:\n",
    "- Let `min_input_sats` be the minimum input value among spent UTXOs.\n",
    "- Candidate is “optimal-ok” if:\n",
    "\n",
    "$$\\text{cand\\_value} \\le \\text{min\\_input} - \\text{fee} + \\text{slack}$$\n",
    "\n",
    "with `slack = cfg.optimal_change_slack_sats` (often 0).\n",
    "\n",
    "This encodes a classic heuristic as a **feature**, not a hard filter.\n",
    "\n",
    "---\n",
    "\n",
    "## Decision policy hyperparameters (model-adjacent, not coefficients)\n",
    "\n",
    "These are not weights; they govern when the model’s output is trusted:\n",
    "\n",
    "- `min_p_gap`: require separation between best and runner-up candidate\n",
    "- `p_accept_base`, `p_accept_log_slope`: threshold schedule based on anchor cluster size\n",
    "- `p_accept_min`, `p_accept_max`: clamp bounds on the schedule\n",
    "\n",
    "This makes the system more conservative for attaching change to already-large entities.\n",
    "\n",
    "---\n",
    "\n",
    "## Evidence bits and what they mean downstream\n",
    "\n",
    "The model doesn’t directly “assign entities”; it triggers evidence + unions:\n",
    "\n",
    "- **bit2 (change-output)**: address was predicted as change output at least once\n",
    "- **bit4 (change-anchor)**: anchor/input address participated in a change prediction\n",
    "- (Separately) **bit1** comes from multi-input heuristic (H1)\n",
    "\n",
    "These bits are persisted in `address_confidence_YYYY.parquet` and used for diagnostics such as top-cluster evidence composition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d87ca32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      " RUN YEAR 2013\n",
      "============================\n",
      "Parquet file counts: {'io': 7678}\n",
      "[INFO] Analysis window: 2013-01-01 .. 2014-01-01 (exclusive)\n",
      "[INFO] Outpoint DB preload start: 2012-01-02 .. 2014-01-01 (exclusive)\n",
      "[INFO] Outpoint DB path: /media/vatereal/Main/outputs/outpoints_2013.sqlite (reset=True)\n",
      "[INFO] Eligible files in window: 753  (preload=375, analysis=378)\n",
      "[INFO] Sanity parquet selected: /media/vatereal/Main/parquet/io/day=2013-01-01/io-000214563-000214724.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbaa548b1814f3ba014ef55f7716021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing parquet:   0%|          | 0/753 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preload 1] Indexing outputs only: /media/vatereal/Main/parquet/io/day=2012-01-02/io-000160190-000160340.parquet (day=2012-01-02) ...\n",
      "[INFO] Detected value unit: btc  (all internal amount logic uses satoshis)\n",
      "[preload 2] Indexing outputs only: /media/vatereal/Main/parquet/io/day=2012-01-03/io-000160341-000160499.parquet (day=2012-01-03) ...\n",
      "[preload 3] Indexing outputs only: /media/vatereal/Main/parquet/io/day=2012-01-04/io-000160500-000160648.parquet (day=2012-01-04) ...\n",
      "[1] Processing /media/vatereal/Main/parquet/io/day=2013-01-01/io-000214563-000214724.parquet (day=2013-01-01) ...\n",
      "[2] Processing /media/vatereal/Main/parquet/io/day=2013-01-02/io-000214725-000214877.parquet (day=2013-01-02) ...\n",
      "[3] Processing /media/vatereal/Main/parquet/io/day=2013-01-03/io-000214878-000215032.parquet (day=2013-01-03) ...\n",
      "[4] Processing /media/vatereal/Main/parquet/io/day=2013-01-03/io-000215033-000215039.parquet (day=2013-01-03) ...\n",
      "[5] Processing /media/vatereal/Main/parquet/io/day=2013-01-04/io-000215040-000215192.parquet (day=2013-01-04) ...\n",
      "[250] Processing ... (day=2013-08-30)\n",
      "[DIAG] Wrote ratio-guard sample log: /media/vatereal/Main/outputs/ratio_guard_samples_2013.tsv  (n=0)\n",
      "[DIAG] Wrote constraint-event log: /media/vatereal/Main/outputs/constraint_events_2013.tsv  (n=0)\n",
      "\n",
      "Finalizing entity mapping (compressing components)...\n",
      "Number of unique addresses with UF nodes: 15,370,982\n",
      "[INFO] prevout lookups: 44,792,299  hits: 44,400,820  hit-rate: 99.126%\n",
      "Number of entities (clusters): 14,060,109\n",
      "\n",
      "[TOP CLUSTERS]\n",
      "  #01  entity_id=       9  size=   113,648  frac=  0.74%\n",
      "  #02  entity_id=     372  size=   101,957  frac=  0.66%\n",
      "  #03  entity_id=     119  size=    79,815  frac=  0.52%\n",
      "  #04  entity_id=     278  size=    53,344  frac=  0.35%\n",
      "  #05  entity_id= 4375556  size=    17,919  frac=  0.12%\n",
      "  #06  entity_id=  237205  size=    17,766  frac=  0.12%\n",
      "  #07  entity_id=      25  size=     8,310  frac=  0.05%\n",
      "  #08  entity_id= 8134869  size=     6,712  frac=  0.04%\n",
      "  #09  entity_id=     173  size=     5,754  frac=  0.04%\n",
      "  #10  entity_id=     499  size=     5,539  frac=  0.04%\n",
      "  #11  entity_id=   24935  size=     5,373  frac=  0.03%\n",
      "  #12  entity_id=     303  size=     5,339  frac=  0.03%\n",
      "  #13  entity_id=  157476  size=     4,884  frac=  0.03%\n",
      "  #14  entity_id=10705308  size=     3,967  frac=  0.03%\n",
      "  #15  entity_id=10519181  size=     3,816  frac=  0.02%\n",
      "  #16  entity_id=    1254  size=     3,730  frac=  0.02%\n",
      "  #17  entity_id=     788  size=     3,662  frac=  0.02%\n",
      "  #18  entity_id=10517050  size=     3,563  frac=  0.02%\n",
      "  #19  entity_id= 8431133  size=     3,544  frac=  0.02%\n",
      "  #20  entity_id= 8280754  size=     3,527  frac=  0.02%\n",
      "\n",
      "[CLUSTER STATS]\n",
      "  Entities: 14,060,109\n",
      "  Largest cluster size: 113,648\n",
      "  Largest cluster fraction of nodes: 0.74%\n",
      "  Median cluster size: 1\n",
      "  90th percentile: 1\n",
      "  99th percentile: 2\n",
      "\n",
      "[TOP CLUSTER EVIDENCE COMPOSITION]\n",
      "  Top entity_id: 9\n",
      "  Addresses in top cluster: 113,648\n",
      "  bit1 (multi-input)   : 112,107  share=98.64%\n",
      "  bit2 (change-output) : 1,788  share=1.57%\n",
      "  bit4 (change-anchor) : 351  share=0.31%\n",
      "  bit2-only            : 1,312  share=1.15%\n",
      "  (collapse signature often: very high bit2-only share + low bit1 share)\n",
      "\n",
      "[HEURISTIC COVERAGE (node-level)]\n",
      "  Total txs processed (>=1 input UTXO): 19,579,808\n",
      "  Mixing-like skipped: 241,103\n",
      "  Multi-input txs (SAFE policy applied): 566,469\n",
      "  Change detected (model): 115,851\n",
      "  Nodes marked multi-input: 1,434,803\n",
      "  Nodes marked change-output: 115,851\n",
      "  Nodes marked change-anchor: 97,819\n",
      "  Nodes touched by any heuristic: 1,623,543\n",
      "\n",
      "[MODEL CALIBRATION (Patch 2)]\n",
      "  easy txs seen (calibration stream): 11,411,974\n",
      "  easy txs accepted: 115,851\n",
      "  easy accept-rate: 1.0152%\n",
      "  p_accept (rolling quantile): 0.8125\n",
      "  p_gap threshold: 0.1000\n",
      "  logit eval samples stored: 500,000\n",
      "\n",
      "[UNION DIAGNOSTICS]\n",
      "  Union attempts: 2,207,496\n",
      "  Unions applied: 1,310,873\n",
      "    - applied via H1:     1,195,248\n",
      "    - applied via CHANGE: 115,625\n",
      "  skip_same_component (already merged): 896,623\n",
      "    - same_component via H1:     896,397\n",
      "    - same_component via CHANGE: 226\n",
      "  Unions skipped by guards/votes/constraints: 0\n",
      "    - skipped (H1):     0\n",
      "    - skipped (CHANGE): 0\n",
      "  Skip breakdown:\n",
      "    - skip_abs_cap: 0  (H1=0, CHANGE=0)\n",
      "    - skip_ratio_guard: 0  (CHANGE-only)\n",
      "    - skip_constraint: 0  (CHANGE-only; vote pending)\n",
      "    - skip_degree_guard: 0  (CHANGE-only)\n",
      "    - skip_vote_pending: 0\n",
      "    - skip_vote_failed:  0\n",
      "\n",
      "[CONSTRAINT DIAGNOSTICS]\n",
      "  Vote-gating pairs seen (unique): 0\n",
      "  Vote-gating evaluations (total): 0\n",
      "  Vote-gating pairs that repeated: 0\n",
      "  Vote pairs tracked (total): 0\n",
      "\n",
      "[SANITY] Cluster summary\n",
      "  UF nodes (n_nodes): 15,370,982\n",
      "  Total nodes from cluster sizes: 15,370,982\n",
      "  Entities (clusters): 14,060,109\n",
      "  Largest cluster size: 113,648\n",
      "  Largest cluster fraction of nodes: 0.74%\n",
      "\n",
      "[SANITY] Top 20 cluster sizes:\n",
      "  [113648, 101957, 79815, 53344, 17919, 17766, 8310, 6712, 5754, 5539, 5373, 5339, 4884, 3967, 3816, 3730, 3662, 3563, 3544, 3527]\n",
      "\n",
      "[SANITY] Quick distribution stats\n",
      "  Median cluster size: 1\n",
      "  90th percentile cluster size: 1\n",
      "  99th percentile cluster size: 2\n",
      "\n",
      "[SANITY] Prevout lookup hit-rate (DB)\n",
      "  Lookups:  44,792,299\n",
      "  Hits:     44,400,820\n",
      "  Hit-rate: 99.13%\n",
      "\n",
      "[PREVOUT-SANITY | DB]\n",
      "File: /media/vatereal/Main/parquet/io/day=2013-01-01/io-000214563-000214724.parquet\n",
      "Vin sample rows: 66728\n",
      "prev_vout integer-like fraction (sample): 1.000000\n",
      "DB prevout hit-rate (sample): 65668/66728 = 98.41%\n",
      "Sample unresolved (prev_txid, prev_n):\n",
      "  ('b990872395e05e695fa1a110fabc7e04c1c6701cdbcde5db8049f2452b0a5a7c', 1)\n",
      "  ('04e48c5339df82a444f95313674c5bfc5000a8fb08845a5f733e9c128b6922b1', 1)\n",
      "  ('bf645385f464f547f619a34b4db316ce1d0fb7cc703609a431a4954103c5fef6', 1)\n",
      "  ('11f51c54d756e32280db8aacdce47e05caf7b8f91ab300b0a04b087b04b22140', 0)\n",
      "  ('8f5256b36430ef9ebafc7ed6513a9ccc3a3345262784f920919dc5a263e5ed81', 0)\n",
      "  ('8e663697f846f1455a116354820851301980466f56a9d43af1c6f6fde889af96', 0)\n",
      "  ('4d6134d307c22ccc7eee42595fa85290271b8456657da71c87d9d1acdb139e00', 0)\n",
      "  ('b4b1aad1bc755ec801e77cfb705bafb581fca3cab7d1435561911561e172e424', 0)\n",
      "  ('6c0a22a2d21b1fa741d9697db36080a8f4bfed35c048acc827b51019d6ad64f1', 0)\n",
      "  ('86490fc12f553423bddd58acccae9be096131cbbd56c41988f892ff7d874c843', 0)\n",
      "\n",
      "[WRITE] Writing entity mapping to: /media/vatereal/Main/outputs/entities_model_features_2013.parquet\n",
      "  [WRITE] rows written: 1,000,000\n",
      "  [WRITE] rows written: 2,000,000\n",
      "  [WRITE] rows written: 3,000,000\n",
      "  [WRITE] rows written: 4,000,000\n",
      "  [WRITE] rows written: 5,000,000\n",
      "  [WRITE] rows written: 6,000,000\n",
      "  [WRITE] rows written: 7,000,000\n",
      "  [WRITE] rows written: 8,000,000\n",
      "  [WRITE] rows written: 9,000,000\n",
      "  [WRITE] rows written: 10,000,000\n",
      "  [WRITE] rows written: 11,000,000\n",
      "  [WRITE] rows written: 12,000,000\n",
      "  [WRITE] rows written: 13,000,000\n",
      "  [WRITE] rows written: 14,000,000\n",
      "  [WRITE] rows written: 15,000,000\n",
      "  [WRITE] rows written: 15,370,982\n",
      "[WRITE] Done.\n",
      "\n",
      "[CONF] Writing confidence proxy to: /media/vatereal/Main/outputs/address_confidence_2013.parquet\n",
      "  [CONF] rows written: 1,000,000\n",
      "  [CONF] rows written: 2,000,000\n",
      "  [CONF] rows written: 3,000,000\n",
      "  [CONF] rows written: 4,000,000\n",
      "  [CONF] rows written: 5,000,000\n",
      "  [CONF] rows written: 6,000,000\n",
      "  [CONF] rows written: 7,000,000\n",
      "  [CONF] rows written: 8,000,000\n",
      "  [CONF] rows written: 9,000,000\n",
      "  [CONF] rows written: 10,000,000\n",
      "  [CONF] rows written: 11,000,000\n",
      "  [CONF] rows written: 12,000,000\n",
      "  [CONF] rows written: 13,000,000\n",
      "  [CONF] rows written: 14,000,000\n",
      "  [CONF] rows written: 15,000,000\n",
      "  [CONF] rows written: 15,370,982\n",
      "[CONF] Done.\n",
      "\n",
      "[PLOT] Generating plots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_111889/2214432097.py:980: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  auc = float(np.trapz(tpr_s, fpr_s))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Done.\n",
      "\n",
      "============================\n",
      " RUN YEAR 2014\n",
      "============================\n",
      "Parquet file counts: {'io': 7678}\n",
      "[INFO] Analysis window: 2014-01-01 .. 2015-01-01 (exclusive)\n",
      "[INFO] Outpoint DB preload start: 2013-01-01 .. 2015-01-01 (exclusive)\n",
      "[INFO] Outpoint DB path: /media/vatereal/Main/outputs/outpoints_2014.sqlite (reset=True)\n",
      "[INFO] Eligible files in window: 755  (preload=378, analysis=377)\n",
      "[INFO] Sanity parquet selected: /media/vatereal/Main/parquet/io/day=2014-01-01/io-000277996-000278200.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85276a04c7c244aaab26feb4a72f1976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing parquet:   0%|          | 0/755 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preload 1] Indexing outputs only: /media/vatereal/Main/parquet/io/day=2013-01-01/io-000214563-000214724.parquet (day=2013-01-01) ...\n",
      "[INFO] Detected value unit: btc  (all internal amount logic uses satoshis)\n",
      "[preload 2] Indexing outputs only: /media/vatereal/Main/parquet/io/day=2013-01-02/io-000214725-000214877.parquet (day=2013-01-02) ...\n",
      "[preload 3] Indexing outputs only: /media/vatereal/Main/parquet/io/day=2013-01-03/io-000214878-000215032.parquet (day=2013-01-03) ...\n",
      "[1] Processing /media/vatereal/Main/parquet/io/day=2014-01-01/io-000277996-000278200.parquet (day=2014-01-01) ...\n",
      "[2] Processing /media/vatereal/Main/parquet/io/day=2014-01-02/io-000278201-000278334.parquet (day=2014-01-02) ...\n",
      "[3] Processing /media/vatereal/Main/parquet/io/day=2014-01-03/io-000278335-000278495.parquet (day=2014-01-03) ...\n",
      "[4] Processing /media/vatereal/Main/parquet/io/day=2014-01-04/io-000278496-000278636.parquet (day=2014-01-04) ...\n",
      "[5] Processing /media/vatereal/Main/parquet/io/day=2014-01-05/io-000278637-000278829.parquet (day=2014-01-05) ...\n",
      "[250] Processing ... (day=2014-08-30)\n",
      "[DIAG] Wrote ratio-guard sample log: /media/vatereal/Main/outputs/ratio_guard_samples_2014.tsv  (n=0)\n",
      "[DIAG] Wrote constraint-event log: /media/vatereal/Main/outputs/constraint_events_2014.tsv  (n=0)\n",
      "\n",
      "Finalizing entity mapping (compressing components)...\n",
      "Number of unique addresses with UF nodes: 33,269,761\n",
      "[INFO] prevout lookups: 70,806,194  hits: 70,633,357  hit-rate: 99.756%\n",
      "Number of entities (clusters): 31,377,319\n",
      "\n",
      "[TOP CLUSTERS]\n",
      "  #01  entity_id=     170  size=   141,032  frac=  0.42%\n",
      "  #02  entity_id=     130  size=   124,747  frac=  0.37%\n",
      "  #03  entity_id=      34  size=   115,466  frac=  0.35%\n",
      "  #04  entity_id=   12666  size=    71,381  frac=  0.21%\n",
      "  #05  entity_id=  156812  size=    46,110  frac=  0.14%\n",
      "  #06  entity_id= 5917351  size=    30,511  frac=  0.09%\n",
      "  #07  entity_id=      50  size=    25,213  frac=  0.08%\n",
      "  #08  entity_id= 3131328  size=    17,102  frac=  0.05%\n",
      "  #09  entity_id=     104  size=    15,689  frac=  0.05%\n",
      "  #10  entity_id= 8633868  size=    15,015  frac=  0.05%\n",
      "  #11  entity_id=18989459  size=    12,994  frac=  0.04%\n",
      "  #12  entity_id=     556  size=    12,205  frac=  0.04%\n",
      "  #13  entity_id=     273  size=    12,111  frac=  0.04%\n",
      "  #14  entity_id= 2733547  size=    11,458  frac=  0.03%\n",
      "  #15  entity_id=  149651  size=    11,350  frac=  0.03%\n",
      "  #16  entity_id=12246078  size=     8,510  frac=  0.03%\n",
      "  #17  entity_id=  415194  size=     8,395  frac=  0.03%\n",
      "  #18  entity_id= 9479205  size=     8,199  frac=  0.02%\n",
      "  #19  entity_id=   11750  size=     7,877  frac=  0.02%\n",
      "  #20  entity_id=    7245  size=     7,468  frac=  0.02%\n",
      "\n",
      "[CLUSTER STATS]\n",
      "  Entities: 31,377,319\n",
      "  Largest cluster size: 141,032\n",
      "  Largest cluster fraction of nodes: 0.42%\n",
      "  Median cluster size: 1\n",
      "  90th percentile: 1\n",
      "  99th percentile: 1\n",
      "\n",
      "[TOP CLUSTER EVIDENCE COMPOSITION]\n",
      "  Top entity_id: 170\n",
      "  Addresses in top cluster: 141,032\n",
      "  bit1 (multi-input)   : 140,861  share=99.88%\n",
      "  bit2 (change-output) : 299  share=0.21%\n",
      "  bit4 (change-anchor) : 267  share=0.19%\n",
      "  bit2-only            : 89  share=0.06%\n",
      "  (collapse signature often: very high bit2-only share + low bit1 share)\n",
      "\n",
      "[HEURISTIC COVERAGE (node-level)]\n",
      "  Total txs processed (>=1 input UTXO): 25,204,855\n",
      "  Mixing-like skipped: 545,368\n",
      "  Multi-input txs (SAFE policy applied): 949,571\n",
      "  Change detected (model): 118,095\n",
      "  Nodes marked multi-input: 2,210,936\n",
      "  Nodes marked change-output: 118,095\n",
      "  Nodes marked change-anchor: 94,301\n",
      "  Nodes touched by any heuristic: 2,399,968\n",
      "\n",
      "[MODEL CALIBRATION (Patch 2)]\n",
      "  easy txs seen (calibration stream): 11,855,873\n",
      "  easy txs accepted: 118,095\n",
      "  easy accept-rate: 0.9961%\n",
      "  p_accept (rolling quantile): 0.8065\n",
      "  p_gap threshold: 0.1000\n",
      "  logit eval samples stored: 500,000\n",
      "\n",
      "[UNION DIAGNOSTICS]\n",
      "  Union attempts: 3,920,650\n",
      "  Unions applied: 1,892,442\n",
      "    - applied via H1:     1,774,567\n",
      "    - applied via CHANGE: 117,875\n",
      "  skip_same_component (already merged): 2,028,208\n",
      "    - same_component via H1:     2,027,988\n",
      "    - same_component via CHANGE: 220\n",
      "  Unions skipped by guards/votes/constraints: 0\n",
      "    - skipped (H1):     0\n",
      "    - skipped (CHANGE): 0\n",
      "  Skip breakdown:\n",
      "    - skip_abs_cap: 0  (H1=0, CHANGE=0)\n",
      "    - skip_ratio_guard: 0  (CHANGE-only)\n",
      "    - skip_constraint: 0  (CHANGE-only; vote pending)\n",
      "    - skip_degree_guard: 0  (CHANGE-only)\n",
      "    - skip_vote_pending: 0\n",
      "    - skip_vote_failed:  0\n",
      "\n",
      "[CONSTRAINT DIAGNOSTICS]\n",
      "  Vote-gating pairs seen (unique): 0\n",
      "  Vote-gating evaluations (total): 0\n",
      "  Vote-gating pairs that repeated: 0\n",
      "  Vote pairs tracked (total): 0\n",
      "\n",
      "[SANITY] Cluster summary\n",
      "  UF nodes (n_nodes): 33,269,761\n",
      "  Total nodes from cluster sizes: 33,269,761\n",
      "  Entities (clusters): 31,377,319\n",
      "  Largest cluster size: 141,032\n",
      "  Largest cluster fraction of nodes: 0.42%\n",
      "\n",
      "[SANITY] Top 20 cluster sizes:\n",
      "  [141032, 124747, 115466, 71381, 46110, 30511, 25213, 17102, 15689, 15015, 12994, 12205, 12111, 11458, 11350, 8510, 8395, 8199, 7877, 7468]\n",
      "\n",
      "[SANITY] Quick distribution stats\n",
      "  Median cluster size: 1\n",
      "  90th percentile cluster size: 1\n",
      "  99th percentile cluster size: 1\n",
      "\n",
      "[SANITY] Prevout lookup hit-rate (DB)\n",
      "  Lookups:  70,806,194\n",
      "  Hits:     70,633,357\n",
      "  Hit-rate: 99.76%\n",
      "\n",
      "[PREVOUT-SANITY | DB]\n",
      "File: /media/vatereal/Main/parquet/io/day=2014-01-01/io-000277996-000278200.parquet\n",
      "Vin sample rows: 101942\n",
      "prev_vout integer-like fraction (sample): 1.000000\n",
      "DB prevout hit-rate (sample): 101214/101942 = 99.29%\n",
      "Sample unresolved (prev_txid, prev_n):\n",
      "  ('c0f2ebd2ee2afb98df29f191bfb67ad74af87755e61de688b49eeb213d449f3b', 1)\n",
      "  ('201388ae8def6fa7fed614edf5871ee066db80f9a09e0a8b490cf47c1eedac95', 0)\n",
      "  ('1b7104be86122b05e34edb10a58118acb836de6a55b2a460bd3066a5872aa667', 0)\n",
      "  ('04eb6fead5e7f0844c27443c04899a67149f808dd84da6f122d4888d11a21623', 0)\n",
      "  ('d37e1957ce649f3b93fa7f1846d36408d259f77588232c74a9acee929f20e543', 105)\n",
      "  ('a949fefde6f2af966b0036b340bca8ab193735fb0c8811e60c33883e05c88ca4', 13)\n",
      "  ('04a6995bf6f8399c83049510f7d97c92f6cd89bfab48579be626244b9363085c', 698)\n",
      "  ('872ca5d13f30ed49157b731c2a8af4b6052ece589862c831da71610e38a929f2', 83)\n",
      "  ('70837fcd2fbee297f2711b4ce86c8688b2bcb4d87ac5402c28c7dee993fdf15f', 0)\n",
      "\n",
      "[WRITE] Writing entity mapping to: /media/vatereal/Main/outputs/entities_model_features_2014.parquet\n",
      "  [WRITE] rows written: 1,000,000\n",
      "  [WRITE] rows written: 2,000,000\n",
      "  [WRITE] rows written: 3,000,000\n",
      "  [WRITE] rows written: 4,000,000\n",
      "  [WRITE] rows written: 5,000,000\n",
      "  [WRITE] rows written: 6,000,000\n",
      "  [WRITE] rows written: 7,000,000\n",
      "  [WRITE] rows written: 8,000,000\n",
      "  [WRITE] rows written: 9,000,000\n",
      "  [WRITE] rows written: 10,000,000\n",
      "  [WRITE] rows written: 11,000,000\n",
      "  [WRITE] rows written: 12,000,000\n",
      "  [WRITE] rows written: 13,000,000\n",
      "  [WRITE] rows written: 14,000,000\n",
      "  [WRITE] rows written: 15,000,000\n",
      "  [WRITE] rows written: 16,000,000\n",
      "  [WRITE] rows written: 17,000,000\n",
      "  [WRITE] rows written: 18,000,000\n",
      "  [WRITE] rows written: 19,000,000\n",
      "  [WRITE] rows written: 20,000,000\n",
      "  [WRITE] rows written: 21,000,000\n",
      "  [WRITE] rows written: 22,000,000\n",
      "  [WRITE] rows written: 23,000,000\n",
      "  [WRITE] rows written: 24,000,000\n",
      "  [WRITE] rows written: 25,000,000\n",
      "  [WRITE] rows written: 26,000,000\n",
      "  [WRITE] rows written: 27,000,000\n",
      "  [WRITE] rows written: 28,000,000\n",
      "  [WRITE] rows written: 29,000,000\n",
      "  [WRITE] rows written: 30,000,000\n",
      "  [WRITE] rows written: 31,000,000\n",
      "  [WRITE] rows written: 32,000,000\n",
      "  [WRITE] rows written: 33,000,000\n",
      "  [WRITE] rows written: 33,269,761\n",
      "[WRITE] Done.\n",
      "\n",
      "[CONF] Writing confidence proxy to: /media/vatereal/Main/outputs/address_confidence_2014.parquet\n",
      "  [CONF] rows written: 1,000,000\n",
      "  [CONF] rows written: 2,000,000\n",
      "  [CONF] rows written: 3,000,000\n",
      "  [CONF] rows written: 4,000,000\n",
      "  [CONF] rows written: 5,000,000\n",
      "  [CONF] rows written: 6,000,000\n",
      "  [CONF] rows written: 7,000,000\n",
      "  [CONF] rows written: 8,000,000\n",
      "  [CONF] rows written: 9,000,000\n",
      "  [CONF] rows written: 10,000,000\n",
      "  [CONF] rows written: 11,000,000\n",
      "  [CONF] rows written: 12,000,000\n",
      "  [CONF] rows written: 13,000,000\n",
      "  [CONF] rows written: 14,000,000\n",
      "  [CONF] rows written: 15,000,000\n",
      "  [CONF] rows written: 16,000,000\n",
      "  [CONF] rows written: 17,000,000\n",
      "  [CONF] rows written: 18,000,000\n",
      "  [CONF] rows written: 19,000,000\n",
      "  [CONF] rows written: 20,000,000\n",
      "  [CONF] rows written: 21,000,000\n",
      "  [CONF] rows written: 22,000,000\n",
      "  [CONF] rows written: 23,000,000\n",
      "  [CONF] rows written: 24,000,000\n",
      "  [CONF] rows written: 25,000,000\n",
      "  [CONF] rows written: 26,000,000\n",
      "  [CONF] rows written: 27,000,000\n",
      "  [CONF] rows written: 28,000,000\n",
      "  [CONF] rows written: 29,000,000\n",
      "  [CONF] rows written: 30,000,000\n",
      "  [CONF] rows written: 31,000,000\n",
      "  [CONF] rows written: 32,000,000\n",
      "  [CONF] rows written: 33,000,000\n",
      "  [CONF] rows written: 33,269,761\n",
      "[CONF] Done.\n",
      "\n",
      "[PLOT] Generating plots...\n",
      "[INFO] Done.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import gc\n",
    "import glob\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sqlite3\n",
    "import sys\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "from datetime import date, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Determinism helpers\n",
    "# =============================================================================\n",
    "\n",
    "def set_determinism(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Makes the pipeline *more reproducible* across runs.\n",
    "    \"\"\"\n",
    "    os.environ.setdefault(\"PYTHONHASHSEED\", str(seed))\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Patch 2: Online quantile calibrator (year-adaptive acceptance, no magic caps)\n",
    "# =============================================================================\n",
    "\n",
    "def _clamp(x: float, lo: float, hi: float) -> float:\n",
    "    return float(max(lo, min(hi, x)))\n",
    "\n",
    "\n",
    "class OnlineQuantileCalibrator:\n",
    "    \"\"\"\n",
    "    Rolling quantile estimator over a fixed-size ring buffer, updated periodically.\n",
    "\n",
    "    Deterministic: uses numpy partition on a copy of the current buffer window.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        quantile: float,\n",
    "        window: int,\n",
    "        update_every: int,\n",
    "        warmup_min_samples: int,\n",
    "        init_value: float,\n",
    "        lo: float,\n",
    "        hi: float,\n",
    "    ):\n",
    "        if not (0.0 < quantile < 1.0):\n",
    "            raise ValueError(\"quantile must be in (0,1)\")\n",
    "        if window <= 0:\n",
    "            raise ValueError(\"window must be > 0\")\n",
    "\n",
    "        self.q = float(quantile)\n",
    "        self.window = int(window)\n",
    "        self.update_every = int(update_every)\n",
    "        self.warmup_min = int(warmup_min_samples)\n",
    "        self.lo = float(lo)\n",
    "        self.hi = float(hi)\n",
    "\n",
    "        self.buf = np.empty(self.window, dtype=np.float32)\n",
    "        self.count = 0\n",
    "        self.idx = 0\n",
    "        self.value = _clamp(float(init_value), self.lo, self.hi)\n",
    "\n",
    "    def update(self, x: float) -> None:\n",
    "        self.buf[self.idx] = float(x)\n",
    "        self.idx = (self.idx + 1) % self.window\n",
    "        self.count += 1\n",
    "\n",
    "        if self.count < self.warmup_min:\n",
    "            return\n",
    "        if (self.count % self.update_every) != 0:\n",
    "            return\n",
    "\n",
    "        n = min(self.count, self.window)\n",
    "        tmp = self.buf[:n].copy()\n",
    "        k = int(max(0, min(n - 1, int(self.q * (n - 1)))))\n",
    "        v = float(np.partition(tmp, k)[k])\n",
    "        self.value = _clamp(v, self.lo, self.hi)\n",
    "\n",
    "\n",
    "class ReservoirSampler:\n",
    "    \"\"\"\n",
    "    Deterministic reservoir sampling given a fixed RNG seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_samples: int, seed: int):\n",
    "        self.max = int(max_samples)\n",
    "        self.n_seen = 0\n",
    "        self.rng = np.random.default_rng(int(seed))\n",
    "\n",
    "    def consider(self) -> Optional[int]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          - None => drop\n",
    "          - idx  => store/replace at idx\n",
    "        \"\"\"\n",
    "        self.n_seen += 1\n",
    "        if self.max <= 0:\n",
    "            return None\n",
    "        if self.n_seen <= self.max:\n",
    "            return self.n_seen - 1\n",
    "        j = int(self.rng.integers(0, self.n_seen))\n",
    "        if j < self.max:\n",
    "            return j\n",
    "        return None\n",
    "\n",
    "\n",
    "class LogitEvalCollector:\n",
    "    \"\"\"\n",
    "    Stores (score,label) pairs with reservoir sampling to cap memory.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_samples: int, seed: int):\n",
    "        self.sampler = ReservoirSampler(max_samples=max_samples, seed=seed)\n",
    "        self.scores: list[float] = []\n",
    "        self.labels: list[int] = []\n",
    "\n",
    "    def add(self, score: float, label: int) -> None:\n",
    "        idx = self.sampler.consider()\n",
    "        if idx is None:\n",
    "            return\n",
    "        if idx == len(self.scores):\n",
    "            self.scores.append(float(score))\n",
    "            self.labels.append(int(label))\n",
    "        else:\n",
    "            self.scores[idx] = float(score)\n",
    "            self.labels[idx] = int(label)\n",
    "\n",
    "    def arrays(self) -> tuple[np.ndarray | None, np.ndarray | None]:\n",
    "        if not self.scores:\n",
    "            return None, None\n",
    "        return np.asarray(self.scores, dtype=np.float32), np.asarray(self.labels, dtype=np.int8)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Config objects (NO DEFAULTS — everything must be explicit)\n",
    "# =============================================================================\n",
    "\n",
    "ValueUnitMode = Literal[\"infer\", \"btc\", \"sats\"]\n",
    "MultiInputPolicy = Literal[\"one_output\", \"one_or_two_nonmix\"]\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PathsConfig:\n",
    "    parquet_io_glob: str\n",
    "    outputs_dir: Path\n",
    "    outpoint_db_path: Path\n",
    "    entity_map_out_path: Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class AnalysisConfig:\n",
    "    analysis_start: date\n",
    "    analysis_end_exclusive: date\n",
    "    outpoint_db_lookback_days: int\n",
    "    value_unit_mode: ValueUnitMode\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class OutputConfig:\n",
    "    reset_outpoint_db: bool\n",
    "    write_entity_mapping: bool\n",
    "    entity_map_compression: str\n",
    "    entity_write_batch: int\n",
    "    top_k_clusters_print: int\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PlotConfig:\n",
    "    enable_plots: bool\n",
    "    zipf_top_k: int\n",
    "    focus_max_size: int\n",
    "    log_bins: int\n",
    "\n",
    "    # NEW: logit performance plots\n",
    "    logit_eval_max_samples: int\n",
    "    logit_calibration_bins: int\n",
    "    logit_roc_points: int\n",
    "\n",
    "    # saving / displaying\n",
    "    save_plots: bool\n",
    "    show_plots: bool\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PerfConfig:\n",
    "    outpoint_commit_every_rows: int\n",
    "    prevout_hybrid_threshold: int\n",
    "    prevout_lookup_chunk: int\n",
    "    iter_buffer_in: int\n",
    "    iter_buffer_out: int\n",
    "    iter_buffer_grouped: int\n",
    "    gc_every_n_files: int\n",
    "    tqdm_mininterval: float\n",
    "    tqdm_miniters: int\n",
    "    tqdm_postfix_every: int\n",
    "\n",
    "    # Patch 3: set-based prevout join path (SQLite join + SQLite aggregates + Polars transport)\n",
    "    use_sql_prevout_join: bool\n",
    "    vinbuf_insert_chunk: int\n",
    "    vinbuf_fetch_chunk: int\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SanityConfig:\n",
    "    run_sanity_checks: bool\n",
    "    run_prevout_db_sanity: bool\n",
    "    prevout_sanity_sample_limit: int\n",
    "    prevout_sanity_parquet: str | None\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class HeuristicToggles:\n",
    "    enable_coinjoin_filter: bool\n",
    "    enable_multi_input: bool\n",
    "    enable_change: bool\n",
    "    enable_merge_guards: bool\n",
    "    precreate_nodes_for_all_output_addrs: bool\n",
    "    create_nodes_for_all_resolved_inputs: bool\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class HeuristicParams:\n",
    "    dust_sats: int\n",
    "    max_fee_abs_sats: int\n",
    "    max_fee_frac: float\n",
    "    max_change_inputs_utxos: int\n",
    "    max_change_spendable_outs: int\n",
    "    change_require_2_outputs: bool\n",
    "    multi_input_policy: MultiInputPolicy\n",
    "\n",
    "\n",
    "UltraVoteRule = tuple[int, int]  # (threshold, votes_required)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class MergeGuardParams:\n",
    "    # Safety ceiling\n",
    "    max_merged_component_size: int\n",
    "\n",
    "    # Ratio guard (CHANGE only) + ignore tiny components (singleton change)\n",
    "    merge_ratio_guard: bool\n",
    "    merge_ratio_max: float\n",
    "    merge_ratio_big_cluster_min: int\n",
    "    merge_ratio_small_floor: int\n",
    "\n",
    "    # Big–big gating (CHANGE only): require multiple \"confirmations\" per component-pair\n",
    "    enable_change_merge_votes: bool\n",
    "    change_vote_min_side: int\n",
    "    change_vote_merged_trigger: int\n",
    "    change_votes_required: int\n",
    "    ultra_change_vote_rules: tuple[UltraVoteRule, ...]\n",
    "\n",
    "    # Degree guard (CHANGE-only): \"change component attempts to attach to too many distinct large anchors\"\n",
    "    enable_change_degree_guard: bool\n",
    "    change_degree_large_min: int\n",
    "    change_degree_small_min: int\n",
    "    change_degree_max_distinct_large: int\n",
    "    change_degree_set_cap: int\n",
    "\n",
    "    # Diagnostics: ratio-guard sample log\n",
    "    ratio_guard_sample_n: int\n",
    "    ratio_guard_sample_path: Path\n",
    "\n",
    "    # Constraint event log\n",
    "    constraint_log_n: int\n",
    "    constraint_log_path: Path\n",
    "\n",
    "    # Debug logging of large merges (optional)\n",
    "    debug_large_change_merges: bool\n",
    "    debug_change_merge_min: int\n",
    "    debug_change_merge_log_path: Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ConfidenceConfig:\n",
    "    \"\"\"\n",
    "    Proxy \"probability\" that an address is *meaningfully clustered* (diagnostic / downstream weight).\n",
    "    This does NOT claim statistical calibration; it's an evidence-based score in [0,1].\n",
    "    \"\"\"\n",
    "    enable_confidence_proxy: bool\n",
    "    confidence_out_path: Path\n",
    "    confidence_compression: str\n",
    "    confidence_write_batch: int\n",
    "\n",
    "    # scoring knobs\n",
    "    size_norm: int\n",
    "    size_bonus_max: float\n",
    "    p_singleton: float\n",
    "    p_base_0: float\n",
    "    p_base_1: float\n",
    "    p_base_2plus: float\n",
    "\n",
    "    include_cluster_size: bool\n",
    "    include_evidence_bits: bool\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Change model: features + logistic scoring + Patch 2 acceptance (adaptive)\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ChangeModelConfig:\n",
    "    \"\"\"\n",
    "    Deterministic logistic scoring + deterministic year-adaptive acceptance via rolling quantiles.\n",
    "\n",
    "    Score:\n",
    "      p = sigmoid(intercept + sum w_i * x_i)\n",
    "\n",
    "    Acceptance (Patch 2):\n",
    "      - maintain rolling window of p_best on \"easy\" txs:\n",
    "          easy := 2-out, non-mixing, resolved-all (and model invoked)\n",
    "      - p_accept := high quantile of p_best (bounded)\n",
    "      - gap threshold:\n",
    "          either fixed min_p_gap_fixed, or rolling quantile of p_gap := p_best - p_second (bounded)\n",
    "    \"\"\"\n",
    "    intercept: float\n",
    "    weights: dict[str, float]\n",
    "\n",
    "    # Fixed gap option (always available)\n",
    "    min_p_gap_fixed: float\n",
    "\n",
    "    # Optional quantile-based gap threshold\n",
    "    p_gap_quantile: float | None\n",
    "    p_gap_window: int\n",
    "    p_gap_update_every: int\n",
    "    p_gap_warmup_min_samples: int\n",
    "    p_gap_init: float\n",
    "    p_gap_min: float\n",
    "    p_gap_max: float\n",
    "\n",
    "    # Quantile-based acceptance threshold\n",
    "    p_accept_quantile: float\n",
    "    p_accept_window: int\n",
    "    p_accept_update_every: int\n",
    "    p_accept_warmup_min_samples: int\n",
    "    p_accept_init: float\n",
    "    p_accept_min: float\n",
    "    p_accept_max: float\n",
    "\n",
    "    # Optional soft \"optimal change\" slack (kept as a feature)\n",
    "    optimal_change_slack_sats: int\n",
    "\n",
    "    # Feature normalization knobs\n",
    "    log1p_fee_norm: float\n",
    "    log1p_value_norm: float\n",
    "\n",
    "\n",
    "def sigmoid(x: float) -> float:\n",
    "    if x >= 0:\n",
    "        z = math.exp(-x)\n",
    "        return 1.0 / (1.0 + z)\n",
    "    z = math.exp(x)\n",
    "    return z / (1.0 + z)\n",
    "\n",
    "\n",
    "def extract_change_features(\n",
    "    *,\n",
    "    n_in_utxos: int,\n",
    "    n_spendable_out: int,\n",
    "    fee_sats: int,\n",
    "    sum_inputs_sats: int,\n",
    "    sum_outputs_sats: int,\n",
    "    min_input_sats: int,\n",
    "    in_type_uniform: bool,\n",
    "    out_type_match_in: bool,\n",
    "    candidate_new: bool,\n",
    "    candidate_value_sats: int,\n",
    "    other_value_sats: int,\n",
    "    candidate_is_min_out: bool,\n",
    "    candidate_lt_other: bool,\n",
    "    is_mixing_like: bool,\n",
    "    cfg: ChangeModelConfig,\n",
    ") -> dict[str, float]:\n",
    "    fee_frac = (fee_sats / max(1, sum_inputs_sats)) if sum_inputs_sats > 0 else 0.0\n",
    "\n",
    "    log_fee = math.log1p(max(0, fee_sats)) / math.log1p(cfg.log1p_fee_norm)\n",
    "    log_cand = math.log1p(max(0, candidate_value_sats)) / math.log1p(cfg.log1p_value_norm)\n",
    "    log_other = math.log1p(max(0, other_value_sats)) / math.log1p(cfg.log1p_value_norm)\n",
    "\n",
    "    optimal_ok = 0.0\n",
    "    if min_input_sats > 0:\n",
    "        if candidate_value_sats <= (min_input_sats - fee_sats + int(cfg.optimal_change_slack_sats)):\n",
    "            optimal_ok = 1.0\n",
    "\n",
    "    return {\n",
    "        \"bias\": 1.0,\n",
    "        \"n_in\": float(n_in_utxos),\n",
    "        \"n_out\": float(n_spendable_out),\n",
    "        \"fee_frac\": float(fee_frac),\n",
    "        \"log_fee\": float(log_fee),\n",
    "        \"log_cand\": float(log_cand),\n",
    "        \"log_other\": float(log_other),\n",
    "        \"in_type_uniform\": 1.0 if in_type_uniform else 0.0,\n",
    "        \"out_type_match_in\": 1.0 if out_type_match_in else 0.0,\n",
    "        \"cand_new\": 1.0 if candidate_new else 0.0,\n",
    "        \"cand_is_min_out\": 1.0 if candidate_is_min_out else 0.0,\n",
    "        \"cand_lt_other\": 1.0 if candidate_lt_other else 0.0,\n",
    "        \"optimal_ok\": float(optimal_ok),\n",
    "        \"is_mixing_like\": 1.0 if is_mixing_like else 0.0,\n",
    "        \"sum_in_gt_sum_out\": 1.0 if (sum_inputs_sats > sum_outputs_sats) else 0.0,\n",
    "    }\n",
    "\n",
    "\n",
    "def score_change_candidate(feats: dict[str, float], cfg: ChangeModelConfig) -> float:\n",
    "    s = float(cfg.intercept)\n",
    "    for k, v in feats.items():\n",
    "        w = cfg.weights.get(k, 0.0)\n",
    "        s += w * float(v)\n",
    "    return float(sigmoid(s))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Utility helpers\n",
    "# =============================================================================\n",
    "\n",
    "def extract_day_from_path(path: str) -> date | None:\n",
    "    p = Path(path)\n",
    "    for part in p.parts:\n",
    "        if part.startswith(\"day=\"):\n",
    "            day_str = part.split(\"=\", 1)[1]\n",
    "            try:\n",
    "                return date.fromisoformat(day_str)\n",
    "            except ValueError:\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def addr_type(addr: str) -> str:\n",
    "    if addr.startswith(\"1\"):\n",
    "        return \"p2pkh\"\n",
    "    if addr.startswith(\"3\"):\n",
    "        return \"p2sh\"\n",
    "    if addr.startswith(\"bc1q\"):\n",
    "        return \"bech32_p2wpkh\"\n",
    "    if addr.startswith(\"bc1p\"):\n",
    "        return \"taproot\"\n",
    "    return \"other\"\n",
    "\n",
    "\n",
    "def infer_value_unit_from_sample(sample_vals: list[float]) -> str:\n",
    "    if not sample_vals:\n",
    "        return \"btc\"\n",
    "    mx = max(sample_vals)\n",
    "    return \"sats\" if mx >= 1e6 else \"btc\"\n",
    "\n",
    "\n",
    "class UnionFind:\n",
    "    \"\"\"\n",
    "    Union-Find with rank, path compression, component sizes,\n",
    "    plus degree-guard metadata.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.parent: list[int] = []\n",
    "        self.rank: list[int] = []\n",
    "        self.size: list[int] = []\n",
    "        self.change_large_anchor_roots: dict[int, set[int]] = {}\n",
    "\n",
    "    def make_set(self) -> int:\n",
    "        idx = len(self.parent)\n",
    "        self.parent.append(idx)\n",
    "        self.rank.append(0)\n",
    "        self.size.append(1)\n",
    "        return idx\n",
    "\n",
    "    def find(self, x: int) -> int:\n",
    "        parent = self.parent\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "\n",
    "    def union_roots(self, rx: int, ry: int, *, change_degree_set_cap: int) -> int:\n",
    "        if rx == ry:\n",
    "            return rx\n",
    "\n",
    "        parent = self.parent\n",
    "        rank = self.rank\n",
    "        size = self.size\n",
    "\n",
    "        if rank[rx] < rank[ry]:\n",
    "            parent[rx] = ry\n",
    "            size[ry] += size[rx]\n",
    "            new_root, old_root = ry, rx\n",
    "        elif rank[rx] > rank[ry]:\n",
    "            parent[ry] = rx\n",
    "            size[rx] += size[ry]\n",
    "            new_root, old_root = rx, ry\n",
    "        else:\n",
    "            parent[ry] = rx\n",
    "            rank[rx] += 1\n",
    "            size[rx] += size[ry]\n",
    "            new_root, old_root = rx, ry\n",
    "\n",
    "        sa = self.change_large_anchor_roots.get(new_root)\n",
    "        sb = self.change_large_anchor_roots.get(old_root)\n",
    "        if sa is None and sb is None:\n",
    "            return new_root\n",
    "\n",
    "        merged: set[int] = set()\n",
    "        if sa:\n",
    "            merged |= sa\n",
    "        if sb:\n",
    "            merged |= sb\n",
    "\n",
    "        if len(merged) > change_degree_set_cap:\n",
    "            merged = set(sorted(merged)[:change_degree_set_cap])\n",
    "\n",
    "        self.change_large_anchor_roots[new_root] = merged\n",
    "        self.change_large_anchor_roots.pop(old_root, None)\n",
    "        return new_root\n",
    "\n",
    "\n",
    "def _count_dupe_values(values: list[int], dust_sats: int) -> tuple[int, int]:\n",
    "    vals = [v for v in values if v >= dust_sats]\n",
    "    if not vals:\n",
    "        return 0, 0\n",
    "    cnt = Counter(vals)\n",
    "    return max(cnt.values()), len(cnt)\n",
    "\n",
    "\n",
    "def detect_mixing_like(n_in_utxos: int, spendable_outs: list[tuple[str, int]], dust_sats: int) -> bool:\n",
    "    n_out = len(spendable_outs)\n",
    "    if n_in_utxos < 3 or n_out < 3:\n",
    "        return False\n",
    "\n",
    "    out_vals = [v for (_a, v) in spendable_outs]\n",
    "    out_types = [addr_type(a) for (a, _v) in spendable_outs if a is not None]\n",
    "    unique_vals = len(set(out_vals))\n",
    "    unique_types = len(set(out_types)) if out_types else 0\n",
    "    max_dupe, distinct_vals_non_dust = _count_dupe_values(out_vals, dust_sats)\n",
    "\n",
    "    if max_dupe >= 2 and distinct_vals_non_dust >= 2:\n",
    "        return True\n",
    "    if abs(n_in_utxos - n_out) <= 1 and min(n_in_utxos, n_out) >= 3:\n",
    "        return True\n",
    "    if n_out >= 4 and unique_vals <= (n_out // 2):\n",
    "        return True\n",
    "    if n_out >= 4 and unique_types == 1 and max_dupe >= 2:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def multi_input_is_safe(policy: MultiInputPolicy, n_in_utxos: int, n_spendable_out: int, is_mixing_like: bool) -> bool:\n",
    "    if n_in_utxos < 2:\n",
    "        return False\n",
    "    if policy == \"one_output\":\n",
    "        return n_spendable_out == 1\n",
    "    if policy == \"one_or_two_nonmix\":\n",
    "        if n_spendable_out == 1:\n",
    "            return True\n",
    "        if n_spendable_out == 2 and not is_mixing_like:\n",
    "            return True\n",
    "        return False\n",
    "    raise ValueError(f\"Unknown multi_input_policy={policy}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SQLite outpoint DB + TEMP join buffers\n",
    "# =============================================================================\n",
    "\n",
    "def open_outpoint_db(db_path: Path, reset: bool) -> sqlite3.Connection:\n",
    "    if reset and db_path.exists():\n",
    "        db_path.unlink()\n",
    "\n",
    "    conn = sqlite3.connect(str(db_path))\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    cur.execute(\"PRAGMA synchronous=OFF;\")\n",
    "    cur.execute(\"PRAGMA temp_store=MEMORY;\")\n",
    "    cur.execute(\"PRAGMA cache_size=-2000000;\")\n",
    "    cur.execute(\"PRAGMA wal_autocheckpoint=5000;\")\n",
    "\n",
    "    conn.isolation_level = None\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS outpoints (\n",
    "            txid TEXT NOT NULL,\n",
    "            n    INTEGER NOT NULL,\n",
    "            address TEXT NOT NULL,\n",
    "            value_sats INTEGER NOT NULL,\n",
    "            PRIMARY KEY(txid, n)\n",
    "        ) WITHOUT ROWID;\n",
    "        \"\"\"\n",
    "    )\n",
    "    return conn\n",
    "\n",
    "\n",
    "def init_lookup_tables(conn: sqlite3.Connection) -> None:\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    cur.execute(\"DROP TABLE IF EXISTS keybuf;\")\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TEMP TABLE keybuf (\n",
    "            txid TEXT NOT NULL,\n",
    "            n    INTEGER NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    cur.execute(\"DROP TABLE IF EXISTS vinbuf;\")\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        CREATE TEMP TABLE vinbuf (\n",
    "            spend_txid TEXT NOT NULL,\n",
    "            prev_txid  TEXT NOT NULL,\n",
    "            prev_n     INTEGER NOT NULL\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "def reset_vinbuf(conn: sqlite3.Connection) -> None:\n",
    "    conn.execute(\"DELETE FROM vinbuf;\")\n",
    "\n",
    "\n",
    "def insert_outpoints_no_commit(conn: sqlite3.Connection, rows: list[tuple[str, int, str, int]]) -> None:\n",
    "    if rows:\n",
    "        conn.executemany(\n",
    "            \"INSERT OR IGNORE INTO outpoints(txid, n, address, value_sats) VALUES (?, ?, ?, ?);\",\n",
    "            rows,\n",
    "        )\n",
    "\n",
    "\n",
    "def lookup_outpoints_or(conn: sqlite3.Connection, keys: list[tuple[str, int]], chunk_size: int) -> dict[tuple[str, int], tuple[str, int]]:\n",
    "    if not keys:\n",
    "        return {}\n",
    "    keys = list(dict.fromkeys(keys))\n",
    "    cur = conn.cursor()\n",
    "    out: dict[tuple[str, int], tuple[str, int]] = {}\n",
    "\n",
    "    for i in range(0, len(keys), chunk_size):\n",
    "        chunk = keys[i : i + chunk_size]\n",
    "        where = \" OR \".join([\"(txid=? AND n=?)\"] * len(chunk))\n",
    "        params = [p for k in chunk for p in k]\n",
    "        cur.execute(f\"SELECT txid, n, address, value_sats FROM outpoints WHERE {where};\", params)\n",
    "        for txid, n, address, value_sats in cur.fetchall():\n",
    "            out[(txid, int(n))] = (address, int(value_sats))\n",
    "    return out\n",
    "\n",
    "\n",
    "def lookup_outpoints_join(conn: sqlite3.Connection, keys: list[tuple[str, int]], chunk_size: int) -> dict[tuple[str, int], tuple[str, int]]:\n",
    "    if not keys:\n",
    "        return {}\n",
    "    keys = list(dict.fromkeys(keys))\n",
    "    cur = conn.cursor()\n",
    "    out: dict[tuple[str, int], tuple[str, int]] = {}\n",
    "\n",
    "    for i in range(0, len(keys), chunk_size):\n",
    "        chunk = keys[i : i + chunk_size]\n",
    "        cur.execute(\"DELETE FROM keybuf;\")\n",
    "        cur.executemany(\"INSERT INTO keybuf(txid, n) VALUES (?, ?);\", chunk)\n",
    "        cur.execute(\n",
    "            \"\"\"\n",
    "            SELECT k.txid, k.n, o.address, o.value_sats\n",
    "            FROM keybuf k\n",
    "            JOIN outpoints o\n",
    "              ON o.txid = k.txid AND o.n = k.n;\n",
    "            \"\"\"\n",
    "        )\n",
    "        for txid, n, address, value_sats in cur.fetchall():\n",
    "            out[(txid, int(n))] = (address, int(value_sats))\n",
    "    return out\n",
    "\n",
    "\n",
    "def lookup_outpoints_hybrid(conn: sqlite3.Connection, keys: list[tuple[str, int]], hybrid_threshold: int, join_chunk: int) -> dict[tuple[str, int], tuple[str, int]]:\n",
    "    if not keys:\n",
    "        return {}\n",
    "    if len(keys) < hybrid_threshold:\n",
    "        return lookup_outpoints_or(conn, keys, chunk_size=500)\n",
    "    return lookup_outpoints_join(conn, keys, chunk_size=join_chunk)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Patch 3: Resolve + aggregate inputs inside SQLite, transport to Polars\n",
    "# =============================================================================\n",
    "\n",
    "def fetch_prevouts_aggs_polars(conn: sqlite3.Connection, fetch_chunk: int) -> pl.DataFrame:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(\n",
    "        \"\"\"\n",
    "        SELECT\n",
    "            v.spend_txid                                 AS txid,\n",
    "            COUNT(*)                                      AS resolved_cnt,\n",
    "            SUM(o.value_sats)                             AS sum_inputs_sats,\n",
    "            MIN(o.value_sats)                             AS min_input_sats,\n",
    "            group_concat(DISTINCT o.address)              AS in_addrs_csv\n",
    "        FROM vinbuf v\n",
    "        JOIN outpoints o\n",
    "          ON o.txid = v.prev_txid AND o.n = v.prev_n\n",
    "        GROUP BY v.spend_txid;\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    dfs: list[pl.DataFrame] = []\n",
    "    while True:\n",
    "        rows = cur.fetchmany(fetch_chunk)\n",
    "        if not rows:\n",
    "            break\n",
    "        dfs.append(\n",
    "            pl.DataFrame(\n",
    "                rows,\n",
    "                schema=[\"txid\", \"resolved_cnt\", \"sum_inputs_sats\", \"min_input_sats\", \"in_addrs_csv\"],\n",
    "                orient=\"row\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    if not dfs:\n",
    "        return pl.DataFrame(\n",
    "            schema={\n",
    "                \"txid\": pl.Utf8,\n",
    "                \"resolved_cnt\": pl.Int64,\n",
    "                \"sum_inputs_sats\": pl.Int64,\n",
    "                \"min_input_sats\": pl.Int64,\n",
    "                \"in_addrs\": pl.List(pl.Utf8),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pl.concat(dfs, how=\"vertical\", rechunk=True)\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"in_addrs_csv\")\n",
    "        .fill_null(\"\")\n",
    "        .str.split(\",\")\n",
    "        .alias(\"in_addrs\")\n",
    "    ).drop(\"in_addrs_csv\")\n",
    "\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"in_addrs\") == [\"\"])\n",
    "        .then(pl.lit([], dtype=pl.List(pl.Utf8)))\n",
    "        .otherwise(pl.col(\"in_addrs\"))\n",
    "        .alias(\"in_addrs\")\n",
    "    )\n",
    "\n",
    "    df = df.with_columns(pl.col(\"in_addrs\").list.sort().alias(\"in_addrs\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Sanity checks + prevout join sanity\n",
    "# =============================================================================\n",
    "\n",
    "OUTPUT_DIR_VALUES = {\"out\", \"vout\", \"output\", \"o\"}\n",
    "INPUT_DIR_VALUES  = {\"in\", \"vin\", \"input\", \"i\"}\n",
    "\n",
    "\n",
    "def run_sanity_checks(\n",
    "    n_nodes: int,\n",
    "    node_to_entity: list[int] | np.ndarray | None = None,\n",
    "    cluster_size_counter: Counter | None = None,\n",
    "    prevout_lookups: int | None = None,\n",
    "    prevout_hits: int | None = None,\n",
    "    top_k: int = 20,\n",
    ") -> None:\n",
    "    if cluster_size_counter is None:\n",
    "        if node_to_entity is None:\n",
    "            raise ValueError(\"Provide either node_to_entity or cluster_size_counter.\")\n",
    "        cluster_size_counter = Counter(list(node_to_entity))\n",
    "\n",
    "    sizes = list(cluster_size_counter.values())\n",
    "    if not sizes:\n",
    "        print(\"[SANITY] No clusters found (sizes empty).\", flush=True)\n",
    "        return\n",
    "\n",
    "    sizes_sorted = sorted(sizes, reverse=True)\n",
    "    total_nodes_from_sizes = sum(sizes_sorted)\n",
    "    largest = sizes_sorted[0]\n",
    "    frac = largest / total_nodes_from_sizes if total_nodes_from_sizes else float(\"nan\")\n",
    "\n",
    "    print(\"\\n[SANITY] Cluster summary\", flush=True)\n",
    "    print(f\"  UF nodes (n_nodes): {n_nodes:,}\", flush=True)\n",
    "    print(f\"  Total nodes from cluster sizes: {total_nodes_from_sizes:,}\", flush=True)\n",
    "    if total_nodes_from_sizes != n_nodes:\n",
    "        print(\"  [WARN] sum(cluster_sizes) != n_nodes  -> mismatch suggests a bug in mapping logic.\", flush=True)\n",
    "    print(f\"  Entities (clusters): {len(cluster_size_counter):,}\", flush=True)\n",
    "    print(f\"  Largest cluster size: {largest:,}\", flush=True)\n",
    "    print(f\"  Largest cluster fraction of nodes: {frac:.2%}\", flush=True)\n",
    "\n",
    "    print(f\"\\n[SANITY] Top {top_k} cluster sizes:\", flush=True)\n",
    "    print(\" \", sizes_sorted[:top_k], flush=True)\n",
    "\n",
    "    def pct(p: float) -> int:\n",
    "        if not sizes_sorted:\n",
    "            return 0\n",
    "        s_asc = sorted(sizes_sorted)\n",
    "        idx = max(0, min(len(s_asc) - 1, math.ceil(p * len(s_asc)) - 1))\n",
    "        return int(s_asc[idx])\n",
    "\n",
    "    med = int(sorted(sizes_sorted)[len(sizes_sorted) // 2])\n",
    "    print(\"\\n[SANITY] Quick distribution stats\", flush=True)\n",
    "    print(f\"  Median cluster size: {med:,}\", flush=True)\n",
    "    print(f\"  90th percentile cluster size: {pct(0.90):,}\", flush=True)\n",
    "    print(f\"  99th percentile cluster size: {pct(0.99):,}\", flush=True)\n",
    "\n",
    "    if prevout_lookups is not None and prevout_hits is not None:\n",
    "        rate = (prevout_hits / prevout_lookups) if prevout_lookups else float(\"nan\")\n",
    "        print(\"\\n[SANITY] Prevout lookup hit-rate (DB)\", flush=True)\n",
    "        print(f\"  Lookups:  {prevout_lookups:,}\", flush=True)\n",
    "        print(f\"  Hits:     {prevout_hits:,}\", flush=True)\n",
    "        print(f\"  Hit-rate: {rate:.2%}\", flush=True)\n",
    "\n",
    "\n",
    "def prevout_join_sanity_db(\n",
    "    conn: sqlite3.Connection,\n",
    "    parquet_path: str,\n",
    "    sample_limit: int,\n",
    "    hybrid_threshold: int,\n",
    "    join_chunk: int,\n",
    ") -> None:\n",
    "    df = pl.read_parquet(parquet_path, columns=[\"dir\", \"txid\", \"prev_txid\", \"prev_vout\"])\n",
    "    df = df.with_columns(pl.col(\"dir\").cast(pl.Utf8, strict=False).str.to_lowercase().alias(\"dir\"))\n",
    "    vin = (\n",
    "        df.filter(pl.col(\"dir\").is_in(list(INPUT_DIR_VALUES)))\n",
    "        .select([pl.col(\"txid\").alias(\"spend_txid\"), \"prev_txid\", \"prev_vout\"])\n",
    "        .filter(pl.col(\"prev_txid\").is_not_null() & pl.col(\"prev_vout\").is_not_null())\n",
    "        .head(sample_limit)\n",
    "    )\n",
    "    print(\"\\n[PREVOUT-SANITY | DB]\", flush=True)\n",
    "    print(\"File:\", parquet_path, flush=True)\n",
    "    print(\"Vin sample rows:\", vin.height, flush=True)\n",
    "    if vin.height == 0:\n",
    "        return\n",
    "\n",
    "    frac_integerlike = (\n",
    "        vin.with_columns(((pl.col(\"prev_vout\") - pl.col(\"prev_vout\").floor()).abs() < 1e-9).alias(\"is_intlike\"))\n",
    "           .select(pl.col(\"is_intlike\").mean())\n",
    "           .item()\n",
    "    )\n",
    "    print(f\"prev_vout integer-like fraction (sample): {float(frac_integerlike):.6f}\", flush=True)\n",
    "\n",
    "    vin2 = vin.with_columns(pl.col(\"prev_vout\").cast(pl.Int64, strict=False).alias(\"prev_vout_i64\")).filter(\n",
    "        pl.col(\"prev_vout_i64\").is_not_null()\n",
    "    )\n",
    "\n",
    "    keys = [(tx, int(n)) for tx, n in vin2.select([\"prev_txid\", \"prev_vout_i64\"]).iter_rows(named=False, buffer_size=200_000)]\n",
    "    hits = lookup_outpoints_hybrid(conn, keys, hybrid_threshold=hybrid_threshold, join_chunk=join_chunk)\n",
    "    print(f\"DB prevout hit-rate (sample): {len(hits)}/{len(keys)} = {len(hits)/len(keys):.2%}\" if keys else \"No keys\", flush=True)\n",
    "\n",
    "    unresolved = []\n",
    "    for k in keys[:5000]:\n",
    "        if k not in hits:\n",
    "            unresolved.append(k)\n",
    "            if len(unresolved) >= 10:\n",
    "                break\n",
    "    if unresolved:\n",
    "        print(\"Sample unresolved (prev_txid, prev_n):\", flush=True)\n",
    "        for k in unresolved:\n",
    "            print(\" \", k, flush=True)\n",
    "    else:\n",
    "        print(\"No unresolved keys in first 5000 keys (good sign).\", flush=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Plot helpers\n",
    "# =============================================================================\n",
    "\n",
    "def _log_bins(max_val: int, bins: int) -> np.ndarray:\n",
    "    max_val = max(1, int(max_val))\n",
    "    return np.logspace(0, math.log10(max_val), num=bins)\n",
    "\n",
    "\n",
    "def _maybe_save_show(save_path: Optional[Path], show: bool) -> None:\n",
    "    if save_path is not None:\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        plt.savefig(str(save_path), dpi=140)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_hist_logbins(\n",
    "    sizes: np.ndarray,\n",
    "    title: str,\n",
    "    color: str,\n",
    "    bins: int,\n",
    "    max_x: int | None,\n",
    "    *,\n",
    "    save_path: Optional[Path],\n",
    "    show: bool,\n",
    ") -> None:\n",
    "    if sizes.size == 0:\n",
    "        print(f\"[PLOT] Skipping empty plot: {title}\", flush=True)\n",
    "        return\n",
    "    if max_x is None:\n",
    "        max_x = int(sizes.max())\n",
    "    b = _log_bins(max_x, bins)\n",
    "    plt.figure(figsize=(11, 6))\n",
    "    plt.hist(sizes, bins=b, log=True, color=color, edgecolor=\"black\", linewidth=0.7)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Cluster size (addresses per entity)\")\n",
    "    plt.ylabel(\"Frequency (log scale)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.35, alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    _maybe_save_show(save_path, show)\n",
    "\n",
    "\n",
    "def plot_zipf(\n",
    "    sizes: np.ndarray,\n",
    "    title: str,\n",
    "    color: str,\n",
    "    top_k: int,\n",
    "    *,\n",
    "    save_path: Optional[Path],\n",
    "    show: bool,\n",
    ") -> None:\n",
    "    if sizes.size == 0:\n",
    "        print(f\"[PLOT] Skipping empty plot: {title}\", flush=True)\n",
    "        return\n",
    "    top_k = min(top_k, sizes.size)\n",
    "    top = np.partition(sizes, -top_k)[-top_k:]\n",
    "    top_sorted = np.sort(top)[::-1]\n",
    "    ranks = np.arange(1, top_sorted.size + 1)\n",
    "\n",
    "    plt.figure(figsize=(11, 6))\n",
    "    plt.plot(ranks, top_sorted, color=color, linewidth=1.3)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.xlabel(\"Rank (log scale)\")\n",
    "    plt.ylabel(\"Cluster size (log scale)\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, which=\"both\", linestyle=\"--\", linewidth=0.35, alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    _maybe_save_show(save_path, show)\n",
    "\n",
    "\n",
    "def plot_logit_eval(\n",
    "    scores: np.ndarray,\n",
    "    labels: np.ndarray,\n",
    "    *,\n",
    "    roc_points: int,\n",
    "    calib_bins: int,\n",
    "    title_prefix: str,\n",
    "    save_roc: Optional[Path],\n",
    "    save_calib: Optional[Path],\n",
    "    show: bool,\n",
    ") -> None:\n",
    "    if scores.size == 0:\n",
    "        print(\"[PLOT] No logit eval samples.\", flush=True)\n",
    "        return\n",
    "\n",
    "    # ROC\n",
    "    thr = np.linspace(0.0, 1.0, int(roc_points))\n",
    "    P = max(1, int(labels.sum()))\n",
    "    N = max(1, int((1 - labels).sum()))\n",
    "\n",
    "    tpr = np.empty_like(thr)\n",
    "    fpr = np.empty_like(thr)\n",
    "    for i, t in enumerate(thr):\n",
    "        pred = scores >= t\n",
    "        tp = int(np.sum(pred & (labels == 1)))\n",
    "        fp = int(np.sum(pred & (labels == 0)))\n",
    "        tpr[i] = tp / P\n",
    "        fpr[i] = fp / N\n",
    "\n",
    "    order = np.argsort(fpr)\n",
    "    fpr_s = fpr[order]\n",
    "    tpr_s = tpr[order]\n",
    "    auc = float(np.trapz(tpr_s, fpr_s))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_s, tpr_s, linewidth=1.5)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1.0)\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.title(f\"{title_prefix} ROC (proxy AUC={auc:.3f})\")\n",
    "    plt.grid(True, linestyle=\"--\", linewidth=0.35, alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    _maybe_save_show(save_roc, show)\n",
    "\n",
    "    # Calibration\n",
    "    bins = np.linspace(0.0, 1.0, int(calib_bins) + 1)\n",
    "    which = np.digitize(scores, bins) - 1  # [0..calib_bins-1]\n",
    "    xs, ys = [], []\n",
    "    for b in range(int(calib_bins)):\n",
    "        m = which == b\n",
    "        if int(np.sum(m)) < 50:\n",
    "            continue\n",
    "        xs.append(float(np.mean(scores[m])))\n",
    "        ys.append(float(np.mean(labels[m])))\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(xs, ys, marker=\"o\", linewidth=1.2)\n",
    "    plt.plot([0, 1], [0, 1], linestyle=\"--\", linewidth=1.0)\n",
    "    plt.xlabel(\"Predicted p_best\")\n",
    "    plt.ylabel(\"Empirical proxy rate (label)\")\n",
    "    plt.title(f\"{title_prefix} Calibration (proxy label)\")\n",
    "    plt.grid(True, linestyle=\"--\", linewidth=0.35, alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    _maybe_save_show(save_calib, show)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Reporting helpers (all final prints live here)\n",
    "# =============================================================================\n",
    "\n",
    "def compute_top_cluster_evidence_composition(\n",
    "    *,\n",
    "    node_to_entity: np.ndarray,\n",
    "    multi_change_flags: bytearray,\n",
    "    top_entity_id: int,\n",
    ") -> dict[str, float | int]:\n",
    "    n_nodes = int(node_to_entity.size)\n",
    "    if n_nodes == 0:\n",
    "        return {\n",
    "            \"top_entity_id\": int(top_entity_id),\n",
    "            \"n_top\": 0,\n",
    "            \"n_bit1\": 0, \"n_bit2\": 0, \"n_bit4\": 0,\n",
    "            \"n_bit2_only\": 0,\n",
    "            \"share_bit1\": float(\"nan\"),\n",
    "            \"share_bit2\": float(\"nan\"),\n",
    "            \"share_bit4\": float(\"nan\"),\n",
    "            \"share_bit2_only\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    bits = np.frombuffer(multi_change_flags, dtype=np.uint8, count=n_nodes)\n",
    "    mask = (node_to_entity == int(top_entity_id))\n",
    "    n_top = int(mask.sum())\n",
    "    if n_top == 0:\n",
    "        return {\n",
    "            \"top_entity_id\": int(top_entity_id),\n",
    "            \"n_top\": 0,\n",
    "            \"n_bit1\": 0, \"n_bit2\": 0, \"n_bit4\": 0,\n",
    "            \"n_bit2_only\": 0,\n",
    "            \"share_bit1\": float(\"nan\"),\n",
    "            \"share_bit2\": float(\"nan\"),\n",
    "            \"share_bit4\": float(\"nan\"),\n",
    "            \"share_bit2_only\": float(\"nan\"),\n",
    "        }\n",
    "\n",
    "    top_bits = bits[mask]\n",
    "    n_bit1 = int(np.count_nonzero(top_bits & 1))\n",
    "    n_bit2 = int(np.count_nonzero(top_bits & 2))\n",
    "    n_bit4 = int(np.count_nonzero(top_bits & 4))\n",
    "    n_bit2_only = int(np.count_nonzero(top_bits == 2))\n",
    "\n",
    "    return {\n",
    "        \"top_entity_id\": int(top_entity_id),\n",
    "        \"n_top\": n_top,\n",
    "        \"n_bit1\": n_bit1,\n",
    "        \"n_bit2\": n_bit2,\n",
    "        \"n_bit4\": n_bit4,\n",
    "        \"n_bit2_only\": n_bit2_only,\n",
    "        \"share_bit1\": float(n_bit1 / n_top),\n",
    "        \"share_bit2\": float(n_bit2 / n_top),\n",
    "        \"share_bit4\": float(n_bit4 / n_top),\n",
    "        \"share_bit2_only\": float(n_bit2_only / n_top),\n",
    "    }\n",
    "\n",
    "\n",
    "def print_final_report(\n",
    "    *,\n",
    "    n_nodes: int,\n",
    "    n_entities: int,\n",
    "    node_to_entity: np.ndarray,\n",
    "    counts: np.ndarray,\n",
    "    cluster_sizes: np.ndarray,\n",
    "    stats: dict[str, int],\n",
    "    multi_change_flags: bytearray,\n",
    "    outputs: OutputConfig,\n",
    "    guards: MergeGuardParams,\n",
    "    enable_constraints_diag: bool,\n",
    "    constraint_unique_pairs: set[tuple[int, int]],\n",
    "    constraint_gate_evals_total: int,\n",
    "    constraint_pairs_repeated: int,\n",
    "    change_merge_votes: dict[tuple[int, int], int],\n",
    "    prevout_lookups: int,\n",
    "    prevout_hits: int,\n",
    "    # Patch 2 / model diagnostics\n",
    "    model_diag: dict[str, float | int],\n",
    ") -> int:\n",
    "    print(\"\\nFinalizing entity mapping (compressing components)...\", flush=True)\n",
    "    print(f\"Number of unique addresses with UF nodes: {n_nodes:,}\", flush=True)\n",
    "\n",
    "    hit_rate = prevout_hits / max(1, prevout_lookups)\n",
    "    print(f\"[INFO] prevout lookups: {prevout_lookups:,}  hits: {prevout_hits:,}  hit-rate: {hit_rate:.3%}\", flush=True)\n",
    "    print(f\"Number of entities (clusters): {n_entities:,}\", flush=True)\n",
    "\n",
    "    if n_entities == 0:\n",
    "        print(\"[WARN] No entities computed.\", flush=True)\n",
    "        return -1\n",
    "\n",
    "    top_k = min(outputs.top_k_clusters_print, n_entities)\n",
    "    top_ids = np.argpartition(counts, -top_k)[-top_k:]\n",
    "    top_ids_sorted = top_ids[np.argsort(counts[top_ids])[::-1]]\n",
    "    top_entity_id = int(top_ids_sorted[0])\n",
    "\n",
    "    print(\"\\n[TOP CLUSTERS]\", flush=True)\n",
    "    for rank, eid in enumerate(top_ids_sorted, start=1):\n",
    "        sz = int(counts[eid])\n",
    "        frac = (sz / n_nodes) if n_nodes else float(\"nan\")\n",
    "        print(f\"  #{rank:02d}  entity_id={int(eid):>8d}  size={sz:>10,d}  frac={frac:>7.2%}\", flush=True)\n",
    "\n",
    "    largest = int(cluster_sizes.max()) if cluster_sizes.size else 0\n",
    "    largest_frac = (largest / n_nodes) if n_nodes else float(\"nan\")\n",
    "    q50, q90, q99 = np.quantile(cluster_sizes, [0.5, 0.9, 0.99]) if cluster_sizes.size else (0, 0, 0)\n",
    "\n",
    "    print(\"\\n[CLUSTER STATS]\", flush=True)\n",
    "    print(f\"  Entities: {n_entities:,}\", flush=True)\n",
    "    print(f\"  Largest cluster size: {largest:,}\", flush=True)\n",
    "    print(f\"  Largest cluster fraction of nodes: {largest_frac:.2%}\", flush=True)\n",
    "    print(f\"  Median cluster size: {int(q50)}\", flush=True)\n",
    "    print(f\"  90th percentile: {int(q90)}\", flush=True)\n",
    "    print(f\"  99th percentile: {int(q99)}\", flush=True)\n",
    "\n",
    "    comp = compute_top_cluster_evidence_composition(\n",
    "        node_to_entity=node_to_entity,\n",
    "        multi_change_flags=multi_change_flags,\n",
    "        top_entity_id=top_entity_id,\n",
    "    )\n",
    "\n",
    "    print(\"\\n[TOP CLUSTER EVIDENCE COMPOSITION]\", flush=True)\n",
    "    print(f\"  Top entity_id: {comp['top_entity_id']:,}\", flush=True)\n",
    "    print(f\"  Addresses in top cluster: {comp['n_top']:,}\", flush=True)\n",
    "    print(f\"  bit1 (multi-input)   : {comp['n_bit1']:,}  share={comp['share_bit1']:.2%}\", flush=True)\n",
    "    print(f\"  bit2 (change-output) : {comp['n_bit2']:,}  share={comp['share_bit2']:.2%}\", flush=True)\n",
    "    print(f\"  bit4 (change-anchor) : {comp['n_bit4']:,}  share={comp['share_bit4']:.2%}\", flush=True)\n",
    "    print(f\"  bit2-only            : {comp['n_bit2_only']:,}  share={comp['share_bit2_only']:.2%}\", flush=True)\n",
    "    print(\"  (collapse signature often: very high bit2-only share + low bit1 share)\", flush=True)\n",
    "\n",
    "    flags_view = multi_change_flags[:n_nodes]\n",
    "    n_addrs_multi = sum(1 for v in flags_view if (v & 1))\n",
    "    n_addrs_change_out = sum(1 for v in flags_view if (v & 2))\n",
    "    n_addrs_change_anchor = sum(1 for v in flags_view if (v & 4))\n",
    "    n_addrs_touched = sum(1 for v in flags_view if (v & 7))\n",
    "\n",
    "    print(\"\\n[HEURISTIC COVERAGE (node-level)]\", flush=True)\n",
    "    print(f\"  Total txs processed (>=1 input UTXO): {stats['n_txs_total']:,}\", flush=True)\n",
    "    print(f\"  Mixing-like skipped: {stats['n_txs_coinjoin_flagged']:,}\", flush=True)\n",
    "    print(f\"  Multi-input txs (SAFE policy applied): {stats['n_txs_with_multiinput']:,}\", flush=True)\n",
    "    print(f\"  Change detected (model): {stats['n_txs_with_change_detected']:,}\", flush=True)\n",
    "    print(f\"  Nodes marked multi-input: {n_addrs_multi:,}\", flush=True)\n",
    "    print(f\"  Nodes marked change-output: {n_addrs_change_out:,}\", flush=True)\n",
    "    print(f\"  Nodes marked change-anchor: {n_addrs_change_anchor:,}\", flush=True)\n",
    "    print(f\"  Nodes touched by any heuristic: {n_addrs_touched:,}\", flush=True)\n",
    "\n",
    "    print(\"\\n[MODEL CALIBRATION (Patch 2)]\", flush=True)\n",
    "    print(f\"  easy txs seen (calibration stream): {int(model_diag.get('easy_seen', 0)):,}\", flush=True)\n",
    "    print(f\"  easy txs accepted: {int(model_diag.get('easy_accepted', 0)):,}\", flush=True)\n",
    "    denom = max(1, int(model_diag.get('easy_seen', 0)))\n",
    "    print(f\"  easy accept-rate: {int(model_diag.get('easy_accepted', 0))/denom:.4%}\", flush=True)\n",
    "    print(f\"  p_accept (rolling quantile): {float(model_diag.get('p_accept', float('nan'))):.4f}\", flush=True)\n",
    "    print(f\"  p_gap threshold: {float(model_diag.get('p_gap', float('nan'))):.4f}\", flush=True)\n",
    "    print(f\"  logit eval samples stored: {int(model_diag.get('eval_samples', 0)):,}\", flush=True)\n",
    "\n",
    "    print(\"\\n[UNION DIAGNOSTICS]\", flush=True)\n",
    "    print(f\"  Union attempts: {stats['union_attempts']:,}\", flush=True)\n",
    "    print(f\"  Unions applied: {stats['unions_applied']:,}\", flush=True)\n",
    "    print(f\"    - applied via H1:     {stats['unions_applied_h1']:,}\", flush=True)\n",
    "    print(f\"    - applied via CHANGE: {stats['unions_applied_change']:,}\", flush=True)\n",
    "\n",
    "    print(f\"  skip_same_component (already merged): {stats['skip_same_component']:,}\", flush=True)\n",
    "    print(f\"    - same_component via H1:     {stats['skip_same_component_h1']:,}\", flush=True)\n",
    "    print(f\"    - same_component via CHANGE: {stats['skip_same_component_change']:,}\", flush=True)\n",
    "\n",
    "    print(f\"  Unions skipped by guards/votes/constraints: {stats['unions_skipped']:,}\", flush=True)\n",
    "    print(f\"    - skipped (H1):     {stats['unions_skipped_h1']:,}\", flush=True)\n",
    "    print(f\"    - skipped (CHANGE): {stats['unions_skipped_change']:,}\", flush=True)\n",
    "\n",
    "    print(\"  Skip breakdown:\", flush=True)\n",
    "    print(f\"    - skip_abs_cap: {stats['skip_abs_cap']:,}  (H1={stats['skip_abs_cap_h1']:,}, CHANGE={stats['skip_abs_cap_change']:,})\", flush=True)\n",
    "    print(f\"    - skip_ratio_guard: {stats['skip_ratio_guard']:,}  (CHANGE-only)\", flush=True)\n",
    "    print(f\"    - skip_constraint: {stats['skip_constraint']:,}  (CHANGE-only; vote pending)\", flush=True)\n",
    "    print(f\"    - skip_degree_guard: {stats['skip_degree_guard']:,}  (CHANGE-only)\", flush=True)\n",
    "    print(f\"    - skip_vote_pending: {stats['skip_vote_pending']:,}\", flush=True)\n",
    "    print(f\"    - skip_vote_failed:  {stats['skip_vote_failed']:,}\", flush=True)\n",
    "\n",
    "    if enable_constraints_diag:\n",
    "        print(\"\\n[CONSTRAINT DIAGNOSTICS]\", flush=True)\n",
    "        print(f\"  Vote-gating pairs seen (unique): {len(constraint_unique_pairs):,}\", flush=True)\n",
    "        print(f\"  Vote-gating evaluations (total): {constraint_gate_evals_total:,}\", flush=True)\n",
    "        print(f\"  Vote-gating pairs that repeated: {constraint_pairs_repeated:,}\", flush=True)\n",
    "        active_pairs = sum(1 for _k, v in change_merge_votes.items() if v > 0)\n",
    "        print(f\"  Vote pairs tracked (total): {active_pairs:,}\", flush=True)\n",
    "\n",
    "    return top_entity_id\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Main executor\n",
    "# =============================================================================\n",
    "\n",
    "def run_entity_clustering(\n",
    "    paths: PathsConfig,\n",
    "    analysis: AnalysisConfig,\n",
    "    outputs: OutputConfig,\n",
    "    plots: PlotConfig,\n",
    "    perf: PerfConfig,\n",
    "    sanity: SanityConfig,\n",
    "    toggles: HeuristicToggles,\n",
    "    params: HeuristicParams,\n",
    "    guards: MergeGuardParams,\n",
    "    confidence: ConfidenceConfig,\n",
    "    change_model: ChangeModelConfig,\n",
    "    *,\n",
    "    determinism_seed: int,\n",
    ") -> None:\n",
    "    set_determinism(determinism_seed)\n",
    "\n",
    "    try:\n",
    "        sys.stdout.reconfigure(line_buffering=True)\n",
    "        sys.stderr.reconfigure(line_buffering=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    pl.Config.set_tbl_rows(20)\n",
    "    pl.Config.set_fmt_str_lengths(80)\n",
    "\n",
    "    paths.outputs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    io_paths = sorted(glob.glob(paths.parquet_io_glob))\n",
    "    print(\"Parquet file counts:\", {\"io\": len(io_paths)}, flush=True)\n",
    "    print(f\"[INFO] Analysis window: {analysis.analysis_start} .. {analysis.analysis_end_exclusive} (exclusive)\", flush=True)\n",
    "\n",
    "    index_start = analysis.analysis_start - timedelta(days=analysis.outpoint_db_lookback_days)\n",
    "    print(f\"[INFO] Outpoint DB preload start: {index_start} .. {analysis.analysis_end_exclusive} (exclusive)\", flush=True)\n",
    "    print(f\"[INFO] Outpoint DB path: {paths.outpoint_db_path} (reset={outputs.reset_outpoint_db})\", flush=True)\n",
    "\n",
    "    eligible: list[tuple[str, date]] = []\n",
    "    for path in io_paths:\n",
    "        d = extract_day_from_path(path)\n",
    "        if d is None:\n",
    "            continue\n",
    "        if index_start <= d < analysis.analysis_end_exclusive:\n",
    "            eligible.append((path, d))\n",
    "\n",
    "    n_preload_total = sum(1 for _p, d in eligible if d < analysis.analysis_start)\n",
    "    n_analysis_total = len(eligible) - n_preload_total\n",
    "    print(f\"[INFO] Eligible files in window: {len(eligible)}  (preload={n_preload_total}, analysis={n_analysis_total})\", flush=True)\n",
    "\n",
    "    if not eligible:\n",
    "        print(\"[WARN] No eligible parquet files found for the requested window.\", flush=True)\n",
    "        return\n",
    "\n",
    "    sanity_parquet = sanity.prevout_sanity_parquet\n",
    "    if sanity_parquet is None:\n",
    "        for p, d in eligible:\n",
    "            if analysis.analysis_start <= d < analysis.analysis_end_exclusive:\n",
    "                sanity_parquet = p\n",
    "                break\n",
    "        if sanity_parquet is None and eligible:\n",
    "            sanity_parquet = eligible[0][0]\n",
    "    if sanity.run_sanity_checks and sanity_parquet:\n",
    "        print(f\"[INFO] Sanity parquet selected: {sanity_parquet}\", flush=True)\n",
    "\n",
    "    conn = open_outpoint_db(paths.outpoint_db_path, reset=outputs.reset_outpoint_db)\n",
    "    init_lookup_tables(conn)\n",
    "\n",
    "    conn.execute(\"BEGIN;\")\n",
    "    pending_outpoint_rows = 0\n",
    "\n",
    "    uf = UnionFind()\n",
    "    addr_to_id: dict[str, int] = {}\n",
    "\n",
    "    # Evidence flags:\n",
    "    # bit 1: multi-input evidence\n",
    "    # bit 2: change-output evidence\n",
    "    # bit 4: change-anchor evidence\n",
    "    multi_change_flags = bytearray()\n",
    "    seen_output_flags = bytearray()\n",
    "\n",
    "    seen_output_addrs: set[str] | None\n",
    "    if toggles.precreate_nodes_for_all_output_addrs:\n",
    "        seen_output_addrs = None\n",
    "    else:\n",
    "        seen_output_addrs = set()\n",
    "\n",
    "    def ensure_flag_capacity(idx: int) -> None:\n",
    "        needed = idx + 1\n",
    "        cur = len(seen_output_flags)\n",
    "        if cur < needed:\n",
    "            delta = needed - cur\n",
    "            seen_output_flags.extend(b\"\\x00\" * delta)\n",
    "            multi_change_flags.extend(b\"\\x00\" * delta)\n",
    "\n",
    "    def get_addr_id(addr: str) -> int:\n",
    "        idx = addr_to_id.get(addr)\n",
    "        if idx is None:\n",
    "            idx = uf.make_set()\n",
    "            addr_to_id[addr] = idx\n",
    "            ensure_flag_capacity(idx)\n",
    "        return idx\n",
    "\n",
    "    # value unit mode\n",
    "    if analysis.value_unit_mode == \"btc\":\n",
    "        value_unit: str | None = \"btc\"\n",
    "    elif analysis.value_unit_mode == \"sats\":\n",
    "        value_unit = \"sats\"\n",
    "    elif analysis.value_unit_mode == \"infer\":\n",
    "        value_unit = None\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown value_unit_mode={analysis.value_unit_mode}\")\n",
    "\n",
    "    def value_expr_to_sats() -> pl.Expr:\n",
    "        nonlocal value_unit\n",
    "        if value_unit is None:\n",
    "            return (pl.col(\"value\").cast(pl.Float64, strict=False) * 100_000_000).round(0).cast(pl.Int64, strict=False)\n",
    "        if value_unit == \"sats\":\n",
    "            return pl.col(\"value\").cast(pl.Float64, strict=False).round(0).cast(pl.Int64, strict=False)\n",
    "        return (pl.col(\"value\").cast(pl.Float64, strict=False) * 100_000_000).round(0).cast(pl.Int64, strict=False)\n",
    "\n",
    "    stats = {\n",
    "        \"n_files_indexed\": 0,\n",
    "        \"n_files_analyzed\": 0,\n",
    "        \"n_txs_total\": 0,\n",
    "        \"n_txs_coinjoin_flagged\": 0,\n",
    "        \"n_txs_with_multiinput\": 0,\n",
    "        \"n_txs_with_change_detected\": 0,\n",
    "        \"n_prevout_lookups\": 0,\n",
    "        \"n_prevout_hits\": 0,\n",
    "\n",
    "        \"union_attempts\": 0,\n",
    "        \"unions_applied\": 0,\n",
    "        \"unions_applied_h1\": 0,\n",
    "        \"unions_applied_change\": 0,\n",
    "\n",
    "        \"skip_same_component\": 0,\n",
    "        \"skip_same_component_h1\": 0,\n",
    "        \"skip_same_component_change\": 0,\n",
    "\n",
    "        \"unions_skipped\": 0,\n",
    "        \"unions_skipped_h1\": 0,\n",
    "        \"unions_skipped_change\": 0,\n",
    "\n",
    "        \"skip_abs_cap\": 0,\n",
    "        \"skip_abs_cap_h1\": 0,\n",
    "        \"skip_abs_cap_change\": 0,\n",
    "\n",
    "        \"skip_ratio_guard\": 0,\n",
    "        \"skip_ratio_guard_change\": 0,\n",
    "\n",
    "        \"skip_constraint\": 0,\n",
    "        \"skip_constraint_change\": 0,\n",
    "\n",
    "        \"skip_degree_guard\": 0,\n",
    "        \"skip_degree_guard_change\": 0,\n",
    "\n",
    "        \"skip_vote_pending\": 0,\n",
    "        \"skip_vote_failed\": 0,\n",
    "    }\n",
    "\n",
    "    # Patch 2 calibrators (year-adaptive acceptance)\n",
    "    p_accept_cal = OnlineQuantileCalibrator(\n",
    "        quantile=change_model.p_accept_quantile,\n",
    "        window=change_model.p_accept_window,\n",
    "        update_every=change_model.p_accept_update_every,\n",
    "        warmup_min_samples=change_model.p_accept_warmup_min_samples,\n",
    "        init_value=change_model.p_accept_init,\n",
    "        lo=change_model.p_accept_min,\n",
    "        hi=change_model.p_accept_max,\n",
    "    )\n",
    "\n",
    "    p_gap_cal: OnlineQuantileCalibrator | None = None\n",
    "    if change_model.p_gap_quantile is not None:\n",
    "        p_gap_cal = OnlineQuantileCalibrator(\n",
    "            quantile=float(change_model.p_gap_quantile),\n",
    "            window=change_model.p_gap_window,\n",
    "            update_every=change_model.p_gap_update_every,\n",
    "            warmup_min_samples=change_model.p_gap_warmup_min_samples,\n",
    "            init_value=change_model.p_gap_init,\n",
    "            lo=change_model.p_gap_min,\n",
    "            hi=change_model.p_gap_max,\n",
    "        )\n",
    "\n",
    "    easy_seen = 0\n",
    "    easy_accepted = 0\n",
    "\n",
    "    # Logit eval collection (proxy labels)\n",
    "    eval_col = LogitEvalCollector(max_samples=plots.logit_eval_max_samples, seed=determinism_seed + analysis.analysis_start.year)\n",
    "\n",
    "    change_merge_votes: dict[tuple[int, int], int] = {}\n",
    "    constraint_gate_evals_total = 0\n",
    "    constraint_pair_counts: dict[tuple[int, int], int] = {}\n",
    "    constraint_unique_pairs: set[tuple[int, int]] = set()\n",
    "    constraint_pairs_repeated = 0\n",
    "\n",
    "    ratio_guard_samples_written = 0\n",
    "    ratio_guard_sample_fh = None\n",
    "    try:\n",
    "        if guards.ratio_guard_sample_n > 0:\n",
    "            ratio_guard_sample_fh = open(guards.ratio_guard_sample_path, \"w\", encoding=\"utf-8\")\n",
    "            ratio_guard_sample_fh.write(\n",
    "                \"txid\\tsa\\tsb\\ts_big\\ts_small\\tratio\\tchange_side\\tbig_side\\tbig_is_change\\tchange_comp_size\\tinput_comp_size\\n\"\n",
    "            )\n",
    "    except Exception:\n",
    "        ratio_guard_sample_fh = None\n",
    "\n",
    "    constraint_log_written = 0\n",
    "    constraint_log_fh = None\n",
    "    try:\n",
    "        if guards.constraint_log_n > 0:\n",
    "            constraint_log_fh = open(guards.constraint_log_path, \"w\", encoding=\"utf-8\")\n",
    "            constraint_log_fh.write(\n",
    "                \"txid\\tevent\\treason\\tsa\\tsb\\tmerged\\tmin_side\\tbig_side\\tratio\\t\"\n",
    "                \"votes\\tvotes_required\\tkey_r0\\tkey_r1\\ttriggered_by\\tbig_is_change\\t\"\n",
    "                \"pair_seen\\tis_repeat\\tchange_root\\tanchor_root\\tchange_distinct_large\\n\"\n",
    "            )\n",
    "    except Exception:\n",
    "        constraint_log_fh = None\n",
    "\n",
    "    def _votes_required_for_sizes(min_side: int, big_side: int, merged: int) -> int:\n",
    "        req = guards.change_votes_required\n",
    "        for thr, req_thr in guards.ultra_change_vote_rules:\n",
    "            if big_side >= thr or merged >= thr:\n",
    "                if req_thr > req:\n",
    "                    req = req_thr\n",
    "        return req\n",
    "\n",
    "    def _log_constraint_event(\n",
    "        *,\n",
    "        txid: str | None,\n",
    "        event: str,\n",
    "        reason: str,\n",
    "        sa: int,\n",
    "        sb: int,\n",
    "        merged: int,\n",
    "        min_side: int,\n",
    "        big_side: int,\n",
    "        ratio: float,\n",
    "        votes: int,\n",
    "        votes_required: int,\n",
    "        key_r0: int,\n",
    "        key_r1: int,\n",
    "        triggered_by: str,\n",
    "        big_is_change: int,\n",
    "        pair_seen: int,\n",
    "        is_repeat: int,\n",
    "        change_root: int,\n",
    "        anchor_root: int,\n",
    "        change_distinct_large: int,\n",
    "    ) -> None:\n",
    "        nonlocal constraint_log_written\n",
    "        if (\n",
    "            constraint_log_fh is None\n",
    "            or constraint_log_written >= guards.constraint_log_n\n",
    "            or txid is None\n",
    "        ):\n",
    "            return\n",
    "        constraint_log_fh.write(\n",
    "            f\"{txid}\\t{event}\\t{reason}\\t{sa}\\t{sb}\\t{merged}\\t{min_side}\\t{big_side}\\t{ratio}\\t\"\n",
    "            f\"{votes}\\t{votes_required}\\t{key_r0}\\t{key_r1}\\t{triggered_by}\\t{big_is_change}\\t\"\n",
    "            f\"{pair_seen}\\t{is_repeat}\\t{change_root}\\t{anchor_root}\\t{change_distinct_large}\\n\"\n",
    "        )\n",
    "        constraint_log_written += 1\n",
    "\n",
    "    def union_guarded(a: int, b: int, reason: Literal[\"H1\", \"CHANGE\"], txid: str | None) -> bool:\n",
    "        nonlocal ratio_guard_samples_written\n",
    "        nonlocal constraint_gate_evals_total, constraint_pairs_repeated\n",
    "\n",
    "        stats[\"union_attempts\"] += 1\n",
    "        ra = uf.find(a)\n",
    "        rb = uf.find(b)\n",
    "        if ra == rb:\n",
    "            stats[\"skip_same_component\"] += 1\n",
    "            if reason == \"H1\":\n",
    "                stats[\"skip_same_component_h1\"] += 1\n",
    "            else:\n",
    "                stats[\"skip_same_component_change\"] += 1\n",
    "            return False\n",
    "\n",
    "        sa = uf.size[ra]\n",
    "        sb = uf.size[rb]\n",
    "        merged = sa + sb\n",
    "\n",
    "        if not toggles.enable_merge_guards:\n",
    "            uf.union_roots(ra, rb, change_degree_set_cap=guards.change_degree_set_cap)\n",
    "            stats[\"unions_applied\"] += 1\n",
    "            if reason == \"H1\":\n",
    "                stats[\"unions_applied_h1\"] += 1\n",
    "            else:\n",
    "                stats[\"unions_applied_change\"] += 1\n",
    "            return True\n",
    "\n",
    "        if merged > guards.max_merged_component_size:\n",
    "            stats[\"unions_skipped\"] += 1\n",
    "            if reason == \"H1\":\n",
    "                stats[\"unions_skipped_h1\"] += 1\n",
    "                stats[\"skip_abs_cap_h1\"] += 1\n",
    "            else:\n",
    "                stats[\"unions_skipped_change\"] += 1\n",
    "                stats[\"skip_abs_cap_change\"] += 1\n",
    "            stats[\"skip_abs_cap\"] += 1\n",
    "            return False\n",
    "\n",
    "        if guards.merge_ratio_guard and reason == \"CHANGE\":\n",
    "            s_big = sa if sa >= sb else sb\n",
    "            s_small = sb if sa >= sb else sa\n",
    "\n",
    "            if s_small >= guards.merge_ratio_small_floor and s_big >= guards.merge_ratio_big_cluster_min:\n",
    "                ratio = (s_big / s_small) if s_small > 0 else float(\"inf\")\n",
    "                if ratio > guards.merge_ratio_max:\n",
    "                    stats[\"unions_skipped\"] += 1\n",
    "                    stats[\"unions_skipped_change\"] += 1\n",
    "                    stats[\"skip_ratio_guard\"] += 1\n",
    "                    stats[\"skip_ratio_guard_change\"] += 1\n",
    "\n",
    "                    if (\n",
    "                        ratio_guard_sample_fh is not None\n",
    "                        and ratio_guard_samples_written < guards.ratio_guard_sample_n\n",
    "                        and txid is not None\n",
    "                    ):\n",
    "                        big_root = ra if sa >= sb else rb\n",
    "                        big_is_change = 1 if big_root == rb else 0\n",
    "                        big_side = \"b\" if big_is_change else \"a\"\n",
    "                        ratio_guard_sample_fh.write(\n",
    "                            f\"{txid}\\t{sa}\\t{sb}\\t{s_big}\\t{s_small}\\t{ratio}\\t\"\n",
    "                            f\"b\\t{big_side}\\t{big_is_change}\\t{sb}\\t{sa}\\n\"\n",
    "                        )\n",
    "                        ratio_guard_samples_written += 1\n",
    "                    return False\n",
    "\n",
    "        if guards.enable_change_degree_guard and reason == \"CHANGE\":\n",
    "            anchor_root = ra\n",
    "            change_root = rb\n",
    "            anchor_size = sa\n",
    "            change_size = sb\n",
    "\n",
    "            change_distinct_large = 0\n",
    "            if (anchor_size >= guards.change_degree_large_min) and (change_size >= guards.change_degree_small_min):\n",
    "                s = uf.change_large_anchor_roots.get(change_root)\n",
    "                if s is None:\n",
    "                    s = set()\n",
    "                    uf.change_large_anchor_roots[change_root] = s\n",
    "                s.add(anchor_root)\n",
    "                if len(s) > guards.change_degree_set_cap:\n",
    "                    uf.change_large_anchor_roots[change_root] = set(sorted(s)[:guards.change_degree_set_cap])\n",
    "                    s = uf.change_large_anchor_roots[change_root]\n",
    "                change_distinct_large = len(s)\n",
    "\n",
    "                if change_distinct_large > guards.change_degree_max_distinct_large:\n",
    "                    stats[\"unions_skipped\"] += 1\n",
    "                    stats[\"unions_skipped_change\"] += 1\n",
    "                    stats[\"skip_degree_guard\"] += 1\n",
    "                    stats[\"skip_degree_guard_change\"] += 1\n",
    "\n",
    "                    s_big = max(sa, sb)\n",
    "                    s_small = min(sa, sb)\n",
    "                    ratio = (s_big / s_small) if s_small > 0 else float(\"inf\")\n",
    "                    big_root = ra if sa >= sb else rb\n",
    "                    big_is_change = 1 if big_root == rb else 0\n",
    "                    r0, r1 = (anchor_root, change_root) if anchor_root < change_root else (change_root, anchor_root)\n",
    "\n",
    "                    _log_constraint_event(\n",
    "                        txid=txid,\n",
    "                        event=\"DEGREE_GUARD\",\n",
    "                        reason=reason,\n",
    "                        sa=sa, sb=sb,\n",
    "                        merged=merged,\n",
    "                        min_side=min(sa, sb),\n",
    "                        big_side=max(sa, sb),\n",
    "                        ratio=ratio,\n",
    "                        votes=0,\n",
    "                        votes_required=0,\n",
    "                        key_r0=r0,\n",
    "                        key_r1=r1,\n",
    "                        triggered_by=\"degree_guard\",\n",
    "                        big_is_change=big_is_change,\n",
    "                        pair_seen=0,\n",
    "                        is_repeat=0,\n",
    "                        change_root=change_root,\n",
    "                        anchor_root=anchor_root,\n",
    "                        change_distinct_large=change_distinct_large,\n",
    "                    )\n",
    "                    return False\n",
    "\n",
    "        if guards.enable_change_merge_votes and reason == \"CHANGE\":\n",
    "            min_side = sa if sa <= sb else sb\n",
    "            big_side = sa if sa >= sb else sb\n",
    "\n",
    "            triggered = False\n",
    "            triggered_by = \"none\"\n",
    "\n",
    "            if guards.change_vote_min_side > 0 and min_side >= guards.change_vote_min_side:\n",
    "                triggered = True\n",
    "                triggered_by = \"min_side\"\n",
    "            if (not triggered) and guards.change_vote_merged_trigger > 0 and merged >= guards.change_vote_merged_trigger:\n",
    "                triggered = True\n",
    "                triggered_by = \"merged\"\n",
    "\n",
    "            if triggered:\n",
    "                r0, r1 = (ra, rb) if ra < rb else (rb, ra)\n",
    "                k = (r0, r1)\n",
    "\n",
    "                constraint_gate_evals_total += 1\n",
    "                prev_seen = constraint_pair_counts.get(k, 0)\n",
    "                pair_seen = prev_seen + 1\n",
    "                constraint_pair_counts[k] = pair_seen\n",
    "                if pair_seen == 1:\n",
    "                    constraint_unique_pairs.add(k)\n",
    "                elif pair_seen == 2:\n",
    "                    constraint_pairs_repeated += 1\n",
    "                is_repeat = 1 if pair_seen > 1 else 0\n",
    "\n",
    "                prev_votes = change_merge_votes.get(k, 0)\n",
    "                votes = prev_votes + 1\n",
    "                change_merge_votes[k] = votes\n",
    "\n",
    "                votes_required = _votes_required_for_sizes(min_side=min_side, big_side=big_side, merged=merged)\n",
    "\n",
    "                if votes < votes_required:\n",
    "                    stats[\"unions_skipped\"] += 1\n",
    "                    stats[\"unions_skipped_change\"] += 1\n",
    "                    stats[\"skip_constraint\"] += 1\n",
    "                    stats[\"skip_constraint_change\"] += 1\n",
    "                    stats[\"skip_vote_pending\"] += 1\n",
    "\n",
    "                    s_big = max(sa, sb)\n",
    "                    s_small = min(sa, sb)\n",
    "                    ratio = (s_big / s_small) if s_small > 0 else float(\"inf\")\n",
    "                    big_root = ra if sa >= sb else rb\n",
    "                    big_is_change = 1 if big_root == rb else 0\n",
    "\n",
    "                    _log_constraint_event(\n",
    "                        txid=txid,\n",
    "                        event=\"VOTE_PENDING\",\n",
    "                        reason=reason,\n",
    "                        sa=sa, sb=sb,\n",
    "                        merged=merged,\n",
    "                        min_side=min_side,\n",
    "                        big_side=big_side,\n",
    "                        ratio=ratio,\n",
    "                        votes=votes,\n",
    "                        votes_required=votes_required,\n",
    "                        key_r0=r0,\n",
    "                        key_r1=r1,\n",
    "                        triggered_by=triggered_by,\n",
    "                        big_is_change=big_is_change,\n",
    "                        pair_seen=pair_seen,\n",
    "                        is_repeat=is_repeat,\n",
    "                        change_root=rb,\n",
    "                        anchor_root=ra,\n",
    "                        change_distinct_large=0,\n",
    "                    )\n",
    "                    return False\n",
    "\n",
    "        if guards.debug_large_change_merges and reason == \"CHANGE\" and merged >= guards.debug_change_merge_min and txid is not None:\n",
    "            try:\n",
    "                with open(guards.debug_change_merge_log_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(f\"{txid}\\t{sa}\\t{sb}\\t{merged}\\n\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        uf.union_roots(ra, rb, change_degree_set_cap=guards.change_degree_set_cap)\n",
    "        stats[\"unions_applied\"] += 1\n",
    "        if reason == \"H1\":\n",
    "            stats[\"unions_applied_h1\"] += 1\n",
    "        else:\n",
    "            stats[\"unions_applied_change\"] += 1\n",
    "        return True\n",
    "\n",
    "    pbar = tqdm(\n",
    "        eligible,\n",
    "        total=len(eligible),\n",
    "        unit=\"file\",\n",
    "        dynamic_ncols=True,\n",
    "        mininterval=perf.tqdm_mininterval,\n",
    "        miniters=perf.tqdm_miniters,\n",
    "        smoothing=0,\n",
    "        desc=\"Processing parquet\",\n",
    "        file=sys.stdout,\n",
    "        leave=True,\n",
    "    )\n",
    "\n",
    "    DIR_NEEDS_NORMALIZATION: bool | None = None\n",
    "\n",
    "    def log(msg: str) -> None:\n",
    "        tqdm.write(msg, file=sys.stdout)\n",
    "\n",
    "    def mark_outputs_seen(outputs_list: list[tuple[str | None, int]]) -> None:\n",
    "        if toggles.precreate_nodes_for_all_output_addrs:\n",
    "            for a, _v in outputs_list:\n",
    "                if a is None:\n",
    "                    continue\n",
    "                a_id = get_addr_id(a)\n",
    "                seen_output_flags[a_id] = 1\n",
    "        else:\n",
    "            assert seen_output_addrs is not None\n",
    "            for a, _v in outputs_list:\n",
    "                if a is not None:\n",
    "                    seen_output_addrs.add(a)\n",
    "\n",
    "    for i, (path, file_day) in enumerate(pbar, start=1):\n",
    "        in_analysis = (analysis.analysis_start <= file_day < analysis.analysis_end_exclusive)\n",
    "\n",
    "        if (i % perf.tqdm_postfix_every) == 0:\n",
    "            pbar.set_postfix_str(f\"{'analysis' if in_analysis else 'preload'} day={file_day}\")\n",
    "\n",
    "        if in_analysis:\n",
    "            stats[\"n_files_analyzed\"] += 1\n",
    "            if stats[\"n_files_analyzed\"] <= 5:\n",
    "                log(f\"[{stats['n_files_analyzed']}] Processing {path} (day={file_day}) ...\")\n",
    "            elif (stats[\"n_files_analyzed\"] % 250) == 0:\n",
    "                log(f\"[{stats['n_files_analyzed']}] Processing ... (day={file_day})\")\n",
    "        else:\n",
    "            stats[\"n_files_indexed\"] += 1\n",
    "            if stats[\"n_files_indexed\"] <= 3:\n",
    "                log(f\"[preload {stats['n_files_indexed']}] Indexing outputs only: {path} (day={file_day}) ...\")\n",
    "\n",
    "        df = pl.read_parquet(path, columns=[\"dir\", \"txid\", \"n\", \"prev_txid\", \"prev_vout\", \"address\", \"value\"])\n",
    "\n",
    "        if DIR_NEEDS_NORMALIZATION is None:\n",
    "            dir_uniques = df.select(pl.col(\"dir\").cast(pl.Utf8, strict=False).unique()).to_series().to_list()\n",
    "            DIR_NEEDS_NORMALIZATION = any(\n",
    "                (d is not None and str(d).lower() in (\"in\", \"out\") and str(d) not in (\"in\", \"out\")) for d in dir_uniques\n",
    "            )\n",
    "            if in_analysis and stats[\"n_files_analyzed\"] == 1:\n",
    "                log(f\"  [debug] distinct dir values in first processed file: {dir_uniques}\")\n",
    "\n",
    "        if DIR_NEEDS_NORMALIZATION:\n",
    "            df = df.with_columns(pl.col(\"dir\").cast(pl.Utf8).str.to_lowercase().alias(\"dir\"))\n",
    "\n",
    "        if value_unit is None:\n",
    "            sample = (\n",
    "                df.filter(pl.col(\"dir\") == \"out\")\n",
    "                .select(pl.col(\"value\").cast(pl.Float64, strict=False))\n",
    "                .drop_nulls()\n",
    "                .head(5000)\n",
    "                .to_series()\n",
    "                .to_list()\n",
    "            )\n",
    "            value_unit = infer_value_unit_from_sample(sample)\n",
    "            log(f\"[INFO] Detected value unit: {value_unit}  (all internal amount logic uses satoshis)\")\n",
    "\n",
    "        out_all = (\n",
    "            df.filter(pl.col(\"dir\") == \"out\")\n",
    "            .filter(pl.col(\"txid\").is_not_null())\n",
    "            .filter(pl.col(\"n\").is_not_null())\n",
    "            .select(\n",
    "                pl.col(\"txid\"),\n",
    "                pl.col(\"n\").cast(pl.Int64, strict=False).alias(\"n\"),\n",
    "                pl.col(\"address\"),\n",
    "                value_expr_to_sats().alias(\"value_sats\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        out_for_db = out_all.filter(pl.col(\"address\").is_not_null()).filter(pl.col(\"value_sats\").is_not_null())\n",
    "        if out_for_db.height > 0:\n",
    "            rows = [\n",
    "                (txid, int(n), addr, int(vs))\n",
    "                for txid, n, addr, vs in out_for_db.select([\"txid\", \"n\", \"address\", \"value_sats\"]).iter_rows(\n",
    "                    named=False, buffer_size=perf.iter_buffer_out\n",
    "                )\n",
    "            ]\n",
    "            insert_outpoints_no_commit(conn, rows)\n",
    "            pending_outpoint_rows += len(rows)\n",
    "\n",
    "            if pending_outpoint_rows >= perf.outpoint_commit_every_rows:\n",
    "                conn.execute(\"COMMIT;\")\n",
    "                conn.execute(\"BEGIN;\")\n",
    "                pending_outpoint_rows = 0\n",
    "\n",
    "            if toggles.precreate_nodes_for_all_output_addrs:\n",
    "                out_u = out_for_db.select(pl.col(\"address\").unique()).to_series().to_list()\n",
    "                for addr in sorted(out_u):\n",
    "                    _ = get_addr_id(addr)\n",
    "\n",
    "        if not in_analysis:\n",
    "            del df, out_all, out_for_db\n",
    "            if (i % perf.gc_every_n_files) == 0:\n",
    "                gc.collect()\n",
    "            continue\n",
    "\n",
    "        in_df = (\n",
    "            df.filter(pl.col(\"dir\") == \"in\")\n",
    "            .filter(pl.col(\"txid\").is_not_null())\n",
    "            .filter(pl.col(\"prev_txid\").is_not_null())\n",
    "            .filter(pl.col(\"prev_vout\").is_not_null())\n",
    "            .select(\n",
    "                pl.col(\"txid\").alias(\"spend_txid\"),\n",
    "                pl.col(\"prev_txid\"),\n",
    "                pl.col(\"prev_vout\").cast(pl.Int64, strict=False).alias(\"prev_n\"),\n",
    "            )\n",
    "            .filter(pl.col(\"prev_n\").is_not_null())\n",
    "        )\n",
    "\n",
    "        del df\n",
    "        if (i % perf.gc_every_n_files) == 0:\n",
    "            gc.collect()\n",
    "\n",
    "        if in_df.height == 0:\n",
    "            del in_df\n",
    "            del out_all, out_for_db\n",
    "            if (i % perf.gc_every_n_files) == 0:\n",
    "                gc.collect()\n",
    "            continue\n",
    "\n",
    "        in_counts = (\n",
    "            in_df.group_by(\"spend_txid\")\n",
    "            .len()\n",
    "            .rename({\"spend_txid\": \"txid\", \"len\": \"n_in_utxos\"})\n",
    "        )\n",
    "\n",
    "        if not perf.use_sql_prevout_join:\n",
    "            raise RuntimeError(\"perf.use_sql_prevout_join=False is not supported in this build; set it True.\")\n",
    "\n",
    "        reset_vinbuf(conn)\n",
    "\n",
    "        cur = conn.cursor()\n",
    "        insert_sql = \"INSERT INTO vinbuf(spend_txid, prev_txid, prev_n) VALUES (?, ?, ?);\"\n",
    "        batch: list[tuple[str, str, int]] = []\n",
    "        total_vin = 0\n",
    "\n",
    "        for spend_txid, prev_txid, prev_n in in_df.select([\"spend_txid\", \"prev_txid\", \"prev_n\"]).iter_rows(\n",
    "            named=False, buffer_size=perf.iter_buffer_in\n",
    "        ):\n",
    "            batch.append((str(spend_txid), str(prev_txid), int(prev_n)))\n",
    "            if len(batch) >= perf.vinbuf_insert_chunk:\n",
    "                cur.executemany(insert_sql, batch)\n",
    "                total_vin += len(batch)\n",
    "                batch.clear()\n",
    "\n",
    "        if batch:\n",
    "            cur.executemany(insert_sql, batch)\n",
    "            total_vin += len(batch)\n",
    "            batch.clear()\n",
    "\n",
    "        stats[\"n_prevout_lookups\"] += total_vin\n",
    "\n",
    "        resolved_aggs = fetch_prevouts_aggs_polars(conn, fetch_chunk=perf.vinbuf_fetch_chunk)\n",
    "\n",
    "        if resolved_aggs.height > 0:\n",
    "            resolved_inputs_sum = resolved_aggs.select(pl.col(\"resolved_cnt\").sum()).item()\n",
    "            stats[\"n_prevout_hits\"] += int(resolved_inputs_sum) if resolved_inputs_sum is not None else 0\n",
    "\n",
    "        del in_df\n",
    "\n",
    "        if toggles.create_nodes_for_all_resolved_inputs and resolved_aggs.height > 0:\n",
    "            u = (\n",
    "                resolved_aggs\n",
    "                .select(pl.col(\"in_addrs\").explode().drop_nulls().unique())\n",
    "                .to_series()\n",
    "                .to_list()\n",
    "            )\n",
    "            for a in sorted(u):\n",
    "                _ = get_addr_id(a)\n",
    "\n",
    "        if out_all.height == 0:\n",
    "            del out_all, out_for_db, in_counts, resolved_aggs\n",
    "            if (i % perf.gc_every_n_files) == 0:\n",
    "                gc.collect()\n",
    "            continue\n",
    "\n",
    "        vout_grouped = (\n",
    "            out_all.group_by(\"txid\")\n",
    "            .agg(\n",
    "                pl.col(\"address\").alias(\"out_addrs_all\"),\n",
    "                pl.col(\"value_sats\").alias(\"out_values_sats_all\"),\n",
    "            )\n",
    "        )\n",
    "        del out_all, out_for_db\n",
    "\n",
    "        tx_joined = (\n",
    "            vout_grouped\n",
    "            .join(in_counts, on=\"txid\", how=\"inner\")\n",
    "            .join(resolved_aggs, on=\"txid\", how=\"left\")\n",
    "        )\n",
    "        del vout_grouped, in_counts, resolved_aggs\n",
    "\n",
    "        for (\n",
    "            txid,\n",
    "            out_addrs_all,\n",
    "            out_values_all,\n",
    "            n_in_utxos,\n",
    "            resolved_cnt,\n",
    "            sum_inputs_sats,\n",
    "            min_input_sats,\n",
    "            in_addrs,\n",
    "        ) in tx_joined.select(\n",
    "            [\"txid\", \"out_addrs_all\", \"out_values_sats_all\", \"n_in_utxos\", \"resolved_cnt\", \"sum_inputs_sats\", \"min_input_sats\", \"in_addrs\"]\n",
    "        ).iter_rows(named=False, buffer_size=perf.iter_buffer_grouped):\n",
    "\n",
    "            if not out_addrs_all or not out_values_all:\n",
    "                continue\n",
    "\n",
    "            n_in_utxos_i = int(n_in_utxos) if n_in_utxos is not None else 0\n",
    "            if n_in_utxos_i == 0:\n",
    "                continue\n",
    "\n",
    "            stats[\"n_txs_total\"] += 1\n",
    "\n",
    "            outputs_list: list[tuple[str | None, int]] = []\n",
    "            sum_outputs_sats = 0\n",
    "            for a, v in zip(out_addrs_all, out_values_all):\n",
    "                if v is None:\n",
    "                    continue\n",
    "                vi = int(v)\n",
    "                outputs_list.append((a, vi))\n",
    "                sum_outputs_sats += vi\n",
    "\n",
    "            spendable = [(a, v) for (a, v) in outputs_list if a is not None and v >= params.dust_sats]\n",
    "            if not spendable:\n",
    "                mark_outputs_seen(outputs_list)\n",
    "                continue\n",
    "\n",
    "            is_mixing_like = False\n",
    "            if toggles.enable_coinjoin_filter:\n",
    "                is_mixing_like = detect_mixing_like(n_in_utxos_i, spendable, dust_sats=params.dust_sats)\n",
    "                if is_mixing_like:\n",
    "                    stats[\"n_txs_coinjoin_flagged\"] += 1\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "            if in_addrs is None:\n",
    "                mark_outputs_seen(outputs_list)\n",
    "                continue\n",
    "\n",
    "            in_addrs_list = list(in_addrs)\n",
    "            if not in_addrs_list:\n",
    "                mark_outputs_seen(outputs_list)\n",
    "                continue\n",
    "\n",
    "            # ---- Multi-input (H1) ----\n",
    "            if toggles.enable_multi_input:\n",
    "                safe = multi_input_is_safe(params.multi_input_policy, n_in_utxos_i, len(spendable), is_mixing_like)\n",
    "                if safe:\n",
    "                    stats[\"n_txs_with_multiinput\"] += 1\n",
    "                    if len(in_addrs_list) >= 2:\n",
    "                        in_ids = [get_addr_id(a) for a in in_addrs_list]\n",
    "                        for idx in in_ids:\n",
    "                            multi_change_flags[idx] |= 1\n",
    "                        first_id = in_ids[0]\n",
    "                        for idx in in_ids[1:]:\n",
    "                            union_guarded(first_id, idx, reason=\"H1\", txid=str(txid))\n",
    "                    else:\n",
    "                        idx = get_addr_id(in_addrs_list[0])\n",
    "                        multi_change_flags[idx] |= 1\n",
    "\n",
    "            # ---- Change (logit + Patch 2 acceptance) ----\n",
    "            if toggles.enable_change:\n",
    "                n_spendable_out = len(spendable)\n",
    "\n",
    "                if params.change_require_2_outputs and n_spendable_out != 2:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                if n_in_utxos_i > params.max_change_inputs_utxos:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "                if n_spendable_out > params.max_change_spendable_outs:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                resolved_cnt_i = int(resolved_cnt) if resolved_cnt is not None else 0\n",
    "                resolved_all = (resolved_cnt_i == n_in_utxos_i)\n",
    "                if not resolved_all:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                in_types = [addr_type(a) for a in in_addrs_list]\n",
    "                in_type_uniform = (len(set(in_types)) == 1)\n",
    "                majority_type = in_types[0] if in_types else \"other\"\n",
    "                if not in_type_uniform:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                spendable_addrs = [a for (a, _v) in spendable]\n",
    "                if len(spendable_addrs) != len(set(spendable_addrs)):\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                in_addr_set_fast = set(in_addrs_list)\n",
    "\n",
    "                candidates: list[tuple[str, int, bool]] = []\n",
    "                for a, v in spendable:\n",
    "                    if a in in_addr_set_fast:\n",
    "                        continue\n",
    "                    if addr_type(a) != majority_type:\n",
    "                        continue\n",
    "\n",
    "                    is_new = True\n",
    "                    if toggles.precreate_nodes_for_all_output_addrs:\n",
    "                        a_id = get_addr_id(a)\n",
    "                        if seen_output_flags[a_id]:\n",
    "                            is_new = False\n",
    "                    else:\n",
    "                        assert seen_output_addrs is not None\n",
    "                        if a in seen_output_addrs:\n",
    "                            is_new = False\n",
    "\n",
    "                    candidates.append((a, v, is_new))\n",
    "\n",
    "                if len(candidates) != 2:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                (cand_a, cand_v, cand_new), (cand_b, cand_w, cand_new2) = candidates\n",
    "\n",
    "                sum_in_i = int(sum_inputs_sats) if sum_inputs_sats is not None else 0\n",
    "                fee = sum_in_i - sum_outputs_sats\n",
    "                if sum_in_i <= 0 or fee < 0:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "                if fee > int(params.max_fee_abs_sats) or fee > int(params.max_fee_frac * sum_in_i):\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                other_a, other_v, other_new = cand_b, cand_w, cand_new2\n",
    "                min_in = int(min_input_sats) if min_input_sats is not None else 0\n",
    "\n",
    "                feats1 = extract_change_features(\n",
    "                    n_in_utxos=n_in_utxos_i,\n",
    "                    n_spendable_out=n_spendable_out,\n",
    "                    fee_sats=fee,\n",
    "                    sum_inputs_sats=sum_in_i,\n",
    "                    sum_outputs_sats=sum_outputs_sats,\n",
    "                    min_input_sats=min_in,\n",
    "                    in_type_uniform=in_type_uniform,\n",
    "                    out_type_match_in=True,\n",
    "                    candidate_new=cand_new,\n",
    "                    candidate_value_sats=cand_v,\n",
    "                    other_value_sats=other_v,\n",
    "                    candidate_is_min_out=(cand_v <= other_v),\n",
    "                    candidate_lt_other=(cand_v < other_v),\n",
    "                    is_mixing_like=is_mixing_like,\n",
    "                    cfg=change_model,\n",
    "                )\n",
    "                p1 = score_change_candidate(feats1, change_model)\n",
    "\n",
    "                feats2 = extract_change_features(\n",
    "                    n_in_utxos=n_in_utxos_i,\n",
    "                    n_spendable_out=n_spendable_out,\n",
    "                    fee_sats=fee,\n",
    "                    sum_inputs_sats=sum_in_i,\n",
    "                    sum_outputs_sats=sum_outputs_sats,\n",
    "                    min_input_sats=min_in,\n",
    "                    in_type_uniform=in_type_uniform,\n",
    "                    out_type_match_in=True,\n",
    "                    candidate_new=other_new,\n",
    "                    candidate_value_sats=other_v,\n",
    "                    other_value_sats=cand_v,\n",
    "                    candidate_is_min_out=(other_v <= cand_v),\n",
    "                    candidate_lt_other=(other_v < cand_v),\n",
    "                    is_mixing_like=is_mixing_like,\n",
    "                    cfg=change_model,\n",
    "                )\n",
    "                p2 = score_change_candidate(feats2, change_model)\n",
    "\n",
    "                # best vs runner-up\n",
    "                if p2 > p1:\n",
    "                    p_best, change_addr, change_val = p2, other_a, other_v\n",
    "                    p_second = p1\n",
    "                    chosen_is_min = (other_v <= cand_v)\n",
    "                else:\n",
    "                    p_best, change_addr, change_val = p1, cand_a, cand_v\n",
    "                    p_second = p2\n",
    "                    chosen_is_min = (cand_v <= other_v)\n",
    "\n",
    "                gap = float(p_best - p_second)\n",
    "\n",
    "                # proxy eval label: \"chosen change is the min output\"\n",
    "                eval_col.add(float(p_best), 1 if chosen_is_min else 0)\n",
    "\n",
    "                # Patch 2: acceptance thresholds from calibrators (no year-specific caps)\n",
    "                thr_accept = float(p_accept_cal.value)\n",
    "                thr_gap = float(p_gap_cal.value) if p_gap_cal is not None else float(change_model.min_p_gap_fixed)\n",
    "\n",
    "                accept = (float(p_best) >= thr_accept) and (gap >= thr_gap)\n",
    "\n",
    "                # define \"easy\" stream: 2-out, non-mixing, resolved-all, model invoked\n",
    "                easy_seen += 1\n",
    "                if accept:\n",
    "                    easy_accepted += 1\n",
    "\n",
    "                # update calibrators AFTER using current threshold (no lookahead)\n",
    "                p_accept_cal.update(float(p_best))\n",
    "                if p_gap_cal is not None:\n",
    "                    p_gap_cal.update(gap)\n",
    "\n",
    "                if not accept:\n",
    "                    mark_outputs_seen(outputs_list)\n",
    "                    continue\n",
    "\n",
    "                stats[\"n_txs_with_change_detected\"] += 1\n",
    "\n",
    "                anchor_id = get_addr_id(in_addrs_list[0])\n",
    "                change_id = get_addr_id(change_addr)\n",
    "\n",
    "                multi_change_flags[change_id] |= 2\n",
    "                multi_change_flags[anchor_id] |= 4\n",
    "\n",
    "                union_guarded(anchor_id, change_id, reason=\"CHANGE\", txid=str(txid))\n",
    "\n",
    "            mark_outputs_seen(outputs_list)\n",
    "\n",
    "        del tx_joined\n",
    "        if (i % perf.gc_every_n_files) == 0:\n",
    "            gc.collect()\n",
    "\n",
    "    pbar.close()\n",
    "    conn.execute(\"COMMIT;\")\n",
    "\n",
    "    try:\n",
    "        if ratio_guard_sample_fh is not None:\n",
    "            ratio_guard_sample_fh.close()\n",
    "            log(f\"[DIAG] Wrote ratio-guard sample log: {guards.ratio_guard_sample_path}  (n={ratio_guard_samples_written})\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if constraint_log_fh is not None:\n",
    "            constraint_log_fh.close()\n",
    "            log(f\"[DIAG] Wrote constraint-event log: {guards.constraint_log_path}  (n={constraint_log_written})\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Finalize entity mapping\n",
    "    n_nodes = len(addr_to_id)\n",
    "    if n_nodes == 0:\n",
    "        print(\"No union-find nodes created.\", flush=True)\n",
    "        conn.close()\n",
    "        return\n",
    "\n",
    "    node_to_entity = np.empty(n_nodes, dtype=np.int32)\n",
    "    root_to_entity: dict[int, int] = {}\n",
    "    next_entity_id = 0\n",
    "    for node in range(n_nodes):\n",
    "        root = uf.find(node)\n",
    "        ent = root_to_entity.get(root)\n",
    "        if ent is None:\n",
    "            ent = next_entity_id\n",
    "            root_to_entity[root] = ent\n",
    "            next_entity_id += 1\n",
    "        node_to_entity[node] = ent\n",
    "\n",
    "    n_entities = next_entity_id\n",
    "    counts = np.bincount(node_to_entity, minlength=n_entities).astype(np.int64)\n",
    "    cluster_sizes = counts[counts > 0]\n",
    "\n",
    "    scores_arr, labels_arr = eval_col.arrays()\n",
    "    model_diag = {\n",
    "        \"easy_seen\": int(easy_seen),\n",
    "        \"easy_accepted\": int(easy_accepted),\n",
    "        \"p_accept\": float(p_accept_cal.value),\n",
    "        \"p_gap\": float(p_gap_cal.value) if p_gap_cal is not None else float(change_model.min_p_gap_fixed),\n",
    "        \"eval_samples\": int(0 if scores_arr is None else scores_arr.size),\n",
    "    }\n",
    "\n",
    "    top_entity_id = print_final_report(\n",
    "        n_nodes=n_nodes,\n",
    "        n_entities=n_entities,\n",
    "        node_to_entity=node_to_entity,\n",
    "        counts=counts,\n",
    "        cluster_sizes=cluster_sizes,\n",
    "        stats=stats,\n",
    "        multi_change_flags=multi_change_flags,\n",
    "        outputs=outputs,\n",
    "        guards=guards,\n",
    "        enable_constraints_diag=(guards.enable_change_merge_votes or guards.enable_change_degree_guard),\n",
    "        constraint_unique_pairs=constraint_unique_pairs,\n",
    "        constraint_gate_evals_total=constraint_gate_evals_total,\n",
    "        constraint_pairs_repeated=constraint_pairs_repeated,\n",
    "        change_merge_votes=change_merge_votes,\n",
    "        prevout_lookups=stats[\"n_prevout_lookups\"],\n",
    "        prevout_hits=stats[\"n_prevout_hits\"],\n",
    "        model_diag=model_diag,\n",
    "    )\n",
    "\n",
    "    # Sanity\n",
    "    if sanity.run_sanity_checks:\n",
    "        run_sanity_checks(\n",
    "            n_nodes=n_nodes,\n",
    "            node_to_entity=node_to_entity,\n",
    "            prevout_lookups=stats[\"n_prevout_lookups\"],\n",
    "            prevout_hits=stats[\"n_prevout_hits\"],\n",
    "            top_k=20,\n",
    "        )\n",
    "        if sanity.run_prevout_db_sanity and sanity_parquet:\n",
    "            prevout_join_sanity_db(\n",
    "                conn,\n",
    "                sanity_parquet,\n",
    "                sample_limit=sanity.prevout_sanity_sample_limit,\n",
    "                hybrid_threshold=perf.prevout_hybrid_threshold,\n",
    "                join_chunk=perf.prevout_lookup_chunk,\n",
    "            )\n",
    "\n",
    "    # Write mapping (address -> entity_id)\n",
    "    if outputs.write_entity_mapping:\n",
    "        print(f\"\\n[WRITE] Writing entity mapping to: {paths.entity_map_out_path}\", flush=True)\n",
    "        if paths.entity_map_out_path.exists():\n",
    "            paths.entity_map_out_path.unlink()\n",
    "\n",
    "        writer: pq.ParquetWriter | None = None\n",
    "        batch_addrs: list[str] = []\n",
    "        batch_eids: list[int] = []\n",
    "        written = 0\n",
    "\n",
    "        for addr, node_id in addr_to_id.items():\n",
    "            batch_addrs.append(addr)\n",
    "            batch_eids.append(int(node_to_entity[node_id]))\n",
    "\n",
    "            if len(batch_addrs) >= outputs.entity_write_batch:\n",
    "                table = pa.table({\"address\": batch_addrs, \"entity_id\": batch_eids})\n",
    "                if writer is None:\n",
    "                    writer = pq.ParquetWriter(\n",
    "                        str(paths.entity_map_out_path),\n",
    "                        table.schema,\n",
    "                        compression=outputs.entity_map_compression,\n",
    "                        use_dictionary=True,\n",
    "                    )\n",
    "                writer.write_table(table)\n",
    "                written += len(batch_addrs)\n",
    "                batch_addrs.clear()\n",
    "                batch_eids.clear()\n",
    "                print(f\"  [WRITE] rows written: {written:,}\", flush=True)\n",
    "\n",
    "        if batch_addrs:\n",
    "            table = pa.table({\"address\": batch_addrs, \"entity_id\": batch_eids})\n",
    "            if writer is None:\n",
    "                writer = pq.ParquetWriter(\n",
    "                    str(paths.entity_map_out_path),\n",
    "                    table.schema,\n",
    "                    compression=outputs.entity_map_compression,\n",
    "                    use_dictionary=True,\n",
    "                )\n",
    "            writer.write_table(table)\n",
    "            written += len(batch_addrs)\n",
    "            print(f\"  [WRITE] rows written: {written:,}\", flush=True)\n",
    "\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "\n",
    "        print(\"[WRITE] Done.\", flush=True)\n",
    "\n",
    "    # Confidence proxy output (optional)\n",
    "    if confidence.enable_confidence_proxy:\n",
    "        print(f\"\\n[CONF] Writing confidence proxy to: {confidence.confidence_out_path}\", flush=True)\n",
    "        if confidence.confidence_out_path.exists():\n",
    "            confidence.confidence_out_path.unlink()\n",
    "\n",
    "        def proxy_prob(node_id: int) -> float:\n",
    "            eid = int(node_to_entity[node_id])\n",
    "            sz = int(counts[eid])\n",
    "            if sz <= 1:\n",
    "                return float(confidence.p_singleton)\n",
    "\n",
    "            bits = int(multi_change_flags[node_id])\n",
    "            e = 0\n",
    "            if bits & 1:\n",
    "                e += 1\n",
    "            if bits & 2:\n",
    "                e += 1\n",
    "            if bits & 4:\n",
    "                e += 1\n",
    "\n",
    "            if e <= 0:\n",
    "                base = float(confidence.p_base_0)\n",
    "            elif e == 1:\n",
    "                base = float(confidence.p_base_1)\n",
    "            else:\n",
    "                base = float(confidence.p_base_2plus)\n",
    "\n",
    "            denom = math.log1p(max(1, confidence.size_norm))\n",
    "            frac = (math.log1p(sz) / denom) if denom > 0 else 1.0\n",
    "            frac = min(1.0, max(0.0, frac))\n",
    "            p = base + float(confidence.size_bonus_max) * frac\n",
    "            return float(min(0.999, max(0.001, p)))\n",
    "\n",
    "        writer2: pq.ParquetWriter | None = None\n",
    "        b_addr: list[str] = []\n",
    "        b_eid: list[int] = []\n",
    "        b_p: list[float] = []\n",
    "        b_sz: list[int] = []\n",
    "        b_bits: list[int] = []\n",
    "        written2 = 0\n",
    "\n",
    "        for addr, node_id in addr_to_id.items():\n",
    "            eid = int(node_to_entity[node_id])\n",
    "            b_addr.append(addr)\n",
    "            b_eid.append(eid)\n",
    "            b_p.append(proxy_prob(node_id))\n",
    "            if confidence.include_cluster_size:\n",
    "                b_sz.append(int(counts[eid]))\n",
    "            if confidence.include_evidence_bits:\n",
    "                b_bits.append(int(multi_change_flags[node_id]))\n",
    "\n",
    "            if len(b_addr) >= confidence.confidence_write_batch:\n",
    "                data = {\"address\": b_addr, \"entity_id\": b_eid, \"p_clustered_proxy\": b_p}\n",
    "                if confidence.include_cluster_size:\n",
    "                    data[\"cluster_size\"] = b_sz\n",
    "                if confidence.include_evidence_bits:\n",
    "                    data[\"evidence_bits\"] = b_bits\n",
    "                table = pa.table(data)\n",
    "                if writer2 is None:\n",
    "                    writer2 = pq.ParquetWriter(\n",
    "                        str(confidence.confidence_out_path),\n",
    "                        table.schema,\n",
    "                        compression=confidence.confidence_compression,\n",
    "                        use_dictionary=True,\n",
    "                    )\n",
    "                writer2.write_table(table)\n",
    "                written2 += len(b_addr)\n",
    "                b_addr.clear()\n",
    "                b_eid.clear()\n",
    "                b_p.clear()\n",
    "                b_sz.clear()\n",
    "                b_bits.clear()\n",
    "                print(f\"  [CONF] rows written: {written2:,}\", flush=True)\n",
    "\n",
    "        if b_addr:\n",
    "            data = {\"address\": b_addr, \"entity_id\": b_eid, \"p_clustered_proxy\": b_p}\n",
    "            if confidence.include_cluster_size:\n",
    "                data[\"cluster_size\"] = b_sz\n",
    "            if confidence.include_evidence_bits:\n",
    "                data[\"evidence_bits\"] = b_bits\n",
    "            table = pa.table(data)\n",
    "            if writer2 is None:\n",
    "                writer2 = pq.ParquetWriter(\n",
    "                    str(confidence.confidence_out_path),\n",
    "                    table.schema,\n",
    "                    compression=confidence.confidence_compression,\n",
    "                    use_dictionary=True,\n",
    "                )\n",
    "            writer2.write_table(table)\n",
    "            written2 += len(b_addr)\n",
    "            print(f\"  [CONF] rows written: {written2:,}\", flush=True)\n",
    "\n",
    "        if writer2 is not None:\n",
    "            writer2.close()\n",
    "\n",
    "        print(\"[CONF] Done.\", flush=True)\n",
    "\n",
    "    # Plots:\n",
    "    # - keep: cluster hist (all) + zipf\n",
    "    # - replace: (excl largest) and (<=focus_max_size) with logit eval (ROC + calibration)\n",
    "    if plots.enable_plots:\n",
    "        print(\"\\n[PLOT] Generating plots...\", flush=True)\n",
    "        y = analysis.analysis_start.year\n",
    "\n",
    "        save_hist = (paths.outputs_dir / f\"cluster_hist_{y}.png\") if plots.save_plots else None\n",
    "        save_zipf = (paths.outputs_dir / f\"cluster_zipf_{y}.png\") if plots.save_plots else None\n",
    "        save_roc  = (paths.outputs_dir / f\"logit_roc_{y}.png\") if plots.save_plots else None\n",
    "        save_cal  = (paths.outputs_dir / f\"logit_calibration_{y}.png\") if plots.save_plots else None\n",
    "\n",
    "        plot_hist_logbins(\n",
    "            sizes=cluster_sizes,\n",
    "            title=f\"Entity Cluster Size Distribution ({y}) — log bins (all clusters)\",\n",
    "            color=\"tab:blue\",\n",
    "            bins=plots.log_bins,\n",
    "            max_x=None,\n",
    "            save_path=save_hist,\n",
    "            show=plots.show_plots,\n",
    "        )\n",
    "\n",
    "        plot_zipf(\n",
    "            sizes=cluster_sizes,\n",
    "            title=f\"Rank–Size (Zipf) Plot ({y}) — top {min(plots.zipf_top_k, cluster_sizes.size):,} clusters\",\n",
    "            color=\"tab:red\",\n",
    "            top_k=plots.zipf_top_k,\n",
    "            save_path=save_zipf,\n",
    "            show=plots.show_plots,\n",
    "        )\n",
    "\n",
    "        if scores_arr is not None and labels_arr is not None:\n",
    "            plot_logit_eval(\n",
    "                scores=scores_arr.astype(np.float32, copy=False),\n",
    "                labels=labels_arr.astype(np.int8, copy=False),\n",
    "                roc_points=plots.logit_roc_points,\n",
    "                calib_bins=plots.logit_calibration_bins,\n",
    "                title_prefix=f\"Logit ({y})\",\n",
    "                save_roc=save_roc,\n",
    "                save_calib=save_cal,\n",
    "                show=plots.show_plots,\n",
    "            )\n",
    "        else:\n",
    "            print(\"[PLOT] No logit eval samples collected; skipping ROC/calibration.\", flush=True)\n",
    "\n",
    "    conn.close()\n",
    "    print(\"[INFO] Done.\", flush=True)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN: run two years (2013 + 2 => 2013, 2014)\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    outputs_dir = Path(\"/media/vatereal/Main/outputs\")\n",
    "\n",
    "    start_year = 2013\n",
    "    n_years = 2\n",
    "    base_seed = 1337\n",
    "\n",
    "    for analysis_year in range(start_year, start_year + n_years):\n",
    "        paths = PathsConfig(\n",
    "            parquet_io_glob=str(Path(\"/media/vatereal/Main/parquet\") / \"io/day=*/io-*.parquet\"),\n",
    "            outputs_dir=outputs_dir,\n",
    "            outpoint_db_path=outputs_dir / f\"outpoints_{analysis_year}.sqlite\",\n",
    "            entity_map_out_path=outputs_dir / f\"entities_model_features_{analysis_year}.parquet\",\n",
    "        )\n",
    "\n",
    "        analysis = AnalysisConfig(\n",
    "            analysis_start=date(analysis_year, 1, 1),\n",
    "            analysis_end_exclusive=date(analysis_year + 1, 1, 1),\n",
    "            outpoint_db_lookback_days=365,\n",
    "            value_unit_mode=\"infer\",\n",
    "        )\n",
    "\n",
    "        outputs = OutputConfig(\n",
    "            reset_outpoint_db=True,\n",
    "            write_entity_mapping=True,\n",
    "            entity_map_compression=\"zstd\",\n",
    "            entity_write_batch=1_000_000,\n",
    "            top_k_clusters_print=20,\n",
    "        )\n",
    "\n",
    "        plots = PlotConfig(\n",
    "            enable_plots=True,\n",
    "            zipf_top_k=200_000,\n",
    "            focus_max_size=2048,  # kept for compatibility; no longer used for plotting\n",
    "            log_bins=40,\n",
    "\n",
    "            # logit eval plots\n",
    "            logit_eval_max_samples=500_000,\n",
    "            logit_calibration_bins=20,\n",
    "            logit_roc_points=250,\n",
    "\n",
    "            # save/display\n",
    "            save_plots=True,\n",
    "            show_plots=False,\n",
    "        )\n",
    "\n",
    "        perf = PerfConfig(\n",
    "            outpoint_commit_every_rows=500_000,\n",
    "            prevout_hybrid_threshold=5_000,\n",
    "            prevout_lookup_chunk=50_000,\n",
    "            iter_buffer_in=200_000,\n",
    "            iter_buffer_out=200_000,\n",
    "            iter_buffer_grouped=50_000,\n",
    "            gc_every_n_files=100,\n",
    "            tqdm_mininterval=2.0,\n",
    "            tqdm_miniters=50,\n",
    "            tqdm_postfix_every=250,\n",
    "            use_sql_prevout_join=True,\n",
    "            vinbuf_insert_chunk=1_000_000,\n",
    "            vinbuf_fetch_chunk=1_000_000,\n",
    "        )\n",
    "\n",
    "        sanity = SanityConfig(\n",
    "            run_sanity_checks=True,\n",
    "            run_prevout_db_sanity=True,\n",
    "            prevout_sanity_sample_limit=200_000,\n",
    "            prevout_sanity_parquet=None,\n",
    "        )\n",
    "\n",
    "        toggles = HeuristicToggles(\n",
    "            enable_coinjoin_filter=True,\n",
    "            enable_multi_input=True,\n",
    "            enable_change=True,\n",
    "            enable_merge_guards=True,\n",
    "            precreate_nodes_for_all_output_addrs=False,\n",
    "            create_nodes_for_all_resolved_inputs=True,\n",
    "        )\n",
    "\n",
    "        params = HeuristicParams(\n",
    "            dust_sats=546,\n",
    "            max_fee_abs_sats=50_000_000,\n",
    "            max_fee_frac=0.05,\n",
    "            max_change_inputs_utxos=10,\n",
    "            max_change_spendable_outs=2,\n",
    "            change_require_2_outputs=True,\n",
    "            multi_input_policy=\"one_output\",\n",
    "        )\n",
    "\n",
    "        guards = MergeGuardParams(\n",
    "            max_merged_component_size=10_000_000,\n",
    "            merge_ratio_guard=True,\n",
    "            merge_ratio_max=200.0,\n",
    "            merge_ratio_big_cluster_min=50_000,\n",
    "            merge_ratio_small_floor=50,\n",
    "\n",
    "            enable_change_merge_votes=True,\n",
    "            change_vote_min_side=25_000,\n",
    "            change_vote_merged_trigger=0,\n",
    "            change_votes_required=2,\n",
    "            ultra_change_vote_rules=((250_000, 3), (500_000, 4), (1_000_000, 5)),\n",
    "\n",
    "            enable_change_degree_guard=True,\n",
    "            change_degree_large_min=25_000,\n",
    "            change_degree_small_min=50,\n",
    "            change_degree_max_distinct_large=3,\n",
    "            change_degree_set_cap=64,\n",
    "\n",
    "            ratio_guard_sample_n=1000,\n",
    "            ratio_guard_sample_path=outputs_dir / f\"ratio_guard_samples_{analysis_year}.tsv\",\n",
    "\n",
    "            constraint_log_n=200_000,\n",
    "            constraint_log_path=outputs_dir / f\"constraint_events_{analysis_year}.tsv\",\n",
    "\n",
    "            debug_large_change_merges=False,\n",
    "            debug_change_merge_min=250_000,\n",
    "            debug_change_merge_log_path=outputs_dir / f\"large_change_merges_{analysis_year}.tsv\",\n",
    "        )\n",
    "\n",
    "        confidence = ConfidenceConfig(\n",
    "            enable_confidence_proxy=True,\n",
    "            confidence_out_path=outputs_dir / f\"address_confidence_{analysis_year}.parquet\",\n",
    "            confidence_compression=\"zstd\",\n",
    "            confidence_write_batch=1_000_000,\n",
    "\n",
    "            size_norm=100_000,\n",
    "            size_bonus_max=0.15,\n",
    "            p_singleton=0.02,\n",
    "            p_base_0=0.25,\n",
    "            p_base_1=0.65,\n",
    "            p_base_2plus=0.85,\n",
    "\n",
    "            include_cluster_size=True,\n",
    "            include_evidence_bits=True,\n",
    "        )\n",
    "\n",
    "        # Logistic weights (featureized rules). Tune later if desired.\n",
    "        change_model = ChangeModelConfig(\n",
    "            intercept=-1.00,\n",
    "            weights={\n",
    "                \"cand_new\": 0.55,\n",
    "                \"cand_is_min_out\": 0.55,\n",
    "                \"cand_lt_other\": 0.20,\n",
    "                \"fee_frac\": -2.50,\n",
    "                \"log_fee\": -0.35,\n",
    "                \"in_type_uniform\": 0.25,\n",
    "                \"out_type_match_in\": 0.35,\n",
    "                \"optimal_ok\": 0.55,\n",
    "                \"is_mixing_like\": -2.00,\n",
    "                \"sum_in_gt_sum_out\": 0.50,\n",
    "                \"n_in\": 0.05,\n",
    "                \"n_out\": -0.25,\n",
    "            },\n",
    "\n",
    "            # Patch 2 (adaptive acceptance)\n",
    "            min_p_gap_fixed=0.10,\n",
    "\n",
    "            p_gap_quantile=None,          # set e.g. 0.90 to quantile-calibrate gap\n",
    "            p_gap_window=200_000,\n",
    "            p_gap_update_every=2000,\n",
    "            p_gap_warmup_min_samples=20_000,\n",
    "            p_gap_init=0.10,\n",
    "            p_gap_min=0.05,\n",
    "            p_gap_max=0.60,\n",
    "\n",
    "            p_accept_quantile=0.99,       # top 1% by p_best on easy stream\n",
    "            p_accept_window=200_000,\n",
    "            p_accept_update_every=2000,\n",
    "            p_accept_warmup_min_samples=20_000,\n",
    "            p_accept_init=0.85,\n",
    "            p_accept_min=0.70,\n",
    "            p_accept_max=0.995,\n",
    "\n",
    "            optimal_change_slack_sats=0,\n",
    "\n",
    "            log1p_fee_norm=1e6,\n",
    "            log1p_value_norm=1e8,\n",
    "        )\n",
    "\n",
    "        print(f\"\\n============================\")\n",
    "        print(f\" RUN YEAR {analysis_year}\")\n",
    "        print(f\"============================\", flush=True)\n",
    "\n",
    "        run_entity_clustering(\n",
    "            paths=paths,\n",
    "            analysis=analysis,\n",
    "            outputs=outputs,\n",
    "            plots=plots,\n",
    "            perf=perf,\n",
    "            sanity=sanity,\n",
    "            toggles=toggles,\n",
    "            params=params,\n",
    "            guards=guards,\n",
    "            confidence=confidence,\n",
    "            change_model=change_model,\n",
    "            determinism_seed=base_seed,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a84529",
   "metadata": {},
   "source": [
    "# Change-output Logit Model (Featureized Heuristic)\n",
    "\n",
    "This pipeline replaces brittle, year-specific hard rules for change detection with a **deterministic logistic model** plus an **online acceptance calibrator**.\n",
    "\n",
    "The change step is used only to propose a *single* additional union per eligible transaction:\n",
    "`anchor_input_address  ↔  selected_change_output_address`.\n",
    "\n",
    "---\n",
    "\n",
    "## 1) Candidate generation (eligibility)\n",
    "\n",
    "A transaction is eligible for change inference only if:\n",
    "\n",
    "- Not mixing-like (if coinjoin filter is enabled)\n",
    "- Inputs are fully resolved via the outpoint DB (all inputs have known previous outputs)\n",
    "- The transaction has exactly 2 spendable outputs (when `change_require_2_outputs=True`)\n",
    "- Input address types are uniform (e.g., all P2PKH), and candidate outputs match that type\n",
    "- Candidate outputs are not among the input addresses\n",
    "- Candidate outputs are distinct (no duplicate output addresses)\n",
    "\n",
    "From the 2 outputs, we build **two candidates**: score output A as “change” and score output B as “change”.\n",
    "\n",
    "---\n",
    "\n",
    "## 2) Feature extraction\n",
    "\n",
    "For each candidate output, we compute a feature vector `x`:\n",
    "\n",
    "### Structural / count features\n",
    "- `n_in`: number of input UTXOs\n",
    "- `n_out`: number of spendable outputs (typically 2 here)\n",
    "\n",
    "### Fee / amount features\n",
    "- `fee_frac`: `fee_sats / sum_inputs_sats`\n",
    "- `log_fee`: `log1p(fee_sats) / log1p(fee_norm)`\n",
    "- `log_cand`: `log1p(candidate_value_sats) / log1p(value_norm)`\n",
    "- `log_other`: `log1p(other_output_value_sats) / log1p(value_norm)`\n",
    "\n",
    "### Type / novelty features\n",
    "- `in_type_uniform`: inputs share a single script/address type\n",
    "- `out_type_match_in`: candidate output type matches the input type\n",
    "- `cand_new`: candidate address has not appeared as an output earlier in the run\n",
    "\n",
    "### Relative-output features\n",
    "- `cand_is_min_out`: candidate value is <= the other output value\n",
    "- `cand_lt_other`: candidate value is strictly < the other output value\n",
    "\n",
    "### “Optimal change” feature (soft constraint)\n",
    "- `optimal_ok`: `1` if `candidate_value_sats <= min_input_sats - fee_sats (+ slack)`, else `0`\n",
    "\n",
    "### Sanity / consistency\n",
    "- `sum_in_gt_sum_out`: `1` if `sum_inputs_sats > sum_outputs_sats`, else `0`\n",
    "\n",
    "### Mixing indicator (should be 0 in eligible stream)\n",
    "- `is_mixing_like`: included for completeness; acts as a strong negative weight\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Logistic scoring\n",
    "\n",
    "The model computes:\n",
    "\n",
    "`score = intercept + Σ_i w_i * x_i`\n",
    "\n",
    "Probability:\n",
    "\n",
    "`p = sigmoid(score) = 1 / (1 + exp(-score))`\n",
    "\n",
    "We score both candidates:\n",
    "- `p1 = P(output A is change)`\n",
    "- `p2 = P(output B is change)`\n",
    "\n",
    "Select the best candidate:\n",
    "- `p_best = max(p1, p2)`\n",
    "- `p_second = min(p1, p2)`\n",
    "\n",
    "Apply a *separation constraint*:\n",
    "- accept only if `p_best - p_second >= min_p_gap`\n",
    "\n",
    "This prevents ambiguous “coin-flip” assignments.\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Patch 2: online year-adaptive acceptance threshold\n",
    "\n",
    "Instead of fixed thresholds per year, we use an **online calibrator**:\n",
    "\n",
    "- Maintain a rolling window of `p_best` values for *easy* transactions:\n",
    "  - 2 spendable outputs\n",
    "  - not mixing-like\n",
    "  - all inputs resolved\n",
    "  - candidate set is exactly 2 outputs\n",
    "\n",
    "- Set the acceptance threshold dynamically:\n",
    "  - `p_accept = quantile(p_best_window, q)` (e.g., q = 0.99)\n",
    "  - clamp to `[p_accept_min, p_accept_max]`\n",
    "\n",
    "Decision rule becomes:\n",
    "\n",
    "Accept a change merge iff:\n",
    "1) the tx is eligible\n",
    "2) `p_gap = p_best - p_second >= min_p_gap`\n",
    "3) `p_best >= p_accept` (rolling quantile threshold)\n",
    "\n",
    "This yields a deterministic, testable policy that adapts naturally across years\n",
    "without guessing “correct” thresholds for 2013 vs 2014.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Effect on clustering\n",
    "\n",
    "When accepted:\n",
    "- Mark evidence bits:\n",
    "  - change output node gets bit2\n",
    "  - anchor/input node gets bit4\n",
    "- Apply a single union:\n",
    "  - `union(anchor_input_addr, selected_change_output_addr)`\n",
    "\n",
    "This keeps change-based linking sparse and reduces risk of runaway component growth.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blockchain-MbR7oPYh-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
